{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8664f6a0-a5af-433a-a326-65ac89015723",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "[How to Monitor Your Models in Production](https://neptune.ai/blog/how-to-monitor-your-models-in-production-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc352b-3102-44fb-a35b-1902ecb8da56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Goal of monitoring step\n",
    "\n",
    "![image.png](_images/6_Models_monitoring/model_degrade.png)\n",
    "\n",
    "**Cause:**\n",
    "- Machine learning models degrade over time. They’re dynamic and sensitive to real changes in the real world\n",
    "- Validation result during development will seldom fully show the model's performance in production\n",
    "- The difference of evironment between development and production, may be caused of the difference of performance\n",
    "\n",
    "**So, the Goal of monitoring model:**\n",
    "- To detect problems with your model and the system serving your model in production before they start to generate negative business value,\n",
    "- To take action by triaging and troubleshooting models in production or the inputs and systems that enable them,\n",
    "- To ensure their predictions and results can be explained and reported, \n",
    "- To ensure the model’s prediction process is transparent to relevant stakeholders for proper governance, \n",
    "- Finally, to provide a path for maintaining and improving the model in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a824cd-4b59-4616-825d-0f4eaa790dcd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Criteria of metrics selection\n",
    "\n",
    "It's very unique for each business case, depend on:\n",
    "- What does your business define as success and the KPIs that were set in business analysis phase ?\n",
    "- What were the performance expectation or result's distribution expectation before deploying to production ?\n",
    "\n",
    "**Criteria of metrics selection to make sense and be comfortable**\n",
    "- Availabel to compare across models\n",
    "- Simple and easy to understand\n",
    "- Can be collected/computed in real-time\n",
    "- Allows to set threshold for actionable alerting on problems.\n",
    "\n",
    "For example of metrics that use for building a loan approval system:\n",
    "1. What is the accuracy of model prediction that pay back at stipulated time ? __(Functional level monitoring)__\n",
    "2. How fast does model score is returned after get the request from client ? __(Operational level monitoring)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98df98-845e-4498-9884-9d2a8c0c92ad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Type of monitoring\n",
    "\n",
    "1. __Functional level monitoring__\n",
    "- Data (input data level)\n",
    "    - Data quality issues\n",
    "    - Data/feature drift\n",
    "    - Outliers\n",
    "- Model\n",
    "    - Monitoring model drift\n",
    "    - Model configuration and artifacts\n",
    "    - Model versions\n",
    "    - Concerted adversaries\n",
    "- Predictions (Output)\n",
    "    - Model evaluation metrics\n",
    "        - When availabel have Y true (the ground truth) ?\n",
    "        - When not availabel have Y true ?\n",
    "        \n",
    "2. __Operational level monitoring__\n",
    "- System performance monitoring for ML models in production\n",
    "    - System performance metrics\n",
    "        - CPU/GPU utilization\n",
    "        - Memory utilization\n",
    "        - Total number of `failed request`\n",
    "        - Total number of `API calls`\n",
    "        - Responce time\n",
    "    - System reliability\n",
    "- Pipelines\n",
    "    - Data pipelines\n",
    "    - Model pipeline\n",
    "- Cost\n",
    "    \n",
    "__Challenges might be met when monitoring__\n",
    "- At input level:\n",
    "    - Data sources in production may be scattered and unreliable\n",
    "    - Do not have clear data requirements\n",
    "    - Data sources don’t have defined ownership\n",
    "    - Metadata for your production data workflow is not discoverable\n",
    "- Teamwork\n",
    "- Model quality\n",
    "    - Ground truth (y_true) are not availability\n",
    "    - Model bias\n",
    "    - Blackbox model\n",
    "    - Tracking hyper-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc750db-dec8-4b3f-836b-6b59a506408f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stage of monitoring model\n",
    "\n",
    "![image.jpg](_images/6_Models_monitoring/Essential-Signals-to-Monitor.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3451b7-5aad-46c5-b933-399479a2e3ca",
   "metadata": {},
   "source": [
    "### Level 0: training and deploying models manually\n",
    "\n",
    "![image.jpg](_images/6_Models_monitoring/lv0.png)\n",
    "\n",
    "At this stage, you probably aren’t even thinking of monitoring your model yet, perhaps just finding a way to validate your model on the test set and hand it off to your IT Ops or software developers to deploy.\n",
    "\n",
    "I know because I was there. I celebrated when I handed it off, as mentioned at the beginning of this article, but as you know—a couple of months later—it has indeed ended in tears and on the hospital bed.\n",
    "\n",
    "For you to avoid this scenario, I propose you prioritize the lowest hanging fruit. Although less informative, and won’t help you monitor model performance, it can still serve as a reasonable performance proxy to tell you if your general application is working as intended. \n",
    "\n",
    "You don’t want to spend long hours focusing on monitoring your model’s metrics or try to justify its performance in line with a business KPI when your workflow is still in its manual deployment stage; such metrics will get easier to measure and analyze when your MLOps system gets mature, and you can collect ground truth labels or integrate other performance proxies in the absence of ground truth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca5078-1fb1-488e-bdb6-0f3365e28f86",
   "metadata": {},
   "source": [
    "\n",
    "### Level 1: continuous training of models\n",
    "\n",
    "![image.jpg](_images/6_Models_monitoring/lv1.png)\n",
    "\n",
    "Being at this level means that you have automated the machine learning pipeline to enable continuous training of your machine learning models based on triggers that have been set by criteria or a defined threshold.\n",
    "\n",
    "At this stage, I reckon you focus more on monitoring:\n",
    "\n",
    "- The business metric used to gauge your model’s performance (see “What Could Go Right” section)—if it doesn’t turn out to be pretty difficult to measure, especially if you can’t spend them on getting ground truth for monitoring model metrics.\n",
    "- The properties of your production data and your model’s performance in production to detect model staleness and degradation; can help with continuous training through triggers that automate the ML production pipelines to retrain models with new production data.\n",
    "- Your model’s retraining process needs to log pipeline metadata, model configuration, and model metadata because you’re most likely going to manually deploy a retrained model, and you want to make sure you can monitor the properties of that model before redeploying it to production.\n",
    "- You also need to monitor your production pipeline health as retraining steps are automated, and your data pipeline validates and preprocesses data from one or more sources.\n",
    "- You should also start monitoring how much your continuous training process is incurring so you don’t wake up with a gigantic AWS bill one day that you or your company did not plan for.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c0957-43d3-4ae2-87a7-c1f9ccc1aa82",
   "metadata": {},
   "source": [
    "### Level 2: completely mature in your MLOps\n",
    "\n",
    "![image.jpg](_images/6_Models_monitoring/lv2.png)\n",
    "\n",
    "Being at this level indicates that you’re completely mature in your MLOps implementation and pretty much the entire pipeline is a robust, automated CI/CD system. Your training, validation, and deployment phases are all automated in a complimentary feedback loop.\n",
    "\n",
    "At this stage, you should pretty much monitor everything but your team’s focus should be on the more informative metrics, making sure that all the relevant stakeholders are empowered with the more informative metrics before spending more time on the least informative metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0e4ab-6603-48bd-9ada-0a38a0d062ac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Best practices for monitoring\n",
    "\n",
    "__General monitoring best practices__\n",
    "\n",
    "- ___Focus on people first___. If you build a culture where data is also treated as the product in your organization, people will most likely be inclined to take ownership of the product to ensure it serves its intended purpose end-to-end. You can learn a lot from DevOps cultural change.\n",
    "- If it’s possible, don’t give the application’s “monitoring power” to one person. If you have a cross-functional team of data professionals and Ops engineers, let everyone handle their service and communicate effectively. This will help decentralize knowledge and know-how and when the use cases scale, no one will be overwhelmed.\n",
    "- Take a lean approach; using too many tools can be very tasking. Centralize your tools but decentralize the team; everyone staying on top of a task.\n",
    "- Monitoring doesn’t start after deployment, it starts when you begin experimentation. Build a culture of monitoring right from the model development stage (monitoring model experimentation metrics, logs, and so on).\n",
    "- Always consider what’s optimal for the productivity of your team when you encounter any crucial decision-making point.\n",
    "- Encourage your team to properly document their troubleshooting framework and create a framework for going from alerting to action to troubleshooting for effective model maintenance.\n",
    "\n",
    "__Best practices for data monitoring__\n",
    "\n",
    "- Batch and streaming data should be processed in the same manner, using the same pipeline so that issues with the data pipeline are a lot more intuitive to troubleshoot.\n",
    "- Ensure you go beyond checking for the drift for an entire dataset and look gradually at the feature drift as that can provide more insights.\n",
    "- Invest in a global data catalog that can help log high-quality metadata for your data that every user (your data and ML team) can rely on; it will help you tackle - challenges with streaming and maintaining reliable data quality. It will also make lineage tracking easier.\n",
    "- Perform a pre-launch validation on your evaluation set before moving your model to production to establish a baseline performance.\n",
    "\n",
    "__Best practices for model monitoring__\n",
    "\n",
    "- Model performance will inevitably degrade over time, but beware of a big dip in performance which is often indicative of something wrong—you can select tools that detect this automatically.\n",
    "- Perform shadow deployment and testing with the challenger model vs the champion model and log the predictions so that performance on the new model can be tracked alongside the current model in production; before you decide to deploy the newly trained (challenger) model.\n",
    "- You can use a metadata store (like Neptune.ai) to store hyperparameters for models that have been versioned and retrained in production; this improves auditing, compliance, lineage traceability, and troubleshooting. \n",
    "\n",
    "__Best practices for monitoring predictions/output__\n",
    "\n",
    "- Prediction drift can be a good performance proxy for model metrics, especially when ground truth isn’t available to collect, but it shouldn’t be used as the sole metric.\n",
    "- Track unreasonable outputs from your model. For example, your classification model predicting the wrong class for a set of inputs with a high confidence score, or your regression model predicting a negative score (when the base metric score should be 0) for a given set of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8eeb54-a9a8-4244-b0db-858b0b4f390f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bonus contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8011c-5ea3-4a52-8de5-cd30ea53d53a",
   "metadata": {},
   "source": [
    "### Monitoring vs Observability\n",
    "\n",
    "[Comparation of two](https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/#monitoring-vs-observability)\n",
    "\n",
    "__Observability__ is your ability to look at the metrics you’ve been monitoring and perform root-cause analysis on them to understand why they are a certain way, and what threat they pose to the overall performance of your system—all to improve system quality. \n",
    "\n",
    "__Monitoring__ is pretty much everything that happens before observability:\n",
    "- Collecting performance metrics, \n",
    "- tracking them, \n",
    "- detecting potential problems, \n",
    "- alerting the right user. \n",
    "\n",
    "__To put it simply, you can monitor without observing, but can’t observe your system’s overall performance without monitoring it. Monitoring is about collecting the dots, observability is about connecting them!__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f0d18-810c-472e-885b-a2389a03218f",
   "metadata": {},
   "source": [
    "### Setting alerts the right way\n",
    "\n",
    "- Test your alerts before they go into production\n",
    "- Monitor the primary metrics as concluded in your needs analysis.\n",
    "- Agree on the media for the alert, so every service owner is comfortable with their medium (email, stack,...)\n",
    "- Send context to the alert by including descriptive information and action by the primary service owner.\n",
    "- Make sure to set up a feedback loop that makes your monitoring better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa5b1a-2c74-4388-9a89-113ee67f86e1",
   "metadata": {},
   "source": [
    "### Write log everything\n",
    "[Read more](https://neptune.ai/blog/how-to-monitor-your-models-in-production-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a665342-89a3-4eb4-bf72-ae84e777f6a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Functional level monitoring\n",
    "![image.png](_images/6_Models_monitoring/Functional-Monitoring.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0cbc0-15e4-4988-a0d7-a761a0e432ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e435d9-abfd-4246-8a6a-0c2deaa10557",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data quality issues\n",
    "Tính toàn vẹn của Data input bị thay đổi. Để validate tính toàn vẹn của data trước khi đưa vào model, cần kiểm tra một số metrics liên quan đến data properties/ datatypes\n",
    "\n",
    "__Nguyên nhân:__\n",
    "- Break in data preprocesing pipelines\n",
    "- Change source of data\n",
    "- Data bị loss in source\n",
    "\n",
    "__Detection techniques:__\n",
    "- Testing input data for duplicates,\n",
    "- Testing input data for missing values,\n",
    "- Catching syntax errors,\n",
    "- Catching data type and format errors,\n",
    "- Kiểm tra source dữ liệu của feature bị detect có issue ,\n",
    "\n",
    "__Possible solutions after detecting data quality issues:__\n",
    "- Tạo alert khi data source thay đổi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f2083",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data/feature drift\n",
    "Sự thay đổi distribution/histogram của dữ liệu training và production sét trên level features/variables\n",
    "\n",
    "__Nguyên nhân:__\n",
    "- Data quality issue\n",
    "- Change in data properies in real world\n",
    "\n",
    "__Detection techniques:__\n",
    "- Testing __statistic estimator__ of input features: mean, STD, median, variance, range,...\n",
    "- For __continuous features__: use divergence and distance test the distribution: KL divergence, KS statistic, Population Stability Index (PSI), Hellinger distance,...\n",
    "- For __categorical features__: use chi-square test, entropy, number of distance, mode,...\n",
    "- Boxsplot\n",
    "\n",
    "_(if there are a lot of features, can be use dimmensionality reducetion techniques (such as PCA,...) before test)_\n",
    "\n",
    "__Possible solutions after detecting data drift:__\n",
    "- Tạo alert và gửi notif khi phát hiện data drift vượt threshold\n",
    "- Retrain set of new data collection in model periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969128bc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### PSI\n",
    "__Rules__:\n",
    "- `PSI` < 0.1 - No change. You can continue using existing model.\n",
    "- `PSI` >=0.1 but less than 0.2 - Slight change is required.\n",
    "- `PSI` >=0.2 - Significant change is required. Ideally, you should not use this model any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e70ceb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _psi(expected: np.ndarray, actual: np.ndarray, bucket_type: str = \"bins\", n_bins: int = 10) -> float:\n",
    "    \"\"\"Calculate PSI metric for two arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        expected : list-like\n",
    "            Array of expected values\n",
    "        actual : list-like\n",
    "            Array of actual values\n",
    "        bucket_type : str\n",
    "            Binning strategy. Accepts two options: 'bins' and 'quantiles'. Defaults to 'bins'.\n",
    "            'bins': input arrays are split into bins with equal\n",
    "                and fixed steps based on 'expected' array\n",
    "            'quantiles': input arrays are binned according to 'expected' array\n",
    "                with given number of n_bins\n",
    "        n_bins : int\n",
    "            Number of buckets for binning. Defaults to 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A single float number\n",
    "    \"\"\"\n",
    "    breakpoints = np.arange(0, n_bins + 1) / (n_bins) * 100\n",
    "    if bucket_type == \"bins\":\n",
    "        breakpoints = np.histogram(expected, n_bins)[1]\n",
    "    elif bucket_type == \"quantiles\":\n",
    "        breakpoints = np.percentile(expected, breakpoints)\n",
    "\n",
    "    # Calculate frequencies\n",
    "    expected_percents = np.histogram(expected, breakpoints)[0] / len(expected)\n",
    "    actual_percents = np.histogram(actual, breakpoints)[0] / len(actual)\n",
    "    # Clip frequencies to avoid zero division\n",
    "    expected_percents = np.clip(expected_percents, a_min=0.0001, a_max=None)\n",
    "    actual_percents = np.clip(actual_percents, a_min=0.0001, a_max=None)\n",
    "    # Calculate PSI\n",
    "    psi_value = (expected_percents - actual_percents) * np.log(expected_percents / actual_percents)\n",
    "    psi_value = sum(psi_value)\n",
    "\n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def calculate_psi(\n",
    "        expected: np.ndarray, actual: np.ndarray, bucket_type: str = \"bins\", n_bins: int = 10, axis: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Apply PSI calculation to 2 1-d or 2-d arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expected : list-like\n",
    "        Array of expected values\n",
    "    actual : list-like\n",
    "        Array of actual values\n",
    "    bucket_type : str\n",
    "        Binning strategy. Accepts two options: 'bins' and 'quantiles'. Defaults to 'bins'.\n",
    "            'bins' - input arrays are split into bins with equal\n",
    "                and fixed steps based on ’expected' array\n",
    "            'quantiles' - input arrays are binned according to ’expected’ array\n",
    "                with given number of n_bins\n",
    "    n_bins : int\n",
    "        Number of buckets for binning. Defaults to 10.\n",
    "    axis : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        np.ndarray\n",
    "\n",
    "    Args:\n",
    "        axis:\n",
    "        axis:\n",
    "    \"\"\"\n",
    "    if len(expected.shape) == 1:\n",
    "        psi_values = np.empty(len(expected.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(expected.shape[axis])\n",
    "\n",
    "    for i in range(0, len(psi_values)):\n",
    "        if len(psi_values) == 1:\n",
    "            psi_values = _psi(expected, actual, bucket_type, n_bins)\n",
    "        elif axis == 0:\n",
    "            psi_values[i] = _psi(expected[:, i], actual[:, i], bucket_type, n_bins)\n",
    "        elif axis == 1:\n",
    "            psi_values[i] = _psi(expected[i, :], actual[i, :], bucket_type, n_bins)\n",
    "        return np.array(psi_values)\n",
    "\n",
    "\n",
    "calculate_psi(feature_train_proba, feature_produ_proba, bucket_type=\"bins\", n_bins=10, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4b06f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Outliers\n",
    "Sự xuất hiện với tần xuất nhiều các outlies có thể ảnh hưởng tới hiệu suất của mô hình, hoặc dấu hiệu chỉ ra 1 pattern mới mà dữ liệu train trước đó chưa được học.\n",
    "\n",
    "__Detection techniques:__\n",
    "- Determine how far/how often from outlier to training dataset\n",
    "\n",
    "__Possible solutions after detecting outliers:__\n",
    "- Tạo subset mới chứa outlier và retrain new model, đánh giá sự khác biệt giữa new model và primary model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378e5e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f92a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model drift\n",
    "Hiện tượng thay đổi relationship giữa biến Y và các biến X (supervised) hoặc giữa các biến X (unsupervised) với nhau, thậm trí không còn mối tương quan, dẫn tới kết quả model giảm tính chính xác overtime so với benchmark/KPIs\n",
    "\n",
    "__Cause:__\n",
    "- The real-world data changes naturally or sudden as stress events\n",
    "\n",
    "__Model drift detection__\n",
    "- Catching the change of correlation/auc/... between X and Y or between Xs\n",
    "- Detect by predictive performance is reduce overtime by setting a predictive metrics threshold\n",
    "- Detect by label drift (change the distribution)\n",
    "\n",
    "__Possible solutions after detecting model/concept drift__\n",
    "- If your business objectives and environment change frequently, you may want to consider automating your system to schedule and execute retraining at predefined intervals compared to more stable businesses\n",
    "- If retraining your models doesn’t improve performance, you may want to consider remodeling or redeveloping models from scratch.\n",
    "- If you’re working on larger scale projects with a good budget and little trade-off between cost and performance (in terms of how well your model catches up with a very dynamic business climate), you may want to consider __online learning algorithms__ for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9050a45",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model configuration and artifacts, version\n",
    "The model configuration file and artifacts contain all the components that were used to build that model, including:\n",
    "\n",
    "- Training dataset location and version,\n",
    "- Test dataset location and version,\n",
    "- Model version\n",
    "- Hyperparameters used,\n",
    "- Default feature values,\n",
    "- Dependencies and their versions; you want to monitor changes in dependency versions to easily find them for root cause analysis when model failure is caused by dependency changes,\n",
    "- Environment variables,\n",
    "- Model type (classification vs regression),\n",
    "- Model author,\n",
    "- Target variable name,\n",
    "- Features to select from the data,\n",
    "- Code and data for testing scenarios,\n",
    "- Code for the model and its preprocessing.\n",
    "\n",
    "Track the configurations for relevance—especially the hyperparameter values used by the model during retraining for any abnormality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a0bb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Protect model by attack\n",
    "\n",
    "Monitor your system for adversarial attacks by using the same steps you use to flag inputs with outlier events because adversarial threats don’t follow a pattern, they’re atypical events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31cccc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predictions (Output)\n",
    "\n",
    "Monitoring model output in production is not just the best indicator of model performance, but it also tells us if business KPIs are being met. In terms of model predictions, the most important thing to monitor is model performance in line with business metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4f238",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model evaluation metrics\n",
    "\n",
    "(Scoring models when ground truth is available)\n",
    "\n",
    "Using metrics to evaluate model performance is a big part of monitoring your model in production. Different metrics can be used here, such as classification, regression, clustering, reinforcement learning, and so on.\n",
    "\n",
    "We typically evaluate the model using predefined model scoring metrics (accuracy, AUC, precision, etc) when you have a ground truth/label to compare your model with.\n",
    "\n",
    "![image.png](_images/6_Models_monitoring/avai_y.png)\n",
    "\n",
    "At `1`, a part of the production data (input data) is channeled to the ground truth service which typically involves real-time ground truth generated by your system (for example, logging if a user clicked on an ad when the model predicted they would), a human label annotator, or other data labeling vendors for more complicated tasks (such as confirming if a customer repaid a loan at the stipulated time, or confirming if a transaction was fraudulent or legitimate after contacting a customer).\n",
    "\n",
    "The event id that tracks prediction and model details is tagged with that ground truth event and logged to a data store. The data is then ingested into the monitoring platform, which computes the model performance metric given the model’s prediction and the actual label.\n",
    "\n",
    "- As you probably already know, metrics for a classification model include:\n",
    "    - Accuracy\n",
    "    - Confusion Matrix,\n",
    "    - ROC-AUC Score,\n",
    "    - Precision and Recall Scores,\n",
    "    - F1-Score.\n",
    "\n",
    "- Metrics for a regression model include:\n",
    "    - Root Mean Square Error (RMSE),\n",
    "    - R-Squared and Adjusted R-Square Metrics,\n",
    "    - Mean Absolute Error (MAE),\n",
    "    - Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "Calculating the model metrics above is only possible when you have the ground truth available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe581c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prediction Drift\n",
    "\n",
    "(Scoring models when ground truth is NOT available)\n",
    "\n",
    "![image.png](_images/6_Models_monitoring/not_avai_y.png)\n",
    "\n",
    "- Metrics:\n",
    "    - Hellinger Distance (HDDDM)\n",
    "    - Kullback-Leibler Divergence: đo sự khác biệt giữa 2 phân phối rời rạc\n",
    "    - Population Stability Index (PSI): đo sự khác biệt giữa 2 phân phối liên tục\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1422ef3-01ae-429f-bacb-9015a3addb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSI\n",
    "import numpy as np\n",
    "\n",
    "def _psi(expected: np.ndarray, actual: np.ndarray, bucket_type: str = \"bins\", n_bins: int = 10) -> float:\n",
    "    \"\"\"Calculate PSI metric for two arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        expected : list-like\n",
    "            Array of expected values\n",
    "        actual : list-like\n",
    "            Array of actual values\n",
    "        bucket_type : str\n",
    "            Binning strategy. Accepts two options: 'bins' and 'quantiles'. Defaults to 'bins'.\n",
    "            'bins': input arrays are split into bins with equal\n",
    "                and fixed steps based on 'expected' array\n",
    "            'quantiles': input arrays are binned according to 'expected' array\n",
    "                with given number of n_bins\n",
    "        n_bins : int\n",
    "            Number of buckets for binning. Defaults to 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A single float number\n",
    "    \"\"\"\n",
    "    breakpoints = np.arange(0, n_bins + 1) / (n_bins) * 100\n",
    "    if bucket_type == \"bins\":\n",
    "        breakpoints = np.histogram(expected, n_bins)[1]\n",
    "    elif bucket_type == \"quantiles\":\n",
    "        breakpoints = np.percentile(expected, breakpoints)\n",
    "\n",
    "    # Calculate frequencies\n",
    "    expected_percents = np.histogram(expected, breakpoints)[0] / len(expected)\n",
    "    actual_percents = np.histogram(actual, breakpoints)[0] / len(actual)\n",
    "    # Clip frequencies to avoid zero division\n",
    "    expected_percents = np.clip(expected_percents, a_min=0.0001, a_max=None)\n",
    "    actual_percents = np.clip(actual_percents, a_min=0.0001, a_max=None)\n",
    "    # Calculate PSI\n",
    "    psi_value = (expected_percents - actual_percents) * np.log(expected_percents / actual_percents)\n",
    "    psi_value = sum(psi_value)\n",
    "\n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def calculate_psi(\n",
    "        expected: np.ndarray, actual: np.ndarray, bucket_type: str = \"bins\", n_bins: int = 10, axis: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Apply PSI calculation to 2 1-d or 2-d arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expected : list-like\n",
    "        Array of expected values\n",
    "    actual : list-like\n",
    "        Array of actual values\n",
    "    bucket_type : str\n",
    "        Binning strategy. Accepts two options: 'bins' and 'quantiles'. Defaults to 'bins'.\n",
    "            'bins' - input arrays are split into bins with equal\n",
    "                and fixed steps based on ’expected' array\n",
    "            'quantiles' - input arrays are binned according to ’expected’ array\n",
    "                with given number of n_bins\n",
    "    n_bins : int\n",
    "        Number of buckets for binning. Defaults to 10.\n",
    "    axis : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        np.ndarray\n",
    "\n",
    "    Args:\n",
    "        axis:\n",
    "        axis:\n",
    "    \"\"\"\n",
    "    if len(expected.shape) == 1:\n",
    "        psi_values = np.empty(len(expected.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(expected.shape[axis])\n",
    "\n",
    "    for i in range(0, len(psi_values)):\n",
    "        if len(psi_values) == 1:\n",
    "            psi_values = _psi(expected, actual, bucket_type, n_bins)\n",
    "        elif axis == 0:\n",
    "            psi_values[i] = _psi(expected[:, i], actual[:, i], bucket_type, n_bins)\n",
    "        elif axis == 1:\n",
    "            psi_values[i] = _psi(expected[i, :], actual[i, :], bucket_type, n_bins)\n",
    "        return np.array(psi_values)\n",
    "\n",
    "\n",
    "calculate_psi(y_train_proba, y_produ_proba, bucket_type=\"bins\", n_bins=10, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5686e5b-50ce-4b42-b4a7-71a8d1670ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.589885181619163"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kullback\n",
    "from scipy.special import rel_entr\n",
    "P = [.05, .1, .2, .05, .15, .25, .08, .12]\n",
    "Q = [.3, .1, .2, .1, .1, .02, .08, .1]\n",
    "#calculate (P || Q)\n",
    "sum(rel_entr(P, Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287a848",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Operational level monitoring\n",
    "![image.jpg](_images/6_Models_monitoring/Operational-Monitoring.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9b309",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## System performance and reliability\n",
    "\n",
    "The system/application performance metrics to monitor that will give you an idea of model performance include:\n",
    "\n",
    "- CPU/GPU utilization when the model is computing predictions on incoming data from each API call; tells you how much your model is consuming per request.\n",
    "- Memory utilization for when the model caches data or input data is cached in memory for faster I/O performance.\n",
    "- Number of failed requests by an event/operation.\n",
    "- Total number of API calls.\n",
    "- Response time of the model server or prediction service.\n",
    "- System reliability: infrastructure and network uptime,...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b967c-9c67-44e6-9192-ebd002605dc0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "## Pipelines\n",
    "\n",
    "Monitor the health of your data and model pipeline. Unhealthy data pipelines can affect data quality, and your model pipeline leakages or unexpected changes can easily generate negative value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d1c1d-598e-462d-b616-2d669b895853",
   "metadata": {},
   "source": [
    "### Data pipelines\n",
    "\n",
    "Monitoring the health of data pipelines is extremely crucial because data quality issues can arise from bad or unhealthy data pipelines. This especially is extremely tricky to monitor for your IT Ops/DevOps team and may require empowering your data engineering/DataOps team to monitor and troubleshoot issues.\n",
    "\n",
    "It also has to be a shared responsibility. Work with your DataOps team, communicate what your model expects, and the team will tell you what the output of their data pipeline is—this can help you tighten up your system and drive positive results.\n",
    "\n",
    "If you’re charged with the responsibility of monitoring your data pipeline, here are some metrics and factors you may want to track:\n",
    "\n",
    "- __Input data__ – are the data and files in the pipeline with the appropriate structure, schema, and completeness? Are there data validation tests and checks in place so that the team can be alerted in case of an oddity in ingested data? Monitor what comes into the data pipeline to keep it healthy.\n",
    "- __Intermediate workflow steps__ – are the inputs and outputs of every task and flow in the DAG as expected, in terms of the number of files and file types? How long does a task take to run in the pipeline? This could be the data preprocessing task, or the validation task, or even the data distribution monitoring task.\n",
    "- __Output data__ – is the output data schema as expected by the machine learning model in terms of features and feature embeddings? What’s the typical file size expected from an output file?\n",
    "- __Data quality metrics__ – tracking the statistical metrics according to the data that flows in. This could be basic statistical properties of the data such as mean, standard deviation, correlation, and so on, or distance metrics (such as KL divergence, Kolmogorov-Smirnov statistic). The statistical metric used will be mostly dependent on the dimension of data expected; a couple of features or several features.\n",
    "- __Scheduled run time__ of a job, actual run time, how long it took to run, and the state of the job (successful, or failed job?).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f66f6c-7f24-4fea-86c1-4a0d65669199",
   "metadata": {},
   "source": [
    "### Model pipeline\n",
    "\n",
    "You want to track crucial factors that can cause your model to break in production after retraining and being redeployed. This includes:\n",
    "\n",
    "- Dependencies – you don’t want a situation where your model was built with Tensorflow 2.0 and a recent dependency update by someone else on your team that’s bundled with Tensorflow 2.4 causes part of your retraining script to fail. Validate the versions of each dependency your model runs on and log that as your pipeline metadata, so dependency updates that cause failure can be easier to debug.\n",
    "- The actual time a retraining job was triggered, how long it took the retraining job to run, resources usage of the job, and the state of the job (successfully retrained and redeployed model, or failed?).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c6266-7973-4291-9285-7dfd76c4e92e",
   "metadata": {},
   "source": [
    "## Cost\n",
    "\n",
    "You need to keep an eye out for how much it’s costing you and your organization to host your entire machine learning application, including data storage and compute costs, retraining, or other types of orchestrated jobs. These costs can add up fast, especially if they’re not being tracked. Also, it takes computational power for your models to make predictions for every request, so you also need to track inference costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
