{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacc5ada",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80787fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7b0214a-52ca-4af5-9b29-1308d036720a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lesson1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21ae61a1d90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the env spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    " # khởi tạo trình chạy pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Lesson1').getOrCreate()\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db57fc2-8720-45e3-a50e-7db6585fc5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cores\n",
    "spark._jsc.sc().getExecutorMemoryStatus().keySet().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d1d04-62ed-4118-af4c-de2a93837a92",
   "metadata": {},
   "source": [
    "## DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95990802-c737-4471-a475-41f8d70eda04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create from row\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64975128-4158-43d0-8db9-aaae421f1419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create from tuple\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4b7109-609c-4108-a2f5-08771c4481ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PySpark DataFrame from an RDD consisting of a list of tuples.\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "405c956d-05ed-4f65-802b-3a4a3408e549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pandas.dataframe\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5c3e9-c0b6-4d9a-944e-2484b5736190",
   "metadata": {},
   "source": [
    "## Viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fb1d520-4ca7-4fa6-88b6-7ea95bfb1042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|_c0|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  0|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  1|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|  2|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f0a2e35-f023-4bc6-be8d-f9eabdef3e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show sample\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "501a8fb8-2e6b-4b1d-be98-a6fee189b979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cd315f4-c268-4e67-817d-78f66b889393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len df \n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6043cc9-caec-459c-8c5f-1bf1ec6e5e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql.repl.eagerEval.maxNumRows # setup maxrow to show\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True) # setup config to show in jupyter\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06245b1-9735-42ad-92c6-6e770c3e2bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dtype\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d6fe7c-99e7-4804-983a-b3e191ff26e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b        c           d                   e\n",
       "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
       "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
       "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert pandas.df\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d7190f-6a56-42ab-90b6-aa966a55dc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd806ef-45f8-4bfb-bd7e-3e5ee2a4c4d2",
   "metadata": {},
   "source": [
    "## Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00688478-b1b6-4fdc-a291-f9e1bbeff98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J:\\\\My Drive\\\\GitCode\\\\My_learning\\\\11. BigData\\\\0. Data\\\\datatest.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file = os.path.normpath(os.path.join(os.getcwd(),\"..\",\"0. Data\",'datatest.csv')) ## \"..\" to get out folder\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4524b1d-f257-4ec1-bee3-a0fbf6ed40ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|_c0|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  0|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(file,header=True)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d78d611-b028-4577-8929-7276f8ab670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|_c0|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  0|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# csv\n",
    "import os\n",
    "# file = os.path.normpath(os.path.join(os.getcwd(),\"..\",\"2.Dask\",'datatest','1989.csv')) ## \"..\" to get out folder\n",
    "df = spark.read.csv(r\"J:\\My Drive\\GitCode\\My_learning\\11. BigData\\0. Data\\datatest.csv\",header=True)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c533e90-1508-40b1-8533-d0a44b2d856d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  _c0 sepal_length sepal_width petal_length petal_width species\n",
       "0   0          5.1         3.5          1.4         0.2  setosa\n",
       "1   1          4.9         3.0          1.4         0.2  setosa\n",
       "2   2          4.7         3.2          1.3         0.2  setosa\n",
       "3   3          4.6         3.1          1.5         0.2  setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import myfunction as mf\n",
    "mf.display(df.limit(4).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b3ba8-8b97-4dc6-8cdc-36b32fab85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read parquet\n",
    "df = spark.read.parquet('data.parquet') # read file parquet\n",
    "\n",
    "# read many file parquet with the sample form\n",
    "df_all = spark.read.parquet('data*.parquet')\n",
    "df1_2 = spark.read.parquet('data1.parquet','data2.parquet')\n",
    "df = spark.read.option('bathPath',path).parquet(path + '//data*.parquet') # set option\n",
    "df = spark.read.parquet(folder1 +\"//data*.parquet\", folder2 +\"//*\") # set option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf48e3-c42b-4f0b-a816-a95cb6d48a58",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "- set schema to datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a2643d-1789-4942-a8d1-992f8bed590b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- sepal_length: string (nullable = true)\n",
      " |-- sepal_width: string (nullable = true)\n",
      " |-- petal_length: string (nullable = true)\n",
      " |-- petal_width: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # get dtype by SAMPLE of beginning datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6a26b92-b4df-4279-ab74-5dd651208504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c0: string, sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7f5b158-e42f-4722-b22c-990d6ae5c9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructField('sepal_length', StringType(), True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema['sepal_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "479ca886-e3a7-4077-8c1d-83db130d811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: float (nullable = true)\n",
      " |-- sepal_width: float (nullable = true)\n",
      " |-- petal_width: float (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# set schema\n",
    "data_schema = [\n",
    "            # StructField('_c0',IntegerType()),\n",
    "              StructField('sepal_length',FloatType()),\n",
    "              StructField('sepal_width',FloatType()),\n",
    "              # StructField('petal_length',FloatType()), # dont need to set all\n",
    "              StructField('petal_width',FloatType()),\n",
    "              StructField('species',StringType()),\n",
    "              ]\n",
    "\n",
    "final_struc = StructType(fields = data_schema)\n",
    "\n",
    "df = spark.read.csv(file,header=True,schema = final_struc)\n",
    "\n",
    "df.printSchema() # chỉ đọc những cột có định nghĩa trong final_struc schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05ce0057-2155-4d64-aa17-5e1b1ba474ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|_c0|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  0|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  1|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- sepal_length: string (nullable = true)\n",
      " |-- sepal_width: string (nullable = true)\n",
      " |-- petal_length: string (nullable = true)\n",
      " |-- petal_width: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40b276-561a-46be-8cc4-89c1a54149fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the dtype columns\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df = df.withColumn('sepal_length',df['sepal_length'].cast(FloatType()) )\\\n",
    "        .withColumn('sepal_width',df['sepal_width'].cast(IntegerType()) )\\\n",
    "        .withColumn('sepal_length',to_date(df['sepal_length'],'yy.dd.mm') )\\\n",
    "        .withColumn('sepal_length',to_timestamp(df['petal_width'],'yy.dd.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2a4d8-b635-433b-b341-3221b3ea15a3",
   "metadata": {},
   "source": [
    "## Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97ca423-29f5-4b8c-be18-dea7c2d0c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(r\"J:\\My Drive\\GitCode\\My_learning\\11. BigData\\0. Data\\datatest.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd83115-58bc-4b69-b86e-fd39d2e46d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(\"/Volumes/GoogleDrive-106231888590528523181/My Drive/GitCode/My_learning/11. BigData/3. Spark/ab\",'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea4f77e8-82ae-4b56-a228-7172c16f3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dc6ad51-2b26-45ea-983c-74d0be171f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_import(spark._jvm,\"org.apache.hadoop.fs.Path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c48aba-f461-44fa-a9cd-622c2ab8be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "file = fs.globStatus(spark._jvm.Path('write_test.csv/part*'))[0].getPath().getName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf298a1-adc4-4799-8525-2cdd9d001574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c55772-11bc-4343-8c21-e025c1a28266",
   "metadata": {},
   "source": [
    "## Selecting and accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3333e609-4a9f-4137-b3ad-739feb6b68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a75eea-63bb-4b12-b827-8a6bae917930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td></tr>\n",
       "<tr><td>3</td><td>4.0</td><td>string3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+\n",
       "|  a|  b|      c|\n",
       "+---+---+-------+\n",
       "|  1|2.0|string1|\n",
       "|  2|3.0|string2|\n",
       "|  3|4.0|string3|\n",
       "+---+---+-------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select specific columns\n",
    "df.select(\"a\", \"b\", \"c\")#.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f938f9d-1fc8-4b02-af0a-43acb548e688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
       "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
       "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c12e0cf5-1ab0-4dc7-94b9-ed182350739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------+------------+\n",
      "|summary|sepal_length|sepal_width|petal_length|\n",
      "+-------+------------+-----------+------------+\n",
      "|  count|         150|        150|         150|\n",
      "|    max|         7.9|        4.4|         6.9|\n",
      "|    min|         4.3|        2.0|         1.0|\n",
      "+-------+------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('sepal_length','sepal_width','petal_length').summary('count','max','min').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a300376-1071-4588-8e17-ae2948b20ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|(c IS NULL)|\n",
      "+-----------+\n",
      "|      false|\n",
      "|      false|\n",
      "|      false|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.c.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fe06138-0f8a-4113-ba41-f1097e41db21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|upper_c|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add new column\n",
    "df.withColumn('upper_c', upper(df.c)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12c02d9e-2748-43dc-9da8-9df638348b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter\n",
    "df.filter(df.a == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e320e5a3-9252-4d58-9eac-e25e3fd689eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|_c0|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  0|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# csv\n",
    "import os\n",
    "# file = os.path.normpath(os.path.join(os.getcwd(),\"..\",\"2.Dask\",'datatest','1989.csv')) ## \"..\" to get out folder\n",
    "df = spark.read.csv(r\"J:\\My Drive\\GitCode\\My_learning\\11. BigData\\0. Data\\datatest.csv\",header=True)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372bd7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------+------------+-----------+-------+\n",
      "|_c0|col1|sepal_width|petal_length|petal_width|species|\n",
      "+---+----+-----------+------------+-----------+-------+\n",
      "|  0| 5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  1| 4.9|        3.0|         1.4|        0.2| setosa|\n",
      "+---+----+-----------+------------+-----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename columns\n",
    "df.withColumnRenamed('sepal_length','col1').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d514f6b-e309-40be-a8fd-0f784d374416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+\n",
      "|sepal_length|sepal_width|petal_length|\n",
      "+------------+-----------+------------+\n",
      "|5.1         |3.5        |1.4         |\n",
      "|4.9         |3.0        |1.4         |\n",
      "|4.7         |3.2        |1.3         |\n",
      "+------------+-----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# truncate to show all value in cell, not \"...\"\n",
    "df.select(['sepal_length','sepal_width','petal_length']).show(3,truncate = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ccb9af-1bf7-4518-82c8-70277e3c43c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+\n",
      "|sepal_length|sepal_width|petal_length|\n",
      "+------------+-----------+------------+\n",
      "|         4.3|        3.0|         1.1|\n",
      "|         4.4|        3.2|         1.3|\n",
      "|         4.4|        3.0|         1.3|\n",
      "|         4.4|        2.9|         1.4|\n",
      "|         4.5|        2.3|         1.3|\n",
      "+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort value\n",
    "df.select(['sepal_length','sepal_width','petal_length']).orderBy(df['sepal_length']).show(5)\n",
    "df.select(['sepal_length','sepal_width','petal_length']).orderBy(df['sepal_length'].desc()).show(5) # sort desc \n",
    "df.orderBy(df['sepal_length'].desc(),df['sepal_width']).show() # multi columns to sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da011344-021a-45a5-ac61-041bf2516a5c",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d7a694-2276-454c-846b-e7e7e5e48567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|_c0|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  0|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# file = os.path.normpath(os.path.join(os.getcwd(),\"..\",\"2.Dask\",'datatest','1989.csv')) ## \"..\" to get out folder\n",
    "df = spark.read.csv(r\"J:\\My Drive\\GitCode\\My_learning\\11. BigData\\0. Data\\datatest.csv\",header=True)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b115455-44f5-4403-9e2a-96e9d162839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+----------+\n",
      "|_c0|sepal_length|sepal_width|petal_length|petal_width|   species|\n",
      "+---+------------+-----------+------------+-----------+----------+\n",
      "|  0|         5.1|        3.5|         1.4|        0.2|    setosa|\n",
      "| 17|         5.1|        3.5|         1.4|        0.3|    setosa|\n",
      "| 19|         5.1|        3.8|         1.5|        0.3|    setosa|\n",
      "| 21|         5.1|        3.7|         1.5|        0.4|    setosa|\n",
      "| 23|         5.1|        3.3|         1.7|        0.5|    setosa|\n",
      "| 39|         5.1|        3.4|         1.5|        0.2|    setosa|\n",
      "| 44|         5.1|        3.8|         1.9|        0.4|    setosa|\n",
      "| 46|         5.1|        3.8|         1.6|        0.2|    setosa|\n",
      "| 98|         5.1|        2.5|         3.0|        1.1|versicolor|\n",
      "+---+------------+-----------+------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter\n",
    "df.filter(df.sepal_length == 5.1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e8dab63-331f-4d7a-aba8-a2eb1147c360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-------+\n",
      "|sepal_length|sepal_width|petal_length|species|\n",
      "+------------+-----------+------------+-------+\n",
      "|         5.1|        3.5|         1.4| setosa|\n",
      "|         4.9|        3.0|         1.4| setosa|\n",
      "+------------+-----------+------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where va filter la nhu nhau \n",
    "df.select(['sepal_length','sepal_width','petal_length','species']).where(df.species.like(\"%eto%\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea8355b5-e1e7-4d9f-8df5-d6cea2e105f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------+\n",
      "|species|substring(species, -4, 3)|\n",
      "+-------+-------------------------+\n",
      "| setosa|                      tos|\n",
      "| setosa|                      tos|\n",
      "| setosa|                      tos|\n",
      "+-------+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('species',df.species.substr(-4,3)).show(2) # string lấy substring từ -4, 3 ký tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1fceeee-75b4-4140-b3cc-87b569b2dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|_c0|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  0|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  1|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|_c0|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  0|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  1|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df.species.isin('setosa','versicolor')].show(2) # isin\n",
    "df[df.species.startswith('s')].show(2) # startswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b563f82c-e095-4ef2-b6cf-bc90a25d0861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+\n",
      "|_c0|sepal_length|sepal_width|\n",
      "+---+------------+-----------+\n",
      "|  0|         5.1|        3.5|\n",
      "|  1|         4.9|        3.0|\n",
      "+---+------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns[:3]).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "265734b0-6dbe-4d34-97e5-323e7117e4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.Row'>\n",
      "virginica\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "a = df.where('sepal_length > 6').collect()\n",
    "print(type(a[0]))\n",
    "print(a[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54819e7d-3a87-4e14-a312-b0fca7a6586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|   species|   sepal_length_list|\n",
      "+----------+--------------------+\n",
      "| virginica|[6.3, 5.8, 7.1, 6...|\n",
      "|versicolor|[7.0, 6.4, 6.9, 5...|\n",
      "|    setosa|[5.1, 4.9, 4.7, 4...|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+\n",
      "|      col1|\n",
      "+----------+\n",
      "|[5.8, 7.1]|\n",
      "|[6.4, 6.9]|\n",
      "|[4.9, 4.7]|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slice\n",
    "from pyspark.sql.functions import collect_list, slice\n",
    "df2 = df.groupBy('species').agg(collect_list(\"sepal_length\").alias(\"sepal_length_list\"))\n",
    "df2.show()\n",
    "df2.select(slice(df2.sepal_length_list,2,2).alias('col1')).show() # slice(self, start, leght)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9780916-d007-4f0e-8bdb-dc0802456651",
   "metadata": {},
   "source": [
    "## Apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4acca116-89f2-4831-818d-8c0743a9c84c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+---+\n",
      "|  a|  b|      c|         d|                  e|age|\n",
      "+---+---+-------+----------+-------------------+---+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|  2|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|  3|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|  4|\n",
      "+---+---+-------+----------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the APIs in a pandas Series within Python native function\n",
    "\n",
    "import pandas\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "\n",
    "df.select(\"*\",pandas_plus_one('a').alias('age')).show() # select * rename columns as 'age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b936f88-0bd2-46d1-bd06-c2feab165eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|percent|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|   20.0|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|   30.0|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|   50.0|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.show(3)\n",
    "df.withColumn('percent',expr(\"b/10*100\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a1f12-cded-438f-8ac8-946b40374fa9",
   "metadata": {},
   "source": [
    "## Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c401388-87a6-4822-a4e6-afb3581671e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col as scol, sum as ssum, avg as savg, max as smax, count as scount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f07eeb2-bdfe-44f5-b44d-d49bb8088320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ff071a4-8bfe-47d9-ab2f-b5b6027ebfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby color with mean\n",
    "df.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebb1c620-4752-4bfc-93e6-2f0e06a2a6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+-------+\n",
      "|color| fruit|sum(v1)|sum(v2)|\n",
      "+-----+------+-------+-------+\n",
      "|  red|banana|      8|     80|\n",
      "| blue|banana|      2|     20|\n",
      "|  red|carrot|      8|     80|\n",
      "| blue| grape|      4|     40|\n",
      "|black|carrot|      6|     60|\n",
      "|  red| grape|      8|     80|\n",
      "+-----+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"color\",\"fruit\").sum(\"v1\",\"v2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fbe80d0-c010-4337-95d9-3b70e0884819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+\n",
      "|color|avg_v1|sum_v2|max_v2|\n",
      "+-----+------+------+------+\n",
      "|  red|   4.8|   240|    80|\n",
      "| blue|   3.0|    60|    40|\n",
      "|black|   6.0|    60|    60|\n",
      "+-----+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"color\").agg(savg(\"v1\").alias(\"avg_v1\"), ssum(\"v2\").alias(\"sum_v2\"), smax(\"v2\").alias(\"max_v2\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc334d78-ef6d-4ebd-a5b0-e51c27c894d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+\n",
      "|color|avg_v1|sum_v2|max_v2|\n",
      "+-----+------+------+------+\n",
      "|red  |4.8   |240   |80    |\n",
      "|black|6.0   |60    |60    |\n",
      "+-----+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tuong tu having sql\n",
    "df.groupBy(\"color\")\\\n",
    ".agg(savg(\"v1\").alias(\"avg_v1\"), \n",
    "     ssum(\"v2\").alias(\"sum_v2\"), \n",
    "     smax(\"v2\").alias(\"max_v2\") )\\\n",
    ".where(scol(\"max_v2\") >= 50) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05bdf4df-3f68-4e3f-bdb3-6a8c7af75b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+---------+\n",
      "|color|v1_groupby_color|v1_custom|\n",
      "+-----+----------------+---------+\n",
      "|  red| [1, 3, 5, 7, 8]|     null|\n",
      "| blue|          [2, 4]|     null|\n",
      "|black|             [6]|     null|\n",
      "+-----+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# custom function for groupby\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "@F.udf(returnType=FloatType()) # define return float\n",
    "def custom_func(x):\n",
    "    return x[0] - x[-1]\n",
    "\n",
    "df.groupby('color')\\\n",
    ".agg(F.collect_list(\"v1\").alias(\"v1_groupby_color\"))\\\n",
    ".withColumn('v1_custom', custom_func('v1_groupby_color'))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "453567dc-bf19-414f-830b-4d707f1850f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| v1|v1_squared|\n",
      "+---+----------+\n",
      "|  1|       1.0|\n",
      "|  2|       4.0|\n",
      "|  3|       9.0|\n",
      "|  4|      16.0|\n",
      "|  5|      25.0|\n",
      "|  6|      36.0|\n",
      "|  7|      49.0|\n",
      "|  8|      64.0|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# udf\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType = FloatType())\n",
    "def square_float(x):\n",
    "    return float(x**2)\n",
    "\n",
    "df.select('v1',square_float('v1').alias('v1_squared')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60623203-dfc1-4a81-930d-d2ed84948b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|color|avg_min|\n",
      "+-----+-------+\n",
      "|black|   33.0|\n",
      "| blue|   11.0|\n",
      "|  red|    5.5|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multi columns in groupby\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "@pandas_udf(DoubleType(), functionType=PandasUDFType.GROUPED_AGG)\n",
    "def f(x, y):\n",
    "    return np.mean([np.min(x), np.min(y)])\n",
    "\n",
    "df.groupBy(\"color\").agg(f(\"v1\", \"v2\").alias(\"avg_min\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00c40abf-3ca0-456d-b222-9b131aa8ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+-----------+\n",
      "|color| fruit| v1| v2|        v1_|\n",
      "+-----+------+---+---+-----------+\n",
      "|black|carrot|  6| 60|       null|\n",
      "| blue|banana|  2| 20|-0.70710677|\n",
      "| blue| grape|  4| 40| 0.70710677|\n",
      "|  red|banana|  1| 10| -1.3270175|\n",
      "|  red|carrot|  3| 30|-0.62858725|\n",
      "|  red|carrot|  5| 50| 0.06984303|\n",
      "|  red|banana|  7| 70| 0.76827335|\n",
      "|  red| grape|  8| 80|  1.1174885|\n",
      "+-----+------+---+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"color string, fruit string, v1 bigint, v2 bigint, v1_ float\",functionType = PandasUDFType.GROUPED_MAP)  \n",
    "def normalize(df):\n",
    "    v1 = df.v1\n",
    "    return df.assign(v1_=(v1 - v1.mean()) / v1.std())\n",
    "\n",
    "df.groupby(\"color\").apply(normalize).show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14880e-3947-4241-91d7-4868760d1f3f",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66c0a7d0-09ee-4bc0-90a4-8a1c3e9b9fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b        c           d                   e\n",
       "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
       "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
       "2  4  5.0  string3  2000-03-01 2000-01-03 12:00:00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set df table name\n",
    "df.createOrReplaceTempView('table1')\n",
    "spark.sql('select * from table1').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import sqlContext\n",
    "sqlContext.registerDataFrameAsTable(df, \"df_table\")\n",
    "\n",
    "median_rating = sqlContext.sql(\"\"\"\n",
    "    SELECT percentile(v2, 0.5) AS median_rating \n",
    "    FROM df_table\n",
    "\"\"\").first()[\"median_rating\"]\n",
    "\n",
    "print(\"Median rating:\", median_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd1543e4-50f3-4b33-8442-2f683d92f765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "sqlTrans = SQLTransformer(statement='select * from __THIS__')\n",
    "sqlTrans.transform(df).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f64156b-ffd8-4e43-80f3-4a61e4759cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a253cb5-8eb8-4992-87ce-f2e1cafea41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.conf import SparkConf\n",
    "# conf = SparkConf()  # create the configuration\n",
    "# conf.set(\"spark.jars\", r\"C:\\Spark_env\\ojdbc11.jar\")  # set the spark.jars\n",
    "driver =\"oracle.jdbc.driver.OracleDriver\"# 'oracle.jdbc.driver.OracleDriver'\n",
    "url = 'jdbc:oracle:thin:@192.168.18.32:1521/DTT'\n",
    "user = 'datkt'\n",
    "password = 'hct5Kg'\n",
    "table = 'DTT_SD.IRIS'\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba516eb-3297-4afd-8a8c-3465c30460b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
