{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8664f6a0-a5af-433a-a326-65ac89015723",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "[How to Monitor Your Models in Production](https://neptune.ai/blog/how-to-monitor-your-models-in-production-guide)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76bc352b-3102-44fb-a35b-1902ecb8da56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Goal of monitoring step\n",
    "\n",
    "<img src = \"_images/01o_model_degrade.png\">\n",
    "\n",
    "\n",
    "**Cause:**\n",
    "- Machine learning models degrade over time. They’re dynamic and sensitive to real changes in the real world\n",
    "- Validation result during development will seldom fully show the model's performance in production\n",
    "- The difference of evironment between development and production, may be caused of the difference of performance\n",
    "\n",
    "**So, the Goal of monitoring model:**\n",
    "- To detect problems with your model and the system serving your model in production before they start to generate negative business value,\n",
    "- To take action by triaging and troubleshooting models in production or the inputs and systems that enable them,\n",
    "- To ensure their predictions and results can be explained and reported, \n",
    "- To ensure the model’s prediction process is transparent to relevant stakeholders for proper governance, \n",
    "- Finally, to provide a path for maintaining and improving the model in production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08a824cd-4b59-4616-825d-0f4eaa790dcd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Criteria of metrics selection\n",
    "\n",
    "It's very unique for each business case, depend on:\n",
    "- What does your business define as success and the KPIs that were set in business analysis phase ?\n",
    "- What were the performance expectation or result's distribution expectation before deploying to production ?\n",
    "\n",
    "**Criteria of metrics selection to make sense and be comfortable**\n",
    "- Availabel to compare across models\n",
    "- Simple and easy to understand\n",
    "- Can be collected/computed in real-time\n",
    "- Allows to set threshold for actionable alerting on problems.\n",
    "\n",
    "For example of metrics that use for building a loan approval system:\n",
    "1. What is the accuracy of model prediction that pay back at stipulated time ? __(Functional level monitoring)__\n",
    "2. How fast does model score is returned after get the request from client ? __(Operational level monitoring)__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa98df98-845e-4498-9884-9d2a8c0c92ad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Type of monitoring\n",
    "\n",
    "1. __Functional level monitoring__\n",
    "- Data (input data level)\n",
    "    - Data quality issues\n",
    "    - Data/feature drift\n",
    "    - Outliers\n",
    "- Model\n",
    "    - Monitoring model drift\n",
    "    - Model configuration and artifacts\n",
    "    - Model versions\n",
    "    - Concerted adversaries\n",
    "- Predictions (Output)\n",
    "    - Model evaluation metrics\n",
    "        - When availabel have Y true (the ground truth) ?\n",
    "        - When not availabel have Y true ?\n",
    "        \n",
    "2. __Operational level monitoring__\n",
    "- System performance monitoring for ML models in production\n",
    "    - System performance metrics\n",
    "        - CPU/GPU utilization\n",
    "        - Memory utilization\n",
    "        - Total number of `failed request`\n",
    "        - Total number of `API calls`\n",
    "        - Responce time\n",
    "    - System reliability\n",
    "- Pipelines\n",
    "    - Data pipelines\n",
    "    - Model pipeline\n",
    "- Cost\n",
    "    \n",
    "__Challenges might be met when monitoring__\n",
    "- At input level:\n",
    "    - Data sources in production may be scattered and unreliable\n",
    "    - Do not have clear data requirements\n",
    "    - Data sources don’t have defined ownership\n",
    "    - Metadata for your production data workflow is not discoverable\n",
    "- Teamwork\n",
    "- Model quality\n",
    "    - Ground truth (y_true) are not availability\n",
    "    - Model bias\n",
    "    - Blackbox model\n",
    "    - Tracking hyper-parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bc750db-dec8-4b3f-836b-6b59a506408f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stage of monitoring model\n",
    "\n",
    "<img src=\"_images/01o_essential.jpg\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e3451b7-5aad-46c5-b933-399479a2e3ca",
   "metadata": {},
   "source": [
    "### Level 0: training and deploying models manually\n",
    "\n",
    "<img src=\"_images/01o_lv0.png\">\n",
    "\n",
    "At this stage, you probably aren’t even thinking of monitoring your model yet, perhaps just finding a way to validate your model on the test set and hand it off to your IT Ops or software developers to deploy.\n",
    "\n",
    "I know because I was there. I celebrated when I handed it off, as mentioned at the beginning of this article, but as you know—a couple of months later—it has indeed ended in tears and on the hospital bed.\n",
    "\n",
    "For you to avoid this scenario, I propose you prioritize the lowest hanging fruit. Although less informative, and won’t help you monitor model performance, it can still serve as a reasonable performance proxy to tell you if your general application is working as intended. \n",
    "\n",
    "You don’t want to spend long hours focusing on monitoring your model’s metrics or try to justify its performance in line with a business KPI when your workflow is still in its manual deployment stage; such metrics will get easier to measure and analyze when your MLOps system gets mature, and you can collect ground truth labels or integrate other performance proxies in the absence of ground truth.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acca5078-1fb1-488e-bdb6-0f3365e28f86",
   "metadata": {},
   "source": [
    "\n",
    "### Level 1: continuous training of models\n",
    "\n",
    "<img src=\"_images/01o_lv1.png\">\n",
    "\n",
    "Being at this level means that you have automated the machine learning pipeline to enable continuous training of your machine learning models based on triggers that have been set by criteria or a defined threshold.\n",
    "\n",
    "At this stage, I reckon you focus more on monitoring:\n",
    "\n",
    "- The business metric used to gauge your model’s performance (see “What Could Go Right” section)—if it doesn’t turn out to be pretty difficult to measure, especially if you can’t spend them on getting ground truth for monitoring model metrics.\n",
    "- The properties of your production data and your model’s performance in production to detect model staleness and degradation; can help with continuous training through triggers that automate the ML production pipelines to retrain models with new production data.\n",
    "- Your model’s retraining process needs to log pipeline metadata, model configuration, and model metadata because you’re most likely going to manually deploy a retrained model, and you want to make sure you can monitor the properties of that model before redeploying it to production.\n",
    "- You also need to monitor your production pipeline health as retraining steps are automated, and your data pipeline validates and preprocesses data from one or more sources.\n",
    "- You should also start monitoring how much your continuous training process is incurring so you don’t wake up with a gigantic AWS bill one day that you or your company did not plan for.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "335c0957-43d3-4ae2-87a7-c1f9ccc1aa82",
   "metadata": {},
   "source": [
    "### Level 2: completely mature in your MLOps\n",
    "\n",
    "<img src=\"_images/01o_lv2.png\">\n",
    "\n",
    "Being at this level indicates that you’re completely mature in your MLOps implementation and pretty much the entire pipeline is a robust, automated CI/CD system. Your training, validation, and deployment phases are all automated in a complimentary feedback loop.\n",
    "\n",
    "At this stage, you should pretty much monitor everything but your team’s focus should be on the more informative metrics, making sure that all the relevant stakeholders are empowered with the more informative metrics before spending more time on the least informative metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90c0e4ab-6603-48bd-9ada-0a38a0d062ac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Best practices for monitoring\n",
    "\n",
    "__General monitoring best practices__\n",
    "\n",
    "- ___Focus on people first___. If you build a culture where data is also treated as the product in your organization, people will most likely be inclined to take ownership of the product to ensure it serves its intended purpose end-to-end. You can learn a lot from DevOps cultural change.\n",
    "- If it’s possible, don’t give the application’s “monitoring power” to one person. If you have a cross-functional team of data professionals and Ops engineers, let everyone handle their service and communicate effectively. This will help decentralize knowledge and know-how and when the use cases scale, no one will be overwhelmed.\n",
    "- Take a lean approach; using too many tools can be very tasking. Centralize your tools but decentralize the team; everyone staying on top of a task.\n",
    "- Monitoring doesn’t start after deployment, it starts when you begin experimentation. Build a culture of monitoring right from the model development stage (monitoring model experimentation metrics, logs, and so on).\n",
    "- Always consider what’s optimal for the productivity of your team when you encounter any crucial decision-making point.\n",
    "- Encourage your team to properly document their troubleshooting framework and create a framework for going from alerting to action to troubleshooting for effective model maintenance.\n",
    "\n",
    "__Best practices for data monitoring__\n",
    "\n",
    "- Batch and streaming data should be processed in the same manner, using the same pipeline so that issues with the data pipeline are a lot more intuitive to troubleshoot.\n",
    "- Ensure you go beyond checking for the drift for an entire dataset and look gradually at the feature drift as that can provide more insights.\n",
    "- Invest in a global data catalog that can help log high-quality metadata for your data that every user (your data and ML team) can rely on; it will help you tackle - challenges with streaming and maintaining reliable data quality. It will also make lineage tracking easier.\n",
    "- Perform a pre-launch validation on your evaluation set before moving your model to production to establish a baseline performance.\n",
    "\n",
    "__Best practices for model monitoring__\n",
    "\n",
    "- Model performance will inevitably degrade over time, but beware of a big dip in performance which is often indicative of something wrong—you can select tools that detect this automatically.\n",
    "- Perform shadow deployment and testing with the challenger model vs the champion model and log the predictions so that performance on the new model can be tracked alongside the current model in production; before you decide to deploy the newly trained (challenger) model.\n",
    "- You can use a metadata store (like Neptune.ai) to store hyperparameters for models that have been versioned and retrained in production; this improves auditing, compliance, lineage traceability, and troubleshooting. \n",
    "\n",
    "__Best practices for monitoring predictions/output__\n",
    "\n",
    "- Prediction drift can be a good performance proxy for model metrics, especially when ground truth isn’t available to collect, but it shouldn’t be used as the sole metric.\n",
    "- Track unreasonable outputs from your model. For example, your classification model predicting the wrong class for a set of inputs with a high confidence score, or your regression model predicting a negative score (when the base metric score should be 0) for a given set of features. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b8eeb54-a9a8-4244-b0db-858b0b4f390f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bonus contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04d8011c-5ea3-4a52-8de5-cd30ea53d53a",
   "metadata": {},
   "source": [
    "### Monitoring vs Observability\n",
    "\n",
    "[Comparation of two](https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/#monitoring-vs-observability)\n",
    "\n",
    "__Observability__ is your ability to look at the metrics you’ve been monitoring and perform root-cause analysis on them to understand why they are a certain way, and what threat they pose to the overall performance of your system—all to improve system quality. \n",
    "\n",
    "__Monitoring__ is pretty much everything that happens before observability:\n",
    "- Collecting performance metrics, \n",
    "- tracking them, \n",
    "- detecting potential problems, \n",
    "- alerting the right user. \n",
    "\n",
    "__To put it simply, you can monitor without observing, but can’t observe your system’s overall performance without monitoring it. Monitoring is about collecting the dots, observability is about connecting them!__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf2f0d18-810c-472e-885b-a2389a03218f",
   "metadata": {},
   "source": [
    "### Setting alerts the right way\n",
    "\n",
    "- Test your alerts before they go into production\n",
    "- Monitor the primary metrics as concluded in your needs analysis.\n",
    "- Agree on the media for the alert, so every service owner is comfortable with their medium (email, stack,...)\n",
    "- Send context to the alert by including descriptive information and action by the primary service owner.\n",
    "- Make sure to set up a feedback loop that makes your monitoring better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9aa5b1a-2c74-4388-9a89-113ee67f86e1",
   "metadata": {},
   "source": [
    "### Write log everything\n",
    "[Read more](https://neptune.ai/blog/how-to-monitor-your-models-in-production-guide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
