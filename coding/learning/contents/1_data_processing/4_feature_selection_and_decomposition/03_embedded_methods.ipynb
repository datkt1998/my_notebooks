{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f9cca86-c332-4218-8bc9-f886e66b87fe",
   "metadata": {},
   "source": [
    "# Embedded methods\n",
    "- Perform the feature selection as a part of ML process by considering the interation of model and subset feature.\n",
    "\n",
    "**1. Advantages**:\n",
    "- Use less computationally expensive than `wrapped` method because this fit model only 1 time by the importance features in these algorithms.\n",
    "- More accurate than `filter` methods\n",
    "- Detect the interactions between features\n",
    "\n",
    "**2. Disavantages**:\n",
    "- Constrained to the limitations of the choosed algorithm.\n",
    "\n",
    "**3. Usage**:\n",
    "- **Typically, the embedded is choosed at the time of selecting features**\n",
    "\n",
    "**4. Produce steps**:\n",
    "- Train a ML model\n",
    "- Derive the feature importance then remove the non-importance features\n",
    "\n",
    "**5. Algorithms ?**:\n",
    "- Lasso\n",
    "- Tree importance\n",
    "- Regression coefficients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c61a63c5-7f69-4543-b1e3-d3f69bcee264",
   "metadata": {},
   "source": [
    "## Linear coefficients\n",
    "- Base on Simple model (`LinearRegression`, `LogisticRegression`, `RandomForestClassifier`, `RandomForestRegressor`) to evaluate the importance of the variables, then select\n",
    "- **The magnitude of the coefficients is directly influenced by the scale of the features**. Therefore, to compare coefficients across features, it is important that all features are on a similar scale. This is why normalisation is important for variable importance and feature selection in linear models.\n",
    "\n",
    "**Linear Regression assumptions:**\n",
    "- There is a linear relationship betweent the predictors Xs and the outcome Y\n",
    "- The residuals follow a normal distribution centered at 0\n",
    "- There is little or no multicollinearity among predictors (Xs should not be linearly related to one another)\n",
    "- Homoscedasticity (variance should be the same)\n",
    "\n",
    "**Advantages**\n",
    "- The ability to assess the importance of variables quite accurately\n",
    "- Useful to interpret the output of the model\n",
    "\n",
    "**Disadvantage**\n",
    "- Need to feature transform and scaling process to met the linear model assumption. There are a lot of assumptions that need to be met in order to make a fair comparison of the features by using only their regression coefficients.\n",
    "- When regularisation applies a penalty on the coefficients, different value of penalty may be got the different of subset features because regularisation masks the true relationship between the predictor X and the outcome Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "458edd22-08d8-4382-8162-9825ac2dedc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;ft_slmodel&#x27;,\n",
       "                 SelectFromModel(estimator=LogisticRegression(C=1000,\n",
       "                                                              max_iter=300,\n",
       "                                                              random_state=10)))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;ft_slmodel&#x27;,\n",
       "                 SelectFromModel(estimator=LogisticRegression(C=1000,\n",
       "                                                              max_iter=300,\n",
       "                                                              random_state=10)))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ft_slmodel: SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=LogisticRegression(C=1000, max_iter=300,\n",
       "                                             random_state=10))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1000, max_iter=300, random_state=10)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1000, max_iter=300, random_state=10)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('ft_slmodel',\n",
       "                 SelectFromModel(estimator=LogisticRegression(C=1000,\n",
       "                                                              max_iter=300,\n",
       "                                                              random_state=10)))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "# remember that here I want to evaluate the coefficient magnitude itself and not whether lasso shrinks coefficients to zero\n",
    "# ideally, I want to avoid regularisation at all, so the coefficients are not affected (modified) by the penalty of the regularisation\n",
    "# In order to do this in sklearn, I set the parameter C really high which is basically like fitting a non-regularised logistic regression\n",
    "# Then I use the selectFromModel object from sklearn to automatically select the features\n",
    "# set C to 1000, to avoid regularisation\n",
    "\n",
    "lr = LogisticRegression(C=1000, penalty='l2', max_iter=300, random_state=10)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ft_slmodel', SelectFromModel(estimator = lr)),\n",
    "        ])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959a4d81-0f21-4f14-be2c-25ed597f2eec",
   "metadata": {},
   "source": [
    "Keep in mind that increasing the penalisation will increase the number of features removed. Therefore, you will need to keep an eye and monitor the final model performance to ensure that you don't set a penalty too high so it removes a lot of features, or too low, and thus useless features are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1a86c83e-d4fa-4194-ad01-f2abee54aa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_3', 'var_11', 'var_21', 'var_23', 'var_24', 'var_26', 'var_32',\n",
       "       'var_33', 'var_39', 'var_40', 'var_48', 'var_50', 'var_52', 'var_55',\n",
       "       'var_56', 'var_60', 'var_63', 'var_69', 'var_70', 'var_72', 'var_74',\n",
       "       'var_77', 'var_80', 'var_83', 'var_84', 'var_88', 'var_89', 'var_91',\n",
       "       'var_93', 'var_98', 'var_100', 'var_102', 'var_106'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command let's me visualise those features that were kept.\n",
    "# sklearn will select those features which coefficients are greater than the mean of all the coefficients.\n",
    "# it compares absolute values of coefficients. More on this in a second.\n",
    "selected_feat = X_train.columns[pipe.named_steps['ft_slmodel'].get_support()]\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b47861fd-1386-485a-9343-427bab366796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 108\n",
      "selected features: 33\n",
      "features with coefficients greater than the mean coefficient: 33\n"
     ]
    }
   ],
   "source": [
    "# and now, let's compare the  number of selected features\n",
    "# with the number of features which coefficient is above the\n",
    "# mean coefficient, to make sure we understand the output of\n",
    "# SelectFromModel\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "\n",
    "print(\n",
    "    'features with coefficients greater than the mean coefficient: {}'.format(\n",
    "        np.sum(\n",
    "            np.abs(pipe.named_steps['ft_slmodel'].estimator_.coef_) > np.abs(\n",
    "                pipe.named_steps['ft_slmodel'].estimator_.coef_).mean())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec81eca9-cdf1-4019-af05-9c9f30f925f9",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "- Regularisation consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and avoid overfitting. In linear model regularization, the penalty is applied to the coefficients that multiply each of the predictors. **The Lasso regularization or l1 has the property that is able to shrink some of the coefficients to zero. Therefore, those features can be removed from the model.**\n",
    "- Keep in mind that increasing the penalisation will increase the number of features removed. Therefore, you will need to keep an eye and monitor the final model performance to ensure that you don't set a penalty too high so it removes a lot of features, or too low, and thus useless features are retained.\n",
    "- `Ridge regularisation` does not shrink coefficients to zero, only `Lasso`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01f977-35d3-4bfd-b50c-0948b0b978cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logistic allow `penalty`\n",
    "sel_ = SelectFromModel(\n",
    "    LogisticRegression(C=0.5, penalty='l1', solver='liblinear', random_state=10)\n",
    ")\n",
    "\n",
    "# Then I use the selectFromModel class from sklearn, which\n",
    "# will select the features which coefficients are non-zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67caa14-9acf-4b85-8e51-7432cabee392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear regression does not allow `penalty` parameter, need to import `Lasso`\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# here, again I will train a Lasso Linear regression and select\n",
    "# the non zero features in one line.\n",
    "\n",
    "# bear in mind that the linear regression object from sklearn does\n",
    "# not allow for regularisation. So If you want to make a regularised\n",
    "# linear regression you need to import specifically \"Lasso\"\n",
    "\n",
    "# alpha is the penalisation, so I set it high\n",
    "# to force the algorithm to shrink some coefficients\n",
    "\n",
    "sel_ = SelectFromModel(Lasso(alpha=100, random_state=10))\n",
    "sel_.fit(scaler.transform(X_train), y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f25ff31f-7a24-4e33-a61d-7c7bd511a864",
   "metadata": {},
   "source": [
    "## Tree importance\n",
    "\n",
    "The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease elicited by each feature is averaged across trees to determine the final importance of the variable.\n",
    "\n",
    "In general, features that are selected at the top of the trees are more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.\n",
    "\n",
    "**Advantages**: Very straightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if you are going to build tree methods.\n",
    "\n",
    "**Note**\n",
    "- Random Forests and decision trees in general give preference to features with high cardinality\n",
    "- Correlated features will be given equal or similar importance, but overall reduced importance compared to the same tree built without correlated counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a915611d-2986-49b0-a1d4-4f910e5c4543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SelectFromModel(estimator=RandomForestClassifier(n_estimators=10,\n",
       "                                                 random_state=10))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=RandomForestClassifier(n_estimators=10,\n",
       "                                                 random_state=10))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=10, random_state=10)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=10, random_state=10)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(n_estimators=10,\n",
       "                                                 random_state=10))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# we fit Random Forests and select features in 2 lines of code\n",
    "\n",
    "# first I specify the Random Forest instance and its parameters\n",
    "\n",
    "# Then I use the selectFromModel class from sklearn\n",
    "# to automatically select the features\n",
    "\n",
    "# SelectFrom model will select those features which importance\n",
    "# is greater than the mean importance of all the features\n",
    "# by default, but you can alter this threshold if you want to\n",
    "\n",
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=10, random_state=10))\n",
    "\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "51b73728-b39d-4994-aa63-2cc570a3114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 108\n",
      "selected features: 27\n",
      "features with importance greater than the mean importance of all features: 27\n"
     ]
    }
   ],
   "source": [
    "# and now, let's compare the  amount of selected features\n",
    "# with the amount of features which importance is above the\n",
    "# mean of all features, to make sure we understand the output of\n",
    "# SelectFromModel\n",
    "\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "\n",
    "print(\n",
    "    'features with importance greater than the mean importance of all features: {}'.format(\n",
    "        np.sum(sel_.estimator_.feature_importances_ >\n",
    "               sel_.estimator_.feature_importances_.mean())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98102511-7752-432a-830e-91b5d0942028",
   "metadata": {},
   "source": [
    "Selecting features by using tree derived feature importance is a very straightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if you are going to build tree methods.\n",
    "\n",
    "However, as I said, correlated features will show in a tree similar importance, but lower than compared to what their importance would be if the tree was built without the correlated counterparts.\n",
    "\n",
    "In situations like this, it is better to select features recursively, rather than altogether like we are doing in this lecture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1205ef1-3459-4e6b-98e0-7e30291c4371",
   "metadata": {},
   "source": [
    "### Recursively tree importance\n",
    "Random Forests assign equal or similar importance to features that are highly correlated. In addition, when features are correlated, the importance assigned is lower than the importance attributed to the feature itself, should the tree be built without the correlated counterparts.\n",
    "\n",
    "Therefore, instead of eliminating features based on importance by brute force like we did in the previous notebook, we could get a better selection by removing one feature at a time, and recalculating the importance on each round. This procedure is called `Recursive Feature Elimination` (`RFE`)\n",
    "\n",
    "RFE is a hybrid between embedded and wrapper methods: it is based on computation derived when fitting the model, but it also requires fitting several models.\n",
    "\n",
    "The cycle is as follows:\n",
    "\n",
    "- Build Random Forests using all features\n",
    "- Remove least important feature\n",
    "- Build Random Forests and recalculate importance\n",
    "- Repeat until a criteria is met\n",
    "\n",
    "In this situation, when a feature that is highly correlated to another one is removed, then, the importance of the remaining feature increases. This may lead to a better feature space selection. On the downside, building several Random Forests is quite time and compute resource consuming, in particular if the dataset contains a high number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d15853-4bb5-4daa-8ad6-fcb02304dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do model training and feature selection in 2 lines of code\n",
    "\n",
    "# first I specify the Random Forest and its parameters\n",
    "\n",
    "# Then RFE from sklearn to remove features recursively\n",
    "\n",
    "# RFE will remove one feature at each iteration => the least  important.\n",
    "# then it will build another random forest and repeat\n",
    "# till a criteria is met.\n",
    "\n",
    "# in sklearn the criteria to stop is an arbitrary number\n",
    "# of features to select, that we need to decide before hand\n",
    "# not the best solution, but a solution\n",
    "\n",
    "sel_ = RFE(RandomForestClassifier(n_estimators=10, random_state=10), n_features_to_select=27)\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e38b1e3-bcd6-424d-8df7-d2301e4da471",
   "metadata": {},
   "source": [
    "In my opinion the RFE from sklearn does not bring forward a massive advantage respect to the SelectFromModel method and personally I tend to use the RFE to select my features if we are not sure about the correlation between features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
