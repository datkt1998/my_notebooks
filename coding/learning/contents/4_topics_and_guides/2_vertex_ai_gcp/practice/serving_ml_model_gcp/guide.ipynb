{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve model deploy to vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new virtual environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create a virtual environment\n",
    "python -m venv .venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "# Source .venv/bin/activate\n",
    ".venv\\Scripts\\activate\n",
    "\n",
    "# (Optional) Deactivate conda environment\n",
    "conda deactivate\n",
    "\n",
    "# Upgrade pip\n",
    "python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a `requirements.txt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "uvicorn[standard]==0.20.0\n",
    "gunicorn==23.0.0\n",
    "fastapi[standard]==0.115.0\n",
    "scikit-learn==1.5.2\n",
    "pytest==8.3.3\n",
    "starlette==0.38.6\n",
    "requests==2.32.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import random\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SimpleSentimentModel(BaseEstimator, TransformerMixin):\n",
    "    negative_length_threshold = 10\n",
    "    positive_length_threshold = 30\n",
    "    negative_ls = [\"tiêu cực\", \"xấu\", \"tệ\", \"negative\"]\n",
    "    positive_ls = [\"tích cực\", \"thích\", \"positive\"]\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, text):\n",
    "        text_lower = text.lower()\n",
    "        if any(word in text_lower for word in self.negative_ls):\n",
    "            return \"negative\", random.randrange(90, 100, step=1) / 100\n",
    "        elif any(word in text_lower for word in self.positive_ls):\n",
    "            return \"positive\", random.randrange(90, 100, step=1) / 100\n",
    "        elif len(text) <= self.negative_length_threshold:\n",
    "            return \"negative\", random.randrange(70, 90, step=1) / 100\n",
    "        elif len(text) >= self.positive_length_threshold:\n",
    "            return \"positive\", random.randrange(70, 90, step=1) / 100\n",
    "        else:\n",
    "            return \"neutral\", random.randrange(70, 95, step=1) / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "from model import SimpleSentimentModel  # Ensure this import is correct\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the model\n",
    "    model = SimpleSentimentModel()\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    model_dir = \"models\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    # Save the model using joblib, ensuring correct context\n",
    "    joblib.dump(model, os.path.join(model_dir, \"model.pkl\"))\n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main app API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import joblib\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from model import SimpleSentimentModel  # noqa: F401\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Sentiment Analysis API\")\n",
    "\n",
    "# Load the model with a safe file path\n",
    "model_path = os.path.join(\"models\", \"model.pkl\")\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "# Load the model, making sure SimpleSentimentModel is already imported\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# Pydantic models for prediction results\n",
    "class Prediction(BaseModel):\n",
    "    sentiment: str\n",
    "    confidence: Optional[float]\n",
    "\n",
    "\n",
    "class Predictions(BaseModel):\n",
    "    predictions: List[Prediction]\n",
    "\n",
    "\n",
    "# Function to process batch predictions\n",
    "def get_prediction(instances):\n",
    "    res = []\n",
    "    for text in instances:\n",
    "        sentiment, confidence = model.predict(text)\n",
    "        res.append(Prediction(sentiment=sentiment, confidence=confidence))\n",
    "    return Predictions(predictions=res)\n",
    "\n",
    "\n",
    "# Health check route\n",
    "@app.get(\"/health\", status_code=200)\n",
    "async def health():\n",
    "    return {\"health\": \"ok\"}\n",
    "\n",
    "\n",
    "# Prediction route to handle batch requests\n",
    "@app.post(\n",
    "    \"/predict\",\n",
    "    response_model=Predictions,\n",
    "    response_model_exclude_unset=True,\n",
    ")\n",
    "async def predict(request: Request):\n",
    "    # Extract the JSON body from the request\n",
    "    body = await request.json()\n",
    "\n",
    "    # Validate the request body\n",
    "    if \"instances\" not in body or not isinstance(body[\"instances\"], list):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"Invalid input format. 'instances' should be a list of texts.\",\n",
    "        )\n",
    "\n",
    "    # Extract the instances (texts) from the request\n",
    "    instances = [x[\"text\"] for x in body[\"instances\"]]\n",
    "\n",
    "    # Get predictions\n",
    "    output = get_prediction(instances)\n",
    "\n",
    "    # Return the predictions\n",
    "    return output\n",
    "\n",
    "\n",
    "# Main function to run the FastAPI app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8080)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test app**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "from main import app\n",
    "\n",
    "client = TestClient(app=app)\n",
    "base_url = \"\"\n",
    "\n",
    "\n",
    "def test_health():\n",
    "    response = client.get(f\"{base_url}/health\")\n",
    "    assert response.status_code == 200\n",
    "    assert response.json() == {\"health\": \"ok\"}\n",
    "    print(\"pass: test_health\")\n",
    "\n",
    "\n",
    "def test_predict_item():\n",
    "    response = client.post(\n",
    "        f\"{base_url}/predict\",\n",
    "        json={\n",
    "            \"instances\": [\n",
    "                {\"text\": \"Cong hoa xa hoi chu nghia\"},\n",
    "                {\"text\": \"doc lap\"},\n",
    "                {\"text\": \"doc lap tich cuc\"},\n",
    "                {\"text\": \"te doc\"},\n",
    "                {\"text\": \"positive doc lap\"},\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "    result = response.json()\n",
    "    sentiments = [i[\"sentiment\"] for i in result[\"predictions\"]]\n",
    "    assert sentiments == [\n",
    "        \"neutral\",\n",
    "        \"negative\",\n",
    "        \"neutral\",\n",
    "        \"negative\",\n",
    "        \"positive\",\n",
    "    ]\n",
    "    print(\"pass: test_predict_item\")\n",
    "\n",
    "\n",
    "def test_predict_item_non_instance():\n",
    "    response = client.post(\n",
    "        f\"{base_url}/predict\",\n",
    "        json={\n",
    "            \"instan\": [\n",
    "                {\"text\": \"Cong hoa xa hoi chu nghia\"},\n",
    "                {\"text\": \"doc lap\"},\n",
    "                {\"text\": \"doc lap tich cuc\"},\n",
    "                {\"text\": \"te doc\"},\n",
    "                {\"text\": \"positive doc lap\"},\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    assert response.status_code == 400\n",
    "    response.json() == {\n",
    "        \"detail\": \"Invalid input format. 'instances' should be a list of texts.\"\n",
    "    }\n",
    "    print(\"pass: test_predict_item_non_instance\")\n",
    "\n",
    "\n",
    "def test_predict_item_not_list():\n",
    "    response = client.post(\n",
    "        f\"{base_url}/predict\",\n",
    "        json={\"instan\": {\"text\": \"Cong hoa xa hoi chu nghia\"}},\n",
    "    )\n",
    "    assert response.status_code == 400\n",
    "    response.json() == {\n",
    "        \"detail\": \"Invalid input format. 'instances' should be a list of texts.\"\n",
    "    }\n",
    "    print(\"pass: test_predict_item_not_list\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test for running container\n",
    "    import requests\n",
    "\n",
    "    client = requests\n",
    "    base_url = \"http://127.0.0.1:8080\"\n",
    "    test_health()\n",
    "    test_predict_item()\n",
    "    test_predict_item_non_instance()\n",
    "    test_predict_item_not_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pytest in `cmd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pytest test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model image to Artifact Registry (GCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "**Write Dockerfile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM tiangolo/uvicorn-gunicorn:python3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY *.py ./\n",
    "COPY models ./models\n",
    "COPY requirements.txt ./requirements.txt\n",
    "\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r ./requirements.txt\n",
    "\n",
    "EXPOSE 8080\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Push image bằng docker-command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build docker image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker build -t asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test image container**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm -p 8080:8080 asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Push image to Artifact Registry (GCP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authen GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# docker login\n",
    "gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker push asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Push image bằng cloud-build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Config cloud build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "steps:\n",
    "# If training model in cloud and save model in GCS\n",
    "# Assume Storage location of model: `gs://dev-aiml-model/models/sentiment`\n",
    "# Download the model file in GCS to embed it into the image\n",
    "  - name: 'gcr.io/cloud-builders/gsutil'\n",
    "    args: ['cp', '-r', '${_MODEL_GCS_PATH}', './models']\n",
    "    id: 'download-model'\n",
    "  \n",
    "  # Build the container image\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    args: ['build', '-t', '${_IMAGE_NAME}', '.']\n",
    "    waitFor: ['download-model']\n",
    "  \n",
    "  # Push the container image to Artifact Registry\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    args: ['push', '${_IMAGE_NAME}']\n",
    "\n",
    "images:\n",
    "  - '${_IMAGE_NAME}'\n",
    "\n",
    "# Substitution variables for flexibility\n",
    "substitutions:\n",
    "  _MODEL_GCS_PATH: 'gs://dev-aiml-model/models/sentiment'\n",
    "  _IMAGE_NAME: 'asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run cloud build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud builds submit --config cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving model container - Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Container Requirement**\n",
    "\n",
    "The docker container needs to follow the [container requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) defined by Google. The most important requirement is: \n",
    "\n",
    "---\n",
    "**1. HTTP server**\n",
    "\n",
    "Provide an `HTTP server` that listens for requests on `0.0.0.0` (must) on port `8080` (can be choice).\n",
    "\n",
    "**HTTP Server** can be using:\n",
    "- **Flask** , **FastAPI**, ...\n",
    "- **TensorFlow Serving**, **TorchServe**, or **KServe Python Server**\n",
    "- ...\n",
    "\n",
    "[**HTTP Server** can be run by](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#server):\n",
    "- [ENTRYPOINT instruction](https://docs.docker.com/engine/reference/builder/#entrypoint), [CMD instruction](https://docs.docker.com/engine/reference/builder/#cmd) or both in ***Dockerfile***\n",
    "- Specify the `containerSpec.command` and `containerSpec.args` fields when you create your `Model` resource (override your container image's `ENTRYPOINT` and `CMD`)\n",
    "\n",
    "---\n",
    "**2. Health checks**\n",
    "\n",
    "***a. startup probe*** (optional)\n",
    "\n",
    "Check whether the container application has started. Nếu không cung cấp thì sẽ ko chạy, và ngay lập tức chạy ***health probe***\n",
    "\n",
    "**Usecase**: Cần sử dụng cho các application cần có thời gian khởi động trong lần đầu tiên. Ví dụ, Nếu App cần thời gian để copy file model mới từ source bên ngoài container mỗi lần khởi động. Chúng ta có thể config ***startup probe*** để chờ cho đến khi việc copy hoàn thành và trả ra success\n",
    "\n",
    "\n",
    "***b. health probe***\n",
    "\n",
    "Check whether the container application is ready to accept traffic or receive request. Nếu không cung cấp path cụ thể thì Vertex sẽ sử dụng default path `/health`. Lưu ý là ***health probe*** chỉ chạy khi ***startup probe*** hoàn thành hoặc không được khai báo\n",
    "\n",
    "Provide an `HTTP path` for **health checks** (default path `/health` with `HTTP GET`, it can be change in config): \n",
    "- Return a `200` within **10 seconds** after call when you’re container is ready to handle requests. Nội dung của phần phản hồi không quan trọng, vì Vertex AI sẽ bỏ qua chúng. Phản hồi này cho thấy rằng server đang hoạt động tốt (healthy). For example, if you need to load the model, ensure you return the `200` status code after the model is loaded.\n",
    "- **If the server isn't ready to handle prediction requests**, nó không nên phản hồi yêu cầu trong vòng **10 giây**, hoặc phản hồi với bất kỳ mã trạng thái nào khác ngoài `200 OK`, ví dụ như `503 Service Unavailable`. Điều này cho thấy server đang không hoạt động tốt (unhealthy).\n",
    "\n",
    "Nếu health probe nhận được phản hồi không tốt từ server (bao gồm cả trường hợp không có phản hồi trong vòng 10 giây), nó sẽ gửi thêm **tối đa 3 lần Health Checks nữa**, mỗi lần cách nhau **10 giây**. Trong khoảng thời gian này, Vertex AI vẫn coi server là hoạt động tốt. Nếu probe nhận được phản hồi tốt từ bất kỳ lần kiểm tra nào, nó sẽ quay lại **Health checks Process**. Tuy nhiên, **nếu probe nhận được 4 phản hồi không tốt liên tiếp**, Vertex AI sẽ dừng việc chuyển tiếp các yêu cầu dự đoán tới container đó (nếu mô hình được triển khai trên nhiều node, các yêu cầu sẽ được chuyển tới các container khác đang hoạt động tốt).\n",
    "\n",
    "Vertex AI không khởi động lại container; thay vào đó, health probe vẫn sẽ tiếp tục gửi các yêu cầu kiểm tra định kỳ tới server không tốt. Nếu nhận được phản hồi tốt, container đó sẽ được đánh dấu là hoạt động tốt và bắt đầu nhận lại yêu cầu dự đoán.\n",
    "\n",
    "**Hướng dẫn thực tế:**\n",
    "- Trong nhiều trường hợp, **server HTTP** trong container của bạn có thể luôn phản hồi với mã trạng thái `200 OK` cho các yêu cầu kiểm tra sức khỏe. Nếu container tải các tài nguyên trước khi khởi động server, container sẽ không hoạt động tốt trong thời gian khởi động và bất kỳ lúc nào server HTTP gặp lỗi. Trong tất cả các thời gian khác, nó sẽ phản hồi là tốt.\n",
    "\n",
    "- Đối với cấu hình phức tạp hơn, bạn có thể thiết kế server HTTP để cố tình phản hồi yêu cầu kiểm tra sức khỏe với trạng thái không tốt vào những thời điểm nhất định. Ví dụ, bạn có thể chặn lưu lượng dự đoán tới node trong một khoảng thời gian để container thực hiện bảo trì.\n",
    "\n",
    "---\n",
    "**3. Prediction**\n",
    "\n",
    "Provide an `HTTP path` for **prediction** (default path `/predict` with `HTTP POST`, it can be change in config)\n",
    "- `Content-Type: application/json` HTTP header\n",
    "\n",
    "---\n",
    "\n",
    "**4. Request body**\n",
    "\n",
    "The request body is `JSON` format and must be 1.5 MB or smaller, need contain an `instances` key and can be has `parameters` :\n",
    "```JSON\n",
    "{\n",
    "   \"instances\":[\n",
    "      {\n",
    "         \"text\":\"DoiT is a great company.\"\n",
    "      },\n",
    "      {\n",
    "         \"text\":\"The beach was nice but overall the hotel was very bad.\"\n",
    "      }\n",
    "   ],\n",
    "   \"parameters\": {}\n",
    "}\n",
    "```\n",
    "- `instances` take is an array of **one or more JSON values** of any type. Each values represents an instance that you are providing a prediction for.\n",
    "- `parameters` (optional if application is designed to require it) take a JSON object containing any parameters that your container requires to help serve predictions on the instances\n",
    "\n",
    "---\n",
    "\n",
    "**5. Response body**\n",
    "\n",
    "The response body is `JSON` format and must be 1.5 MB or smaller, need contain an `predictions` key :\n",
    "```JSON\n",
    "{\n",
    " \"predictions\": [\n",
    "   {\n",
    "     \"confidence\": 0.9409326314926147,\n",
    "     \"sentiment\": \"POSITIVE\"\n",
    "   }\n",
    " ],\n",
    "  \"deployedModelId\": <string>, # id of the Endpoint's DeployedModel\n",
    "  \"model\": <string>, # The resource name of the Model\n",
    "  \"modelVersionId\": <string>, # The version id of the Model\n",
    "  \"modelDisplayName\": <string>, # The display name of the Model \n",
    "  \"metadata\": <value> # Request-level metadata returned by the model\n",
    "}\n",
    "```\n",
    "- `predictions` take is an array of **one or more JSON values** representing the predictions that your container has generated for each of the INSTANCES in the corresponding request.\n",
    "\n",
    "**6. Publishing requirements**\n",
    "- Location: `asia-southeast1`\n",
    "- [Permissions](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#permissions)\n",
    "- [Environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#variables)\n",
    "\n",
    "**7. Access model artifacts**\n",
    "\n",
    "[Doc](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#artifacts)\n",
    "\n",
    "- **Nếu sử dụng pre-build container làm môi trường**: Thì phải cung cấp địa chỉ tại GCS (folder) chứa các file model được training sẽ chạy trên environment build từ pre-build container đó\n",
    "\n",
    "- **Nếu sử dụng custom container làm môi trường**: Việc cung cấp địa chỉ GCS (folder) chứa các file trained model là optional, nó cần thiết trong việc sử dụng custom container chỉ làm environment runtime và ko chứa sẵn model, khi đó cần phải copy model vào để run trong environment đó. Còn nếu trong container chứa sẵn file model thì việc cung cấp địa chỉ folder (GCS) chứa file model là ko cần thiết"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to Model Registry (VertexAI)\n",
    "\n",
    "Ta cần import Model Image từ **Artifact Registry** sang **Vertex AI** để có thể tận dụng các tính năng quản lý model AI của Vertex và serve được model.\n",
    "\n",
    "**Chi phí sử dụng Model Registry (VertexAI)**: No Cost\n",
    "\n",
    "Chỉ phát sinh chi phí khi sử dụng prediction: `Online prediction via Endpoint` hoặc `Batch Prediction`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import bằng giao diện UI: [Doc](https://cloud.google.com/vertex-ai/docs/model-registry/import-model#custom-container)\n",
    "\n",
    "<img src = \"_image/import_model_registry.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import model command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud ai models upload \\\n",
    "  --container-ports=8080 \\\n",
    "  --container-predict-route=\"/predict\" \\\n",
    "  --container-health-route=\"/health\" \\\n",
    "  --region=asia-southeast1 \\\n",
    "  --display-name=sentiment-fast-api \\\n",
    "  --container-image-uri=asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Serve Vertex model by batch prediction\n",
    "\n",
    "**Batch Prediction** là gửi request trực tiếp tới Model đã được imported vào **Model Registry** mà Model này không cần deploy thành endpoint. Khi đó data gửi vào trong 1 single request (có thể large size) và không yêu cầu reponse trả ra real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost\n",
    "\n",
    "Chi phí được tính bằng thời gian sử dụng [***resource per node hour***](https://cloud.google.com/vertex-ai/pricing#pred_apac), tổng của:\n",
    "- **vCPU cost**: measured in vCPU hours\n",
    "- **RAM cost**: measured in GB hours\n",
    "- **GPU cost**: if either built into the machine or optionally configured, measured in GPU hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config Input data\n",
    "\n",
    "Input for batch prediction:\n",
    "- CSV file\n",
    "- File-list in GCS\n",
    "- Bigquery table\n",
    "- JSON Line (JSONL)\n",
    "- `tf-record` or `tf-record-gzip`\n",
    "\n",
    "> To use a BigQuery table as input, you must set [`InstanceConfig.instanceType`](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#instanceconfig) to `object` using the Vertex AI API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "##### [Input data requirement](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#input_data_requirements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partition data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter and transformation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request a batch prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine type & replica count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch prediction output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve Vertex model by online prediction (endpoint - realtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model enpoint\n",
    "\n",
    "Trước khi deploy model vào "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy model to an enpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving model container - Cloud Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build docker image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai models upload \\\n",
    "  --container-ports=80 \\\n",
    "  --container-predict-route=\"/predict\" \\\n",
    "  --container-health-route=\"/health\" \\\n",
    "  --region=asia-southeast1 \\\n",
    "  --display-name=sentiment-fast-api \\\n",
    "  --container-image-uri=gcr.io/sascha-playground-doit/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai endpoints create \\\n",
    "  --project=ext-pinetree-dw \\\n",
    "  --region=us-central1 \\\n",
    "  --display-name=sentiment-fast-api-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "project = \"ext-pinetree-dw\"\n",
    "location = \"asia-southeast1\"\n",
    "\n",
    "aiplatform.init(project=project, location=location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(predictions=[{'sentiment': 'Tiêu cực', 'confidence': 8.0}, {'sentiment': 'Tích cực', 'confidence': 8.6}], deployed_model_id='8408136941417005056', metadata=None, model_version_id='1', model_resource_name='projects/723874410918/locations/asia-southeast1/models/sentiment-fast-api-test', explanations=None)\n"
     ]
    }
   ],
   "source": [
    "instances = [\n",
    "    {\"text\": \"DoiT is a great company.\"},\n",
    "    {\"text\": \"The beach was nice but overall the hotel was very bad.\"},\n",
    "]\n",
    "\n",
    "\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    \"projects/723874410918/locations/asia-southeast1/endpoints/4126673847229349888\"\n",
    ")\n",
    "\n",
    "prediction = endpoint.predict(instances=instances)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "steps:\n",
    "# Download the model to embed it into the image\n",
    "# - name: 'gcr.io/cloud-builders/gsutil'\n",
    "#   args: ['cp', '-r', 'gs://dev-joyas-recommendation/models/sentiment', '.']\n",
    "#   id: 'download-model'\n",
    "\n",
    "# Build the container image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'asia-southeast1-docker.pkg.dev/joyas-vietnam/dev-aiml-model/sentiment-fast-api', '.']\n",
    "  # waitFor: ['download-model']\n",
    "\n",
    "# Push the container image to Artifact Registry\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['push', 'asia-southeast1-docker.pkg.dev/joyas-vietnam/dev-aiml-model/sentiment-fast-api']\n",
    "\n",
    "images:\n",
    "- asia-southeast1-docker.pkg.dev/joyas-vietnam/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create request issued for: [dev-aiml-model]\n",
      "Waiting for operation [projects/joyas-vietnam/locations/asia-southeast1/operations/c10a00b3-2f8b-45bf-b923-45e42675f358] to complete...\n",
      ".................done.\n",
      "Created repository [dev-aiml-model].\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create dev-aiml-model \\\n",
    "  --repository-format=docker \\\n",
    "  --location=asia-southeast1 \\\n",
    "  --description=\"My Docker repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --config cloudbuild.yaml ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 278B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
      "#2 ...\n",
      "\n",
      "#3 [auth] tiangolo/uvicorn-gunicorn-fastapi:pull token for registry-1.docker.io\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
      "#2 DONE 2.2s\n",
      "\n",
      "#4 [internal] load .dockerignore\n",
      "#4 transferring context: 2B done\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [1/5] FROM docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim@sha256:cce370ade672f3bfcac80d0c80314fc6b6530d3c623dab384af12da76cd2db6b\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [internal] load build context\n",
      "#6 transferring context: 574B done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "#7 [2/5] COPY main.py ./main.py\n",
      "#7 CACHED\n",
      "\n",
      "#8 [3/5] COPY requirements.txt ./requirements.txt\n",
      "#8 CACHED\n",
      "\n",
      "#9 [4/5] COPY models ./models\n",
      "#9 DONE 5.7s\n",
      "\n",
      "#10 [5/5] RUN pip install --no-cache-dir -r ./requirements.txt\n",
      "#10 4.266 Collecting fastapi==0.115.0\n",
      "#10 4.405   Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "#10 4.444      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.6/94.6 kB 2.5 MB/s eta 0:00:00\n",
      "#10 5.004 ERROR: Ignored the following versions that require a different python version: 1.25.0 Requires-Python >=3.9; 1.25.1 Requires-Python >=3.9; 1.25.2 Requires-Python >=3.9; 1.26.0 Requires-Python <3.13,>=3.9; 1.26.1 Requires-Python <3.13,>=3.9; 1.26.2 Requires-Python >=3.9; 1.26.3 Requires-Python >=3.9; 1.26.4 Requires-Python >=3.9; 2.0.0 Requires-Python >=3.9; 2.0.1 Requires-Python >=3.9; 2.0.2 Requires-Python >=3.9; 2.1.0 Requires-Python >=3.10; 2.1.0rc1 Requires-Python >=3.10; 2.1.1 Requires-Python >=3.10\n",
      "#10 5.005 ERROR: Could not find a version that satisfies the requirement numpy==1.26.4 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "#10 5.006 ERROR: No matching distribution found for numpy==1.26.4\n",
      "#10 5.313 \n",
      "#10 5.313 [notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "#10 5.313 [notice] To update, run: pip install --upgrade pip\n",
      "#10 ERROR: process \"/bin/sh -c pip install --no-cache-dir -r ./requirements.txt\" did not complete successfully: exit code: 1\n",
      "------\n",
      " > [5/5] RUN pip install --no-cache-dir -r ./requirements.txt:\n",
      "4.266 Collecting fastapi==0.115.0\n",
      "4.405   Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "4.444      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.6/94.6 kB 2.5 MB/s eta 0:00:00\n",
      "5.004 ERROR: Ignored the following versions that require a different python version: 1.25.0 Requires-Python >=3.9; 1.25.1 Requires-Python >=3.9; 1.25.2 Requires-Python >=3.9; 1.26.0 Requires-Python <3.13,>=3.9; 1.26.1 Requires-Python <3.13,>=3.9; 1.26.2 Requires-Python >=3.9; 1.26.3 Requires-Python >=3.9; 1.26.4 Requires-Python >=3.9; 2.0.0 Requires-Python >=3.9; 2.0.1 Requires-Python >=3.9; 2.0.2 Requires-Python >=3.9; 2.1.0 Requires-Python >=3.10; 2.1.0rc1 Requires-Python >=3.10; 2.1.1 Requires-Python >=3.10\n",
      "5.005 ERROR: Could not find a version that satisfies the requirement numpy==1.26.4 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "5.006 ERROR: No matching distribution found for numpy==1.26.4\n",
      "5.313 \n",
      "5.313 [notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "5.313 [notice] To update, run: pip install --upgrade pip\n",
      "------\n",
      "Dockerfile:6\n",
      "--------------------\n",
      "   4 |     COPY requirements.txt ./requirements.txt\n",
      "   5 |     COPY models ./models\n",
      "   6 | >>> RUN pip install --no-cache-dir -r ./requirements.txt\n",
      "   7 |     \n",
      "   8 |     EXPOSE 8080\n",
      "--------------------\n",
      "ERROR: failed to solve: process \"/bin/sh -c pip install --no-cache-dir -r ./requirements.txt\" did not complete successfully: exit code: 1\n",
      "\n",
      "View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/wwaxbjtwht1okupgtdbzci9l3\n"
     ]
    }
   ],
   "source": [
    "!docker build -t asia-southeast1-docker.pkg.dev/joyas-vietnam/dev-aiml-model/sentiment-fast-api ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
