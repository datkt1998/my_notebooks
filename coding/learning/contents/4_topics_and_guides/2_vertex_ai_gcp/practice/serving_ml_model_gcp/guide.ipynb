{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve model deploy to vertex AI\n",
    "\n",
    "references:\n",
    "- https://medium.com/google-cloud/serving-machine-learning-models-with-google-vertex-ai-5d9644ededa3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new virtual environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create a virtual environment\n",
    "python -m venv .venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "# Source .venv/bin/activate\n",
    ".venv\\Scripts\\activate\n",
    "\n",
    "# (Optional) Deactivate conda environment\n",
    "conda deactivate\n",
    "\n",
    "# Upgrade pip\n",
    "python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a `requirements.txt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "uvicorn[standard]==0.20.0\n",
    "gunicorn==23.0.0\n",
    "fastapi[standard]==0.115.0\n",
    "scikit-learn==1.5.2\n",
    "pytest==8.3.3\n",
    "starlette==0.38.6\n",
    "requests==2.32.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import random\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SimpleSentimentModel(BaseEstimator, TransformerMixin):\n",
    "    negative_length_threshold = 10\n",
    "    positive_length_threshold = 30\n",
    "    negative_ls = [\"tiêu cực\", \"xấu\", \"tệ\", \"negative\"]\n",
    "    positive_ls = [\"tích cực\", \"thích\", \"positive\"]\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, text):\n",
    "        text_lower = text.lower()\n",
    "        if any(word in text_lower for word in self.negative_ls):\n",
    "            return \"negative\", random.randrange(90, 100, step=1) / 100\n",
    "        elif any(word in text_lower for word in self.positive_ls):\n",
    "            return \"positive\", random.randrange(90, 100, step=1) / 100\n",
    "        elif len(text) <= self.negative_length_threshold:\n",
    "            return \"negative\", random.randrange(70, 90, step=1) / 100\n",
    "        elif len(text) >= self.positive_length_threshold:\n",
    "            return \"positive\", random.randrange(70, 90, step=1) / 100\n",
    "        else:\n",
    "            return \"neutral\", random.randrange(70, 95, step=1) / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "from model import SimpleSentimentModel  # Ensure this import is correct\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the model\n",
    "    model = SimpleSentimentModel()\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    model_dir = \"models\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    # Save the model using joblib, ensuring correct context\n",
    "    joblib.dump(model, os.path.join(model_dir, \"model.pkl\"))\n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main app API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import joblib\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from model import SimpleSentimentModel  # noqa: F401\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Sentiment Analysis API\")\n",
    "\n",
    "# Load the model with a safe file path\n",
    "model_path = os.path.join(\"models\", \"model.pkl\")\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "# Load the model, making sure SimpleSentimentModel is already imported\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# Pydantic models for prediction results\n",
    "class Prediction(BaseModel):\n",
    "    sentiment: str\n",
    "    confidence: Optional[float]\n",
    "\n",
    "\n",
    "class Predictions(BaseModel):\n",
    "    predictions: List[Prediction]\n",
    "\n",
    "\n",
    "# Function to process batch predictions\n",
    "def get_prediction(instances):\n",
    "    res = []\n",
    "    for text in instances:\n",
    "        sentiment, confidence = model.predict(text)\n",
    "        res.append(Prediction(sentiment=sentiment, confidence=confidence))\n",
    "    return Predictions(predictions=res)\n",
    "\n",
    "\n",
    "# Health check route\n",
    "@app.get(\"/health\", status_code=200)\n",
    "async def health():\n",
    "    return {\"health\": \"ok\"}\n",
    "\n",
    "\n",
    "# Prediction route to handle batch requests\n",
    "@app.post(\n",
    "    \"/predict\",\n",
    "    response_model=Predictions,\n",
    "    response_model_exclude_unset=True,\n",
    ")\n",
    "async def predict(request: Request):\n",
    "    # Extract the JSON body from the request\n",
    "    body = await request.json()\n",
    "\n",
    "    # Validate the request body\n",
    "    if \"instances\" not in body or not isinstance(body[\"instances\"], list):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"Invalid input format. 'instances' should be a list of texts.\",\n",
    "        )\n",
    "\n",
    "    # Extract the instances (texts) from the request\n",
    "    instances = [x[\"text\"] for x in body[\"instances\"]]\n",
    "\n",
    "    # Get predictions\n",
    "    output = get_prediction(instances)\n",
    "\n",
    "    # Return the predictions\n",
    "    return output\n",
    "\n",
    "\n",
    "# Main function to run the FastAPI app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8080)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test app**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "from main import app\n",
    "\n",
    "client = TestClient(app=app)\n",
    "base_url = \"\"\n",
    "\n",
    "\n",
    "def test_health():\n",
    "    response = client.get(f\"{base_url}/health\")\n",
    "    assert response.status_code == 200\n",
    "    assert response.json() == {\"health\": \"ok\"}\n",
    "    print(\"pass: test_health\")\n",
    "\n",
    "\n",
    "def test_predict_item():\n",
    "    response = client.post(\n",
    "        f\"{base_url}/predict\",\n",
    "        json={\n",
    "            \"instances\": [\n",
    "                {\"text\": \"Cong hoa xa hoi chu nghia\"},\n",
    "                {\"text\": \"doc lap\"},\n",
    "                {\"text\": \"doc lap tich cuc\"},\n",
    "                {\"text\": \"te doc\"},\n",
    "                {\"text\": \"positive doc lap\"},\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "    result = response.json()\n",
    "    sentiments = [i[\"sentiment\"] for i in result[\"predictions\"]]\n",
    "    assert sentiments == [\n",
    "        \"neutral\",\n",
    "        \"negative\",\n",
    "        \"neutral\",\n",
    "        \"negative\",\n",
    "        \"positive\",\n",
    "    ]\n",
    "    print(\"pass: test_predict_item\")\n",
    "\n",
    "\n",
    "def test_predict_item_non_instance():\n",
    "    response = client.post(\n",
    "        f\"{base_url}/predict\",\n",
    "        json={\n",
    "            \"instan\": [\n",
    "                {\"text\": \"Cong hoa xa hoi chu nghia\"},\n",
    "                {\"text\": \"doc lap\"},\n",
    "                {\"text\": \"doc lap tich cuc\"},\n",
    "                {\"text\": \"te doc\"},\n",
    "                {\"text\": \"positive doc lap\"},\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    assert response.status_code == 400\n",
    "    response.json() == {\n",
    "        \"detail\": \"Invalid input format. 'instances' should be a list of texts.\"\n",
    "    }\n",
    "    print(\"pass: test_predict_item_non_instance\")\n",
    "\n",
    "\n",
    "def test_predict_item_not_list():\n",
    "    response = client.post(\n",
    "        f\"{base_url}/predict\",\n",
    "        json={\"instan\": {\"text\": \"Cong hoa xa hoi chu nghia\"}},\n",
    "    )\n",
    "    assert response.status_code == 400\n",
    "    response.json() == {\n",
    "        \"detail\": \"Invalid input format. 'instances' should be a list of texts.\"\n",
    "    }\n",
    "    print(\"pass: test_predict_item_not_list\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test for running container\n",
    "    import requests\n",
    "\n",
    "    client = requests\n",
    "    base_url = \"http://127.0.0.1:8080\"\n",
    "    test_health()\n",
    "    test_predict_item()\n",
    "    test_predict_item_non_instance()\n",
    "    test_predict_item_not_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pytest in `cmd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pytest test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model image to Artifact Registry (GCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "**Write Dockerfile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM tiangolo/uvicorn-gunicorn:python3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY *.py ./\n",
    "COPY models ./models\n",
    "COPY requirements.txt ./requirements.txt\n",
    "\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r ./requirements.txt\n",
    "\n",
    "EXPOSE 8080\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Push image bằng docker-command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build docker image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker build -t asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test image container**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm -p 8080:8080 asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Push image to Artifact Registry (GCP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authen GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# docker login\n",
    "gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker push asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Push image bằng cloud-build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Config cloud build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "steps:\n",
    "# If training model in cloud and save model in GCS\n",
    "# Assume Storage location of model: `gs://dev-aiml-model/models/sentiment`\n",
    "# Download the model file in GCS to embed it into the image\n",
    "  - name: 'gcr.io/cloud-builders/gsutil'\n",
    "    args: ['cp', '-r', '${_MODEL_GCS_PATH}', './models']\n",
    "    id: 'download-model'\n",
    "  \n",
    "  # Build the container image\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    args: ['build', '-t', '${_IMAGE_NAME}', '.']\n",
    "    waitFor: ['download-model']\n",
    "  \n",
    "  # Push the container image to Artifact Registry\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    args: ['push', '${_IMAGE_NAME}']\n",
    "\n",
    "images:\n",
    "  - '${_IMAGE_NAME}'\n",
    "\n",
    "# Substitution variables for flexibility\n",
    "substitutions:\n",
    "  _MODEL_GCS_PATH: 'gs://dev-aiml-model/models/sentiment'\n",
    "  _IMAGE_NAME: 'asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run cloud build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud builds submit --config cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving model container - Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Container Requirement**\n",
    "\n",
    "The docker container needs to follow the [container requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) defined by Google. The most important requirement is: \n",
    "\n",
    "---\n",
    "**1. HTTP server**\n",
    "\n",
    "Provide an `HTTP server` that listens for requests on `0.0.0.0` (must) on port `8080` (can be choice).\n",
    "\n",
    "**HTTP Server** can be using:\n",
    "- **Flask** , **FastAPI**, ...\n",
    "- **TensorFlow Serving**, **TorchServe**, or **KServe Python Server**\n",
    "- ...\n",
    "\n",
    "[**HTTP Server** can be run by](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#server):\n",
    "- [ENTRYPOINT instruction](https://docs.docker.com/engine/reference/builder/#entrypoint), [CMD instruction](https://docs.docker.com/engine/reference/builder/#cmd) or both in ***Dockerfile***\n",
    "- Specify the `containerSpec.command` and `containerSpec.args` fields when you create your `Model` resource (override your container image's `ENTRYPOINT` and `CMD`)\n",
    "\n",
    "---\n",
    "**2. Health checks**\n",
    "\n",
    "***a. startup probe*** (optional)\n",
    "\n",
    "Check whether the container application has started. Nếu không cung cấp thì sẽ ko chạy, và ngay lập tức chạy ***health probe***\n",
    "\n",
    "**Usecase**: Cần sử dụng cho các application cần có thời gian khởi động trong lần đầu tiên. Ví dụ, Nếu App cần thời gian để copy file model mới từ source bên ngoài container mỗi lần khởi động. Chúng ta có thể config ***startup probe*** để chờ cho đến khi việc copy hoàn thành và trả ra success\n",
    "\n",
    "\n",
    "***b. health probe***\n",
    "\n",
    "Check whether the container application is ready to accept traffic or receive request. Nếu không cung cấp path cụ thể thì Vertex sẽ sử dụng default path `/health`. Lưu ý là ***health probe*** chỉ chạy khi ***startup probe*** hoàn thành hoặc không được khai báo\n",
    "\n",
    "Provide an `HTTP path` for **health checks** (default path `/health` with `HTTP GET`, it can be change in config): \n",
    "- Return a `200` within **10 seconds** after call when you’re container is ready to handle requests. Nội dung của phần phản hồi không quan trọng, vì Vertex AI sẽ bỏ qua chúng. Phản hồi này cho thấy rằng server đang hoạt động tốt (healthy). For example, if you need to load the model, ensure you return the `200` status code after the model is loaded.\n",
    "- **If the server isn't ready to handle prediction requests**, nó không nên phản hồi yêu cầu trong vòng **10 giây**, hoặc phản hồi với bất kỳ mã trạng thái nào khác ngoài `200 OK`, ví dụ như `503 Service Unavailable`. Điều này cho thấy server đang không hoạt động tốt (unhealthy).\n",
    "\n",
    "Nếu health probe nhận được phản hồi không tốt từ server (bao gồm cả trường hợp không có phản hồi trong vòng 10 giây), nó sẽ gửi thêm **tối đa 3 lần Health Checks nữa**, mỗi lần cách nhau **10 giây**. Trong khoảng thời gian này, Vertex AI vẫn coi server là hoạt động tốt. Nếu probe nhận được phản hồi tốt từ bất kỳ lần kiểm tra nào, nó sẽ quay lại **Health checks Process**. Tuy nhiên, **nếu probe nhận được 4 phản hồi không tốt liên tiếp**, Vertex AI sẽ dừng việc chuyển tiếp các yêu cầu dự đoán tới container đó (nếu mô hình được triển khai trên nhiều node, các yêu cầu sẽ được chuyển tới các container khác đang hoạt động tốt).\n",
    "\n",
    "Vertex AI không khởi động lại container; thay vào đó, health probe vẫn sẽ tiếp tục gửi các yêu cầu kiểm tra định kỳ tới server không tốt. Nếu nhận được phản hồi tốt, container đó sẽ được đánh dấu là hoạt động tốt và bắt đầu nhận lại yêu cầu dự đoán.\n",
    "\n",
    "**Hướng dẫn thực tế:**\n",
    "- Trong nhiều trường hợp, **server HTTP** trong container của bạn có thể luôn phản hồi với mã trạng thái `200 OK` cho các yêu cầu kiểm tra sức khỏe. Nếu container tải các tài nguyên trước khi khởi động server, container sẽ không hoạt động tốt trong thời gian khởi động và bất kỳ lúc nào server HTTP gặp lỗi. Trong tất cả các thời gian khác, nó sẽ phản hồi là tốt.\n",
    "\n",
    "- Đối với cấu hình phức tạp hơn, bạn có thể thiết kế server HTTP để cố tình phản hồi yêu cầu kiểm tra sức khỏe với trạng thái không tốt vào những thời điểm nhất định. Ví dụ, bạn có thể chặn lưu lượng dự đoán tới node trong một khoảng thời gian để container thực hiện bảo trì.\n",
    "\n",
    "---\n",
    "**3. Prediction**\n",
    "\n",
    "Provide an `HTTP path` for **prediction** (default path `/predict` with `HTTP POST`, it can be change in config)\n",
    "- `Content-Type: application/json` HTTP header\n",
    "\n",
    "---\n",
    "\n",
    "**4. Request body**\n",
    "\n",
    "The request body is `JSON` format and must be 1.5 MB or smaller, need contain an `instances` key and can be has `parameters` :\n",
    "```JSON\n",
    "{\n",
    "   \"instances\":[\n",
    "      {\n",
    "         \"text\":\"DoiT is a great company.\"\n",
    "      },\n",
    "      {\n",
    "         \"text\":\"The beach was nice but overall the hotel was very bad.\"\n",
    "      }\n",
    "   ],\n",
    "   \"parameters\": {}\n",
    "}\n",
    "```\n",
    "- `instances` take is an array of **one or more JSON values** of any type. Each values represents an instance that you are providing a prediction for.\n",
    "- `parameters` (optional if application is designed to require it) take a JSON object containing any parameters that your container requires to help serve predictions on the instances\n",
    "\n",
    "---\n",
    "\n",
    "**5. Response body**\n",
    "\n",
    "The response body is `JSON` format and must be 1.5 MB or smaller, need contain an `predictions` key :\n",
    "```JSON\n",
    "{\n",
    " \"predictions\": [\n",
    "   {\n",
    "     \"confidence\": 0.9409326314926147,\n",
    "     \"sentiment\": \"POSITIVE\"\n",
    "   }\n",
    " ],\n",
    "  \"deployedModelId\": <string>, # id of the Endpoint's DeployedModel\n",
    "  \"model\": <string>, # The resource name of the Model\n",
    "  \"modelVersionId\": <string>, # The version id of the Model\n",
    "  \"modelDisplayName\": <string>, # The display name of the Model \n",
    "  \"metadata\": <value> # Request-level metadata returned by the model\n",
    "}\n",
    "```\n",
    "- `predictions` take is an array of **one or more JSON values** representing the predictions that your container has generated for each of the INSTANCES in the corresponding request.\n",
    "\n",
    "**6. Publishing requirements**\n",
    "- Location: `asia-southeast1`\n",
    "- [Permissions](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#permissions)\n",
    "- [Environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#variables)\n",
    "\n",
    "**7. Access model artifacts**\n",
    "\n",
    "[Doc](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#artifacts)\n",
    "\n",
    "- **Nếu sử dụng pre-build container làm môi trường**: Thì phải cung cấp địa chỉ tại GCS (folder) chứa các file model được training sẽ chạy trên environment build từ pre-build container đó\n",
    "\n",
    "- **Nếu sử dụng custom container làm môi trường**: Việc cung cấp địa chỉ GCS (folder) chứa các file trained model là optional, nó cần thiết trong việc sử dụng custom container chỉ làm environment runtime và ko chứa sẵn model, khi đó cần phải copy model vào để run trong environment đó. Còn nếu trong container chứa sẵn file model thì việc cung cấp địa chỉ folder (GCS) chứa file model là ko cần thiết"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to Model Registry (VertexAI)\n",
    "\n",
    "Ta cần import Model Image từ **Artifact Registry** sang **Vertex AI** để có thể tận dụng các tính năng quản lý model AI của Vertex và serve được model.\n",
    "\n",
    "**Chi phí sử dụng Model Registry (VertexAI)**: No Cost\n",
    "\n",
    "Chỉ phát sinh chi phí khi sử dụng prediction: `Online prediction via Endpoint` hoặc `Batch Prediction`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import bằng giao diện UI: [Doc](https://cloud.google.com/vertex-ai/docs/model-registry/import-model#custom-container)\n",
    "\n",
    "<img src = \"_image/import_model_registry.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import model command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud ai models upload \\\n",
    "  --container-ports=8080 \\\n",
    "  --container-predict-route=\"/predict\" \\\n",
    "  --container-health-route=\"/health\" \\\n",
    "  --region=asia-southeast1 \\\n",
    "  --display-name=sentiment-fast-api \\\n",
    "  --container-image-uri=asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sử dụng python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project='ext-pinetree-dw', location='asia-southeast1')\n",
    "aiplatform.Model.upload(\n",
    "          display_name=\"sentiment-fast-api\",\n",
    "          serving_container_image_uri=\"asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api\",\n",
    "          serving_container_predict_route=\"/predict\",\n",
    "          serving_container_health_route=\"/health\",\n",
    "          serving_container_ports=[8080]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model after import\n",
    "model = aiplatform.Model(\"projects/ext-pinetree-dw/locations/asia-southeast1/models/<model_id>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Serve Vertex model by batch prediction\n",
    "\n",
    "**Batch Prediction** là gửi request trực tiếp tới Model đã được imported vào **Model Registry** mà **Model này không cần deploy thành endpoint**. Khi đó data gửi vào trong 1 single request (có thể large size) và không yêu cầu reponse trả ra real-time.\n",
    "\n",
    "Vertex AI Batch Prediction is made for **large datasets** that would **take too much time with an online prediction approach**. It provides a **scalable**, **serverless**, and **efficient service** for cases where you **don’t need an immediate response** (asynchronous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost\n",
    "\n",
    "Chi phí được tính bằng thời gian sử dụng [***resource per node hour***](https://cloud.google.com/vertex-ai/pricing#pred_apac), tổng của:\n",
    "- **vCPU cost**: measured in vCPU hours\n",
    "- **RAM cost**: measured in GB hours\n",
    "- **GPU cost**: if either built into the machine or optionally configured, measured in GPU hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "##### [Input data format](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#input_data_requirements)\n",
    "\n",
    "Support các loại input data sau:\n",
    "- **JSONL (JSON Line)**: Linh hoạt và support việc parse object dưới dạng key-value (hữu ích in case muốn xác định chính xác feature-name : feature value muốn truyền vào model)\n",
    "- **CSV**: lấy cột và pass them mà không sử dụng header (không lấy dòng đầu tiên). Tất cả các string value phải được đặt trong double-quote.\n",
    "- **File-list**: được encoded as a base64 string that contain content in file.\n",
    "- **TFRecord** : `tf-record` or `tf-record-gzip` read as a binary and encoded as base64\n",
    "- **Bigquery**: is the same CSV. Cần phải chú trọng thứ tự cột truyền vào để xác định feature chính xác do không nhận thông tin feature name/column name.\n",
    "\n",
    "> To use a BigQuery table as input, you must set [`InstanceConfig.instanceType`](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#instanceconfig) to `object` using the Vertex AI API.\n",
    "\n",
    "Với mỗi 1 format, VertexAI sẽ tự động **transforms them into a JSON** before sending it to the model (check chi tiết phía dưới để xem phần định dạng sau khi convert từ raw data sang JSON format)\n",
    "\n",
    "**Chú ý: Hầu hết việc fail batch prediction job xảy ra khi xuất hiện lỗi trong việc transform data to JSON format. Sau khi transform thì model lại nhận được được differenct format**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### JSON Lines\n",
    "\n",
    "[JSON Lines](https://jsonlines.org/) file store in a Cloud Storage bucket.\n",
    "\n",
    "- JSON Lines file where each line contains an **array**:\n",
    "\n",
    "    ***data is:***\n",
    "    ```text\n",
    "    [1, 2, 3, 4]\n",
    "    [5, 6, 7, 8]\n",
    "    ```\n",
    "\n",
    "    ***convert to request body JSON format is:***\n",
    "    ```json\n",
    "    {\"instances\": [ [1, 2, 3, 4], [5, 6, 7, 8] ]}\n",
    "    ```\n",
    "\n",
    "- JSON Lines file where each line contains an **object**:\n",
    "\n",
    "    ***data is:***\n",
    "    ```text\n",
    "    { \"values\": [1, 2, 3, 4], \"key\": 1 }\n",
    "    { \"values\": [5, 6, 7, 8], \"key\": 2 }\n",
    "    ```\n",
    "\n",
    "    ***convert to request body JSON format is:***\n",
    "    ```json\n",
    "    {\"instances\": [\n",
    "      { \"values\": [1, 2, 3, 4], \"key\": 1 },\n",
    "      { \"values\": [5, 6, 7, 8], \"key\": 2 }\n",
    "    ]}\n",
    "    ```\n",
    "\n",
    "> Note: For **PyTorch prebuilt containers**, Vertex AI wraps each instance in a `data` field before sending it to the prediction container. This is because TorchServe's default handlers expect each instance to be wrapped in a `data` field.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bigquery\n",
    "\n",
    "Vertex AI transforms each row from the table to a JSON instance.\n",
    "\n",
    "***data table is:***\n",
    "\n",
    "| column 1 | column 2 | column 3|\n",
    "|----------|----------|---------|\n",
    "| 1.0 | 3.0 | \"Cat1\"|\n",
    "| 2.0 | 4.0 | \"Cat2\"|\n",
    "\n",
    "***convert to request body JSON format is:***\n",
    "```json\n",
    "{\"instances\": [ [1.0,3.0,\"cat1\"], [2.0,4.0,\"cat2\"] ]}\n",
    "```\n",
    "\n",
    "**Convert datatype from bigquery to body request**\n",
    "\n",
    "| **BigQuery Type** | **JSON Type** | **Example value**                  |\n",
    "| ----------------- | ------------- | ---------------------------------- |\n",
    "| String            | String        | \"abc\"                              |\n",
    "| Integer           | Integer       | 1                                  |\n",
    "| Float             | Float         | 1.2                                |\n",
    "| Numeric           | Float         | 4925.000000000                     |\n",
    "| Boolean           | Boolean       | true                               |\n",
    "| TimeStamp         | String        | \"2019-01-01 23:59:59.999999+00:00\" |\n",
    "| Date              | String        | \"2018-12-31\"                       |\n",
    "| Time              | String        | \"23:59:59.999999\"                  |\n",
    "| DateTime          | String        | \"2019-01-01T00:00:00\"              |\n",
    "| Record            | Object        | { \"A\": 1,\"B\": 2}                   |\n",
    "| Repeated Type     | Array[Type]   | [1, 2]                             |\n",
    "| Nested Record     | Object        | {\"A\": {\"a\": 0}, \"B\": 1}            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### CSV\n",
    "\n",
    "- One input instance per row in a CSV file. \n",
    "- The first row must be a header row. \n",
    "- Enclose all strings in double quotation marks (\"). \n",
    "- Vertex AI doesn't accept cell values that contain newlines. \n",
    "- Non-quoted values are read as floating point numbers.\n",
    "\n",
    "***data in csv:***\n",
    "\n",
    "```text\n",
    "\"input1\",\"input2\",\"input3\"\n",
    "0.1,1.2,\"cat1\"\n",
    "4.0,5.0,\"cat2\"\n",
    "```\n",
    "\n",
    "***convert to request body JSON format is:***\n",
    "```json\n",
    "{\"instances\": [ [0.1,1.2,\"cat1\"], [4.0,5.0,\"cat2\"] ]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partition data\n",
    "\n",
    "**MapReduce** được sử dụng để chia data thành các replica, với yêu cầu data có khả năng partition:\n",
    "- Automatically partitions **BigQuery**, **file list**, and **JSON lines** input (sử dụng mỗi partition thành 1 replica)\n",
    "- **CSV** not able to partition\n",
    "- **TFRecord** partition by file (mỗi file là 1 replica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter and transformation data\n",
    "\n",
    "- **Filter**: cấu hình subset field would be selected or exclude to input data\n",
    "\n",
    "- **Transform**: config to `instanceType` is `array` or `object` format\n",
    "\n",
    "Ví dụ: cấu hình [`instanceConfig`](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#instanceconfig) trong [`BatchPredictionJob`](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"batchJob1\",\n",
    "  ...\n",
    "  \"instanceConfig\": {\n",
    "    \"excludedFields\":[\"customerId\"] # remove 'customerID' column\n",
    "    \"instanceType\":\"object\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"batchJob1\",\n",
    "  ...\n",
    "  \"instanceConfig\": {\n",
    "    \"includedFields\": [\"col1\",\"col2\"] # include col1 + col2\n",
    "    \"instanceType\":\"object\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config request a batch prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build pipeline for batch job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**1. Use gcloud command**](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#google-cloud-console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_JOB_NAME = \"sentiment-fast-api-batch-test\"\n",
    "MODEL_URI = model.resource_name\n",
    "INPUT_FORMAT = \"bigquery\"\n",
    "INPUT_URI = f\"bq://{TABLE_ID}\"\n",
    "OUTPUT_FORMAT = \"bigquery\"\n",
    "OUTPUT_URI = f\"bq://{PROJECT_ID}\"\n",
    "MACHINE_TYPE = \"n1-standard-2\"\n",
    "EXCLUDED_FIELDS = [ID_COLUMN_NAME]\n",
    "\n",
    "# Create a list of columns to be included\n",
    "ALL_COLUMNS = list(df_x_with_id.columns)\n",
    "INCLUDED_FIELDS = ALL_COLUMNS.copy()\n",
    "INCLUDED_FIELDS.remove(ID_COLUMN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Exclude fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON body requests - Exclude fields\n",
    "import json\n",
    "\n",
    "request_with_excluded_fields = {\n",
    "    \"displayName\": f\"{BATCH_JOB_NAME}-excluded_fields\",\n",
    "    \"model\": MODEL_URI,\n",
    "    \"inputConfig\": {\n",
    "        \"instancesFormat\": INPUT_FORMAT,\n",
    "        \"bigquerySource\": {\"inputUri\": INPUT_URI},\n",
    "    },\n",
    "    \"outputConfig\": {\n",
    "        \"predictionsFormat\": OUTPUT_FORMAT,\n",
    "        \"bigqueryDestination\": {\"outputUri\": OUTPUT_URI},\n",
    "    },\n",
    "    \"dedicatedResources\": {\n",
    "        \"machineSpec\": {\n",
    "            \"machineType\": MACHINE_TYPE,\n",
    "        }\n",
    "    },\n",
    "    \"instanceConfig\": {\"excludedFields\": EXCLUDED_FIELDS},\n",
    "}\n",
    "\n",
    "with open(\"request_with_excluded_fields.json\", \"w\") as outfile:\n",
    "    json.dump(request_with_excluded_fields, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to run job\n",
    "! curl \\\n",
    "  -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d @request_with_excluded_fields.json \\\n",
    "  https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/batchPredictionJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Include fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON body requests - Include fields\n",
    "request_with_included_fields = {\n",
    "    \"displayName\": f\"{BATCH_JOB_NAME}-included_fields\",\n",
    "    \"model\": MODEL_URI,\n",
    "    \"inputConfig\": {\n",
    "        \"instancesFormat\": INPUT_FORMAT,\n",
    "        \"bigquerySource\": {\"inputUri\": INPUT_URI},\n",
    "    },\n",
    "    \"outputConfig\": {\n",
    "        \"predictionsFormat\": OUTPUT_FORMAT,\n",
    "        \"bigqueryDestination\": {\"outputUri\": OUTPUT_URI},\n",
    "    },\n",
    "    \"dedicatedResources\": {\n",
    "        \"machineSpec\": {\n",
    "            \"machineType\": MACHINE_TYPE,\n",
    "        }\n",
    "    },\n",
    "    \"instanceConfig\": {\"includedFields\": INCLUDED_FIELDS},\n",
    "}\n",
    "\n",
    "with open(\"request_with_included_fields.json\", \"w\") as outfile:\n",
    "    json.dump(request_with_included_fields, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to run job\n",
    "! curl \\\n",
    "  -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d @request_with_included_fields.json \\\n",
    "  https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/batchPredictionJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**2. Request by Python SDK**](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#aiplatform_batch_predict_custom_trained-python_vertex_ai_sdk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input data source: **GCS JSON Lines**\n",
    "- Output data source: **GCS JSON Lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"ext-pinetree-dw\", location=\"asia-southeast1\")\n",
    "\n",
    "model_resource_name = \"projects/ext-pinetree-dw/locations/asia-southeast1/models/<model_id>\"\n",
    "model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "model.batch_predict(\n",
    "    job_display_name=\"fast-api-batch-test\",\n",
    "    gcs_source = ['gs://ext-pinetree-dw/data/test.jsonl',] ,   # list of jsonl files or gcs path folder\n",
    "    gcs_destination = 'gs://ext-pinetree-dw/data/predictions',\n",
    "    instances_format = \"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    machine_type=\"n1-standard-2\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    starting_replica_count=4,  # use only `starting_replica_count`, not use `max_replica_count`\n",
    "    sync = True, # Set to False if you don't want to wait for the job to finish\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input data source: **Bigquery**\n",
    "- Output data source: **Bigquery**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"ext-pinetree-dw\", location=\"asia-southeast1\")\n",
    "\n",
    "model_resource_name = \"projects/ext-pinetree-dw/locations/asia-southeast1/models/<model_id>\"\n",
    "model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "model.batch_predict(\n",
    "    job_display_name=\"fast-api-batch-test\",\n",
    "    bigquery_source= \"bq://ext-pinetree-dw.dataset.test_table\",   # list of jsonl files or gcs path folder\n",
    "    bigquery_destination_prefix = \"bq://your-project.your_dataset.your_output_table\",\n",
    "    instances_format = \"bigquery\",\n",
    "    predictions_format=\"bigquery\",\n",
    "    machine_type=\"n1-standard-2\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    starting_replica_count=4,  # use only `starting_replica_count`, not use `max_replica_count`\n",
    "    sync = True, # Set to False if you don't want to wait for the job to finish\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Workflow\n",
    "\n",
    "- **Batch**: Tất cả dữ liệu sẽ được chia thành nhiều nhóm (batches), mỗi batch với kích thước batch_size và lần lượt sẽ được đưa vào model để dự đoán.\n",
    "- **Replica**: Replica là một bản sao của machine được cấu hình để thực hiện prediction. Hệ thống sẽ tạo ra nhiều replica giống nhau, 1 replica sẽ xử lý 1 batch nhất định song song với replica khác. Hệ thống sẽ tự động phân chia dữ liệu và gán cho từng replica để xử lý, điều này giúp tăng tốc đáng kể cho các công việc lớn.\n",
    "\n",
    "Trong batch prediction, toàn bộ dữ liệu đầu vào được chia thành các batch và mỗi replica sẽ xử lý một số lượng batch nhất định. Hệ thống sử dụng một cơ chế tương tự như MapReduce để phân chia dữ liệu cho các replicas.\n",
    "\n",
    "Quá trình phân chia này hoạt động hiệu quả nếu dữ liệu có thể được partitioned, tức là có thể chia thành các phần độc lập để các replicas xử lý song song. \n",
    "- Dữ liệu đầu vào từ BigQuery, danh sách file, và file JSON lines là những loại có thể tự động được phân chia. \n",
    "- Tuy nhiên, dữ liệu CSV không phải là định dạng phù hợp cho việc này do nó không dễ chia nhỏ một cách an toàn và hiệu quả.\n",
    "\n",
    "**Recommendation**: ***specify the smallest machine type possible for your job and increase the number of replicas.*** Với mỗi 1 batch sẽ được chạy bởi 1 replica\n",
    "\n",
    "**Thời gian chạy mỗi replica**\n",
    "\n",
    "Để tối ưu chi phí, nên lựa chọn số replica sao cho thời gian chạy mỗi replica tối thiểu là 10 phút. Do billed được tính dựa trên per replica node hour, mà trong đó đã mất khoảng 5 phút để khởi động cho mỗi 1 replica, nếu thời gian chạy predition mỗi replica quá thấp thì chi phí chủ yếu chỉ là thời gian khởi tạo replica.\n",
    "\n",
    "Cụ thể:\n",
    "\n",
    "- Mỗi replica được phân chia một tập hợp các batch dữ liệu khi công việc dự đoán bắt đầu. Hệ thống sẽ tự động gán các batch này cho từng replica.\n",
    "- Quá trình hoạt động liên tục: Sau khi một replica xử lý xong một batch, nó sẽ chuyển sang batch tiếp theo trong danh sách được gán cho nó mà không cần khởi động lại.\n",
    "- Một replica chỉ dừng lại khi đã hoàn thành tất cả các batch mà nó được phân công hoặc khi toàn bộ công việc dự đoán kết thúc.\n",
    "\n",
    "**Số lượng replicas**\n",
    "\n",
    "- Với batch size <= 1 triệu bản ghi: Nên set `starting_replica_count` khoảng vài chục\n",
    "- Với batch size >= 1 triệu bản ghi: Nên set `starting_replica_count` khoảng vài trăm\n",
    "- Hoặc theo công thức: `Số replica` = `Số batches` / (`Số phút kỳ vọng hoàn thành batch prediction job` * `60` / `Số giây cần thiết để 1 replica hoàn thành được 1 batch`)\n",
    "    > `Số batches` = `Số bản ghi` / `batch_size`\n",
    "\n",
    "Tăng chỉ số `starting_replica_count` sẽ làm tăng nhanh quá trình hoàn thành prediction job, tuy nhiên cũng làm lãng phí nhiều thời gian cho việc khởi động replica mà không có tác dụng trong việc predict. Vertex fix cố định `starting_replica_count` là số replica sử dụng mà không có cơ chế autoscale trong quá trình chạy job.\n",
    "\n",
    "**Cấu hình resource**\n",
    "- Khác với online prediction, Batch prediction không autoscale resource\n",
    "- Sử dụng `starting_replica_count`, không sử dụng đến tham số `max_replica_count` (như online prediction)\n",
    "- Nếu sử dụng GPU, GPU machine types take more time to startup (10 minutes), nên khuyến nghị là kéo dài thời gian batch prediction để tận dụng được thời gian 1 lần khởi tạo (ít nhất 20 phút replica runtime) replica không chiếm đa số thời gian của việc chạy job.\n",
    "- Nên điều chỉnh batch_size phù hợp tuỳ thuộc vào machine, data input and model type để đảm bảo về performance, giá trị ban đầu được set `batch_size = 64`\n",
    "\n",
    "**Cách optimize prediction process time**\n",
    "1. Dữ liệu sẽ được chia thành các batch với `batch_size = 64 records` mặc định, mỗi batch sẽ được gửi đến 1 replica dể thực hiện quá trình predict.\n",
    "   - Tuỳ thuộc vào config matchine thì với mỗi batch sẽ chạy qua replica bị too large (khiến tràn memory --> fail batch) hoặc quá nhỏ khiến không tận dụng được sức mạnh của replica (mỗi replica phải chạy nhiều batch hơn --> tốn thời gian chạy --> tăng chi phí). Do đó nên test và điều chỉnh batch_size phù hợp.\n",
    "2. Sử dụng GPU sẽ làm giảm thời gian processing nếu model được design để sử dụng GPU, bằng việc config `accelerator_type` và `accelerator_count`\n",
    "\n",
    "**Schedule a batch prediction job**\n",
    "\n",
    "Combine: **Cloud Scheduler** + **Cloud Function**, **Cloud Run**, or **Vertex AI Pipelines**\n",
    "\n",
    "**Retries if error**\n",
    "\n",
    "If your model or prediction container runs into an error, Google retries the prediction three times. This is applied to each batch.\n",
    "\n",
    "**Monitoring**\n",
    "- Không có hệ thống monitoring sẵn có cho batch prediction, chỉ có cho online prediction (nhưng tốn chi phí resource cho phần quản lý monitoring)\n",
    "\n",
    "**Cách thức triển khai**\n",
    "- Hãy test kết quả trước bằng việc triển khai theo phương pháp online prediction (deploy model to VertexAI Endpoint) để có thể ensure được model run success và check được output của model đúng, quá trình debug cũng dễ dàng hơn.\n",
    "    >If your model already fails with online predictions, it will also fail with batch predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch prediction output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Body response format\n",
    "\n",
    "1. Nếu request body nhận được là **list of array**\n",
    "\n",
    "***request contains:***\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instances\": [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "]}\n",
    "```\n",
    "\n",
    "***The prediction container returns:***\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"predictions\": [\n",
    "    [0.1,0.9],\n",
    "    [0.7,0.3]\n",
    "  ],\n",
    "}\n",
    "```\n",
    "\n",
    "***Then the JSON Lines output file is:***\n",
    "\n",
    "```json\n",
    "{ \"instance\": [1, 2, 3, 4], \"prediction\": [0.1,0.9]}\n",
    "{ \"instance\": [5, 6, 7, 8], \"prediction\": [0.7,0.3]}\n",
    "```\n",
    "\n",
    "---\n",
    "2. Nếu request body nhận được là **list of object**\n",
    "\n",
    "***request contains:***\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instances\": [\n",
    "    {\"values\": [1, 2, 3, 4], \"key\": 1},\n",
    "    {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "]}\n",
    "```\n",
    "\n",
    "***The prediction container returns:***\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"predictions\": [\n",
    "    {\"result\":1},\n",
    "    {\"result\":0}\n",
    "  ],\n",
    "}\n",
    "```\n",
    "\n",
    "  ***Then the JSON Lines output file is:***\n",
    "\n",
    "  ```json\n",
    "  { \"instance\": {\"values\": [1, 2, 3, 4], \"key\": 1}, \"prediction\": {\"result\":1}}\n",
    "  { \"instance\": {\"values\": [5, 6, 7, 8], \"key\": 2}, \"prediction\": {\"result\":0}}\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output Result\n",
    "\n",
    "Output có thể lưu ở **GCS** hoặc **Bigquery**\n",
    "\n",
    "**1. Output Cloud Storage**\n",
    "\n",
    "- Output có thể chứa cả `prediction.errors` and `prediction.results` files\n",
    "- Mỗi batch prediction job thì sẽ có nhiều file output được tách ra do cơ chế shared và chạy replica\n",
    "- The `prediction.results` sẽ luôn bao gồm 2 thành phần theo format ở mục trên: `instance` (the data we sent to the batch prediction) + `prediction` itself\n",
    "\n",
    "`Json file`:\n",
    "```json\n",
    "{\"instance\": {\"text\": \"DoiT is a great company.\"}, \"prediction\": {\"sentiment\": \"POSITIVE\", \"confidence\": 0.9409326314926147}}\n",
    "{\"instance\": {\"text\": \"The beach was nice but overall the hotel was very bad.\"}, \"prediction\": {\"sentiment\": \"NEGATIVE\", \"confidence\": 0.9964427351951599}}\n",
    "```\n",
    "\n",
    ">Unfortunately, removing the instance from the batch prediction response is not possible. Consider this if you have a large dataset and want to calculate the storage costs for the batch prediction.\n",
    "\n",
    "**2. Output Bigquery**\n",
    "- Kết quả của batch prediction sẽ **luôn luôn trên 1 bảng mới** : job create the `table` and or the `dataset` automatically\n",
    "- Sẽ bao gồm cột `instance` + `prediction`, mỗi dòng tương ứng với 1 record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve Vertex model by online prediction (endpoint - realtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bằng cách deploy model vào 1 endpoint (1 endpoint có thể chứa nhiều model cùng deploy vào). Khi đó, cách serve model được thực hiện tương tự với action call API và trả ra output một cách real-time.\n",
    "\n",
    "**Limitations**\n",
    "- Request Body and Response Body bị giới hạn với kích thước 1.5MB, có thể phải dùng giải pháp khác là lưu output tại GCS là trả ra kết quả là path tại GCS\n",
    "- Sẽ luôn phải trả phí cho ít nhất 1 instance buộc phải chạy liên tục (để hứng chờ request), nếu số request tăng lên thì tự động autoscale theo config. Sẽ không phù hợp với việc sử dụng nhiều loại model và mỗi model phải sử dụng 1 endpoint. Thay vào đó, chuyển hướng sang sử dụng **Cloud Run**\n",
    "\n",
    "**Cost**\n",
    "\n",
    "Chi phí được tính theo used [***resource per node hour***](https://cloud.google.com/vertex-ai/pricing#pred_apac). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model enpoint\n",
    "\n",
    "Trước khi deploy model, ta cần tạo **endpoint** để từ đó sẽ đẩy online request và nhận lại real time prediction\n",
    "\n",
    "**Endpoint** là nơi mà model có thể deploy vào đó. 1 **Endpoint** có thể chứa nhiều model version hoặc model type khác nhau hoặc có thể tái sử dụng cho model version mới hơn, tuỳ thuộc vào cách chia `%traffic` cho từng deployed model trong endpoint thì sẽ có tỷ lệ 1 request sẽ run bởi model nào --> Phù hợp cho việc testing và chuyển đổi giữa nhiều model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud ai endpoints create \\\n",
    "  --project=ext-pinetree-dw \\\n",
    "  --region=asia-southeast1 \\\n",
    "  --display-name=sentiment-fast-api-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy model to an enpoint\n",
    "\n",
    "Deploy model to exited endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud ai endpoints deploy-model <endpoint_id> \\\n",
    "  --project=ext-pinetree-dw \\\n",
    "  --region=asia-southeast1 \\\n",
    "  --model=<model_id> \\\n",
    "  --display-name=sentiment-fast-api-model-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "project = \"ext-pinetree-dw\"\n",
    "location = \"asia-southeast1\"\n",
    "project_model_id = \"234439745674\"\n",
    "endpoint_id = \"7608484124768075776\"\n",
    "\n",
    "aiplatform.init(project=project, location=location)\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    f\"projects/{project_model_id}/locations/{location}/endpoints/{endpoint_id}\"\n",
    ")\n",
    "\n",
    "instances = [\n",
    "    {\"text\": \"DoiT is a great company.\"},\n",
    "    {\"text\": \"The beach was nice but overall the hotel was very bad.\"},\n",
    "]\n",
    "\n",
    "prediction = endpoint.predict(instances=instances)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving model container - Cloud Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
