{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve model deploy to vertex AI\n",
    "\n",
    "references:\n",
    "- https://medium.com/google-cloud/serving-machine-learning-models-with-google-vertex-ai-5d9644ededa3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new virtual environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create a virtual environment\n",
    "python -m venv .venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "# Source .venv/bin/activate\n",
    ".venv\\Scripts\\activate\n",
    "\n",
    "# (Optional) Deactivate conda environment\n",
    "conda deactivate\n",
    "\n",
    "# Upgrade pip\n",
    "python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a `requirements.txt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "uvicorn[standard]==0.20.0\n",
    "gunicorn==23.0.0\n",
    "fastapi[standard]==0.115.0\n",
    "scikit-learn==1.5.2\n",
    "pytest==8.3.3\n",
    "starlette==0.38.6\n",
    "requests==2.32.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import random\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SimpleSentimentModel(BaseEstimator, TransformerMixin):\n",
    "    negative_length_threshold = 10\n",
    "    positive_length_threshold = 30\n",
    "    negative_ls = [\"tiêu cực\", \"xấu\", \"tệ\", \"negative\"]\n",
    "    positive_ls = [\"tích cực\", \"thích\", \"positive\"]\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, text):\n",
    "        text_lower = text.lower()\n",
    "        if any(word in text_lower for word in self.negative_ls):\n",
    "            return \"negative\", random.randrange(90, 100, step=1) / 100\n",
    "        elif any(word in text_lower for word in self.positive_ls):\n",
    "            return \"positive\", random.randrange(90, 100, step=1) / 100\n",
    "        elif len(text) <= self.negative_length_threshold:\n",
    "            return \"negative\", random.randrange(70, 90, step=1) / 100\n",
    "        elif len(text) >= self.positive_length_threshold:\n",
    "            return \"positive\", random.randrange(70, 90, step=1) / 100\n",
    "        else:\n",
    "            return \"neutral\", random.randrange(70, 95, step=1) / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "from model import SimpleSentimentModel  # Ensure this import is correct\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the model\n",
    "    model = SimpleSentimentModel()\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    model_dir = \"models\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    # Save the model using joblib, ensuring correct context\n",
    "    joblib.dump(model, os.path.join(model_dir, \"model.pkl\"))\n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main app API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import joblib\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from model import SimpleSentimentModel  # noqa: F401\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Sentiment Analysis API\")\n",
    "\n",
    "# Load the model with a safe file path\n",
    "model_path = os.path.join(\"models\", \"model.pkl\")\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "# Load the model, making sure SimpleSentimentModel is already imported\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# Pydantic models for prediction results\n",
    "class Prediction(BaseModel):\n",
    "    sentiment: str\n",
    "    confidence: Optional[float]\n",
    "\n",
    "\n",
    "class Predictions(BaseModel):\n",
    "    predictions: List[Prediction]\n",
    "\n",
    "\n",
    "# Function to process batch predictions\n",
    "def get_prediction(instances):\n",
    "    res = []\n",
    "    for text in instances:\n",
    "        sentiment, confidence = model.predict(text)\n",
    "        res.append(Prediction(sentiment=sentiment, confidence=confidence))\n",
    "    return Predictions(predictions=res)\n",
    "\n",
    "\n",
    "# Health check route\n",
    "@app.get(\"/health\", status_code=200)\n",
    "async def health():\n",
    "    return {\"health\": \"ok\"}\n",
    "\n",
    "\n",
    "# Prediction route to handle batch requests\n",
    "@app.post(\n",
    "    \"/predict\",\n",
    "    response_model=Predictions,\n",
    "    response_model_exclude_unset=True,\n",
    ")\n",
    "async def predict(request: Request):\n",
    "    # Extract the JSON body from the request\n",
    "    body = await request.json()\n",
    "\n",
    "    # Validate the request body\n",
    "    if \"instances\" not in body or not isinstance(body[\"instances\"], list):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"Invalid input format. 'instances' should be a list of texts.\",\n",
    "        )\n",
    "\n",
    "    # Extract the instances (texts) from the request\n",
    "    instances = [x[\"text\"] for x in body[\"instances\"]]\n",
    "\n",
    "    # Get predictions\n",
    "    output = get_prediction(instances)\n",
    "\n",
    "    # Return the predictions\n",
    "    return output\n",
    "\n",
    "\n",
    "# Main function to run the FastAPI app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8080)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test app**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "from main import app\n",
    "\n",
    "client = TestClient(app=app)\n",
    "base_url = \"\"\n",
    "\n",
    "\n",
    "def test_health():\n",
    "    response = client.get(f\"{base_url}/health\")\n",
    "    assert response.status_code == 200\n",
    "    assert response.json() == {\"health\": \"ok\"}\n",
    "    print(\"pass: test_health\")\n",
    "\n",
    "\n",
    "def test_predict_item():\n",
    "    response = client.post(\n",
    "        f\"{base_url}/predict\",\n",
    "        json={\n",
    "            \"instances\": [\n",
    "                {\"text\": \"Cong hoa xa hoi chu nghia\"},\n",
    "                {\"text\": \"doc lap\"},\n",
    "                {\"text\": \"doc lap tich cuc\"},\n",
    "                {\"text\": \"te doc\"},\n",
    "                {\"text\": \"positive doc lap\"},\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "    result = response.json()\n",
    "    sentiments = [i[\"sentiment\"] for i in result[\"predictions\"]]\n",
    "    assert sentiments == [\n",
    "        \"neutral\",\n",
    "        \"negative\",\n",
    "        \"neutral\",\n",
    "        \"negative\",\n",
    "        \"positive\",\n",
    "    ]\n",
    "    print(\"pass: test_predict_item\")\n",
    "\n",
    "\n",
    "def test_predict_item_non_instance():\n",
    "    response = client.post(\n",
    "        f\"{base_url}/predict\",\n",
    "        json={\n",
    "            \"instan\": [\n",
    "                {\"text\": \"Cong hoa xa hoi chu nghia\"},\n",
    "                {\"text\": \"doc lap\"},\n",
    "                {\"text\": \"doc lap tich cuc\"},\n",
    "                {\"text\": \"te doc\"},\n",
    "                {\"text\": \"positive doc lap\"},\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    assert response.status_code == 400\n",
    "    response.json() == {\n",
    "        \"detail\": \"Invalid input format. 'instances' should be a list of texts.\"\n",
    "    }\n",
    "    print(\"pass: test_predict_item_non_instance\")\n",
    "\n",
    "\n",
    "def test_predict_item_not_list():\n",
    "    response = client.post(\n",
    "        f\"{base_url}/predict\",\n",
    "        json={\"instan\": {\"text\": \"Cong hoa xa hoi chu nghia\"}},\n",
    "    )\n",
    "    assert response.status_code == 400\n",
    "    response.json() == {\n",
    "        \"detail\": \"Invalid input format. 'instances' should be a list of texts.\"\n",
    "    }\n",
    "    print(\"pass: test_predict_item_not_list\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test for running container\n",
    "    import requests\n",
    "\n",
    "    client = requests\n",
    "    base_url = \"http://127.0.0.1:8080\"\n",
    "    test_health()\n",
    "    test_predict_item()\n",
    "    test_predict_item_non_instance()\n",
    "    test_predict_item_not_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pytest in `cmd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pytest test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model image to Artifact Registry (GCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "**Write Dockerfile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM tiangolo/uvicorn-gunicorn:python3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY *.py ./\n",
    "COPY models ./models\n",
    "COPY requirements.txt ./requirements.txt\n",
    "\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r ./requirements.txt\n",
    "\n",
    "EXPOSE 8080\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Push image bằng docker-command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build docker image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker build -t asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test image container**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm -p 8080:8080 asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Push image to Artifact Registry (GCP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authen GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# docker login\n",
    "gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker push asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Push image bằng cloud-build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Config cloud build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "steps:\n",
    "# If training model in cloud and save model in GCS\n",
    "# Assume Storage location of model: `gs://dev-aiml-model/models/sentiment`\n",
    "# Download the model file in GCS to embed it into the image\n",
    "  - name: 'gcr.io/cloud-builders/gsutil'\n",
    "    args: ['cp', '-r', '${_MODEL_GCS_PATH}', './models']\n",
    "    id: 'download-model'\n",
    "  \n",
    "  # Build the container image\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    args: ['build', '-t', '${_IMAGE_NAME}', '.']\n",
    "    waitFor: ['download-model']\n",
    "  \n",
    "  # Push the container image to Artifact Registry\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    args: ['push', '${_IMAGE_NAME}']\n",
    "\n",
    "images:\n",
    "  - '${_IMAGE_NAME}'\n",
    "\n",
    "# Substitution variables for flexibility\n",
    "substitutions:\n",
    "  _MODEL_GCS_PATH: 'gs://dev-aiml-model/models/sentiment'\n",
    "  _IMAGE_NAME: 'asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run cloud build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud builds submit --config cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving model container - Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Container Requirement**\n",
    "\n",
    "The docker container needs to follow the [container requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) defined by Google. The most important requirement is: \n",
    "\n",
    "---\n",
    "**1. HTTP server**\n",
    "\n",
    "Provide an `HTTP server` that listens for requests on `0.0.0.0` (must) on port `8080` (can be choice).\n",
    "\n",
    "**HTTP Server** can be using:\n",
    "- **Flask** , **FastAPI**, ...\n",
    "- **TensorFlow Serving**, **TorchServe**, or **KServe Python Server**\n",
    "- ...\n",
    "\n",
    "[**HTTP Server** can be run by](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#server):\n",
    "- [ENTRYPOINT instruction](https://docs.docker.com/engine/reference/builder/#entrypoint), [CMD instruction](https://docs.docker.com/engine/reference/builder/#cmd) or both in ***Dockerfile***\n",
    "- Specify the `containerSpec.command` and `containerSpec.args` fields when you create your `Model` resource (override your container image's `ENTRYPOINT` and `CMD`)\n",
    "\n",
    "---\n",
    "**2. Health checks**\n",
    "\n",
    "***a. startup probe*** (optional)\n",
    "\n",
    "Check whether the container application has started. Nếu không cung cấp thì sẽ ko chạy, và ngay lập tức chạy ***health probe***\n",
    "\n",
    "**Usecase**: Cần sử dụng cho các application cần có thời gian khởi động trong lần đầu tiên. Ví dụ, Nếu App cần thời gian để copy file model mới từ source bên ngoài container mỗi lần khởi động. Chúng ta có thể config ***startup probe*** để chờ cho đến khi việc copy hoàn thành và trả ra success\n",
    "\n",
    "\n",
    "***b. health probe***\n",
    "\n",
    "Check whether the container application is ready to accept traffic or receive request. Nếu không cung cấp path cụ thể thì Vertex sẽ sử dụng default path `/health`. Lưu ý là ***health probe*** chỉ chạy khi ***startup probe*** hoàn thành hoặc không được khai báo\n",
    "\n",
    "Provide an `HTTP path` for **health checks** (default path `/health` with `HTTP GET`, it can be change in config): \n",
    "- Return a `200` within **10 seconds** after call when you’re container is ready to handle requests. Nội dung của phần phản hồi không quan trọng, vì Vertex AI sẽ bỏ qua chúng. Phản hồi này cho thấy rằng server đang hoạt động tốt (healthy). For example, if you need to load the model, ensure you return the `200` status code after the model is loaded.\n",
    "- **If the server isn't ready to handle prediction requests**, nó không nên phản hồi yêu cầu trong vòng **10 giây**, hoặc phản hồi với bất kỳ mã trạng thái nào khác ngoài `200 OK`, ví dụ như `503 Service Unavailable`. Điều này cho thấy server đang không hoạt động tốt (unhealthy).\n",
    "\n",
    "Nếu health probe nhận được phản hồi không tốt từ server (bao gồm cả trường hợp không có phản hồi trong vòng 10 giây), nó sẽ gửi thêm **tối đa 3 lần Health Checks nữa**, mỗi lần cách nhau **10 giây**. Trong khoảng thời gian này, Vertex AI vẫn coi server là hoạt động tốt. Nếu probe nhận được phản hồi tốt từ bất kỳ lần kiểm tra nào, nó sẽ quay lại **Health checks Process**. Tuy nhiên, **nếu probe nhận được 4 phản hồi không tốt liên tiếp**, Vertex AI sẽ dừng việc chuyển tiếp các yêu cầu dự đoán tới container đó (nếu mô hình được triển khai trên nhiều node, các yêu cầu sẽ được chuyển tới các container khác đang hoạt động tốt).\n",
    "\n",
    "Vertex AI không khởi động lại container; thay vào đó, health probe vẫn sẽ tiếp tục gửi các yêu cầu kiểm tra định kỳ tới server không tốt. Nếu nhận được phản hồi tốt, container đó sẽ được đánh dấu là hoạt động tốt và bắt đầu nhận lại yêu cầu dự đoán.\n",
    "\n",
    "**Hướng dẫn thực tế:**\n",
    "- Trong nhiều trường hợp, **server HTTP** trong container của bạn có thể luôn phản hồi với mã trạng thái `200 OK` cho các yêu cầu kiểm tra sức khỏe. Nếu container tải các tài nguyên trước khi khởi động server, container sẽ không hoạt động tốt trong thời gian khởi động và bất kỳ lúc nào server HTTP gặp lỗi. Trong tất cả các thời gian khác, nó sẽ phản hồi là tốt.\n",
    "\n",
    "- Đối với cấu hình phức tạp hơn, bạn có thể thiết kế server HTTP để cố tình phản hồi yêu cầu kiểm tra sức khỏe với trạng thái không tốt vào những thời điểm nhất định. Ví dụ, bạn có thể chặn lưu lượng dự đoán tới node trong một khoảng thời gian để container thực hiện bảo trì.\n",
    "\n",
    "---\n",
    "**3. Prediction**\n",
    "\n",
    "Provide an `HTTP path` for **prediction** (default path `/predict` with `HTTP POST`, it can be change in config)\n",
    "- `Content-Type: application/json` HTTP header\n",
    "\n",
    "---\n",
    "\n",
    "**4. Request body**\n",
    "\n",
    "The request body is `JSON` format and must be 1.5 MB or smaller, need contain an `instances` key and can be has `parameters` :\n",
    "```JSON\n",
    "{\n",
    "   \"instances\":[\n",
    "      {\n",
    "         \"text\":\"DoiT is a great company.\"\n",
    "      },\n",
    "      {\n",
    "         \"text\":\"The beach was nice but overall the hotel was very bad.\"\n",
    "      }\n",
    "   ],\n",
    "   \"parameters\": {}\n",
    "}\n",
    "```\n",
    "- `instances` take is an array of **one or more JSON values** of any type. Each values represents an instance that you are providing a prediction for.\n",
    "- `parameters` (optional if application is designed to require it) take a JSON object containing any parameters that your container requires to help serve predictions on the instances\n",
    "\n",
    "---\n",
    "\n",
    "**5. Response body**\n",
    "\n",
    "The response body is `JSON` format and must be 1.5 MB or smaller, need contain an `predictions` key :\n",
    "```JSON\n",
    "{\n",
    " \"predictions\": [\n",
    "   {\n",
    "     \"confidence\": 0.9409326314926147,\n",
    "     \"sentiment\": \"POSITIVE\"\n",
    "   }\n",
    " ],\n",
    "  \"deployedModelId\": <string>, # id of the Endpoint's DeployedModel\n",
    "  \"model\": <string>, # The resource name of the Model\n",
    "  \"modelVersionId\": <string>, # The version id of the Model\n",
    "  \"modelDisplayName\": <string>, # The display name of the Model \n",
    "  \"metadata\": <value> # Request-level metadata returned by the model\n",
    "}\n",
    "```\n",
    "- `predictions` take is an array of **one or more JSON values** representing the predictions that your container has generated for each of the INSTANCES in the corresponding request.\n",
    "\n",
    "**6. Publishing requirements**\n",
    "- Location: `asia-southeast1`\n",
    "- [Permissions](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#permissions)\n",
    "- [Environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#variables)\n",
    "\n",
    "**7. Access model artifacts**\n",
    "\n",
    "[Doc](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#artifacts)\n",
    "\n",
    "- **Nếu sử dụng pre-build container làm môi trường**: Thì phải cung cấp địa chỉ tại GCS (folder) chứa các file model được training sẽ chạy trên environment build từ pre-build container đó\n",
    "\n",
    "- **Nếu sử dụng custom container làm môi trường**: Việc cung cấp địa chỉ GCS (folder) chứa các file trained model là optional, nó cần thiết trong việc sử dụng custom container chỉ làm environment runtime và ko chứa sẵn model, khi đó cần phải copy model vào để run trong environment đó. Còn nếu trong container chứa sẵn file model thì việc cung cấp địa chỉ folder (GCS) chứa file model là ko cần thiết"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to Model Registry (VertexAI)\n",
    "\n",
    "Ta cần import Model Image từ **Artifact Registry** sang **Vertex AI** để có thể tận dụng các tính năng quản lý model AI của Vertex và serve được model.\n",
    "\n",
    "**Chi phí sử dụng Model Registry (VertexAI)**: No Cost\n",
    "\n",
    "Chỉ phát sinh chi phí khi sử dụng prediction: `Online prediction via Endpoint` hoặc `Batch Prediction`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import bằng giao diện UI: [Doc](https://cloud.google.com/vertex-ai/docs/model-registry/import-model#custom-container)\n",
    "\n",
    "<img src = \"_image/import_model_registry.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import model command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud ai models upload \\\n",
    "  --container-ports=8080 \\\n",
    "  --container-predict-route=\"/predict\" \\\n",
    "  --container-health-route=\"/health\" \\\n",
    "  --region=asia-southeast1 \\\n",
    "  --display-name=sentiment-fast-api \\\n",
    "  --container-image-uri=asia-southeast1-docker.pkg.dev/ext-pinetree-dw/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Serve Vertex model by batch prediction\n",
    "\n",
    "**Batch Prediction** là gửi request trực tiếp tới Model đã được imported vào **Model Registry** mà Model này không cần deploy thành endpoint. Khi đó data gửi vào trong 1 single request (có thể large size) và không yêu cầu reponse trả ra real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost\n",
    "\n",
    "Chi phí được tính bằng thời gian sử dụng [***resource per node hour***](https://cloud.google.com/vertex-ai/pricing#pred_apac), tổng của:\n",
    "- **vCPU cost**: measured in vCPU hours\n",
    "- **RAM cost**: measured in GB hours\n",
    "- **GPU cost**: if either built into the machine or optionally configured, measured in GPU hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config Input data\n",
    "\n",
    "Input for batch prediction:\n",
    "- CSV file\n",
    "- File-list in GCS\n",
    "- Bigquery table\n",
    "- JSON Line (JSONL)\n",
    "- `tf-record` or `tf-record-gzip`\n",
    "\n",
    "> To use a BigQuery table as input, you must set [`InstanceConfig.instanceType`](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#instanceconfig) to `object` using the Vertex AI API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "##### [Input data requirement](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#input_data_requirements)\n",
    "\n",
    "**1. JSON Lines**\n",
    "\n",
    "[JSON Lines](https://jsonlines.org/) file store in a Cloud Storage bucket.\n",
    "\n",
    "- JSON Lines file where each line contains an **array**:\n",
    "\n",
    "    ***data is:***\n",
    "    ```text\n",
    "    [1, 2, 3, 4]\n",
    "    [5, 6, 7, 8]\n",
    "    ```\n",
    "\n",
    "    ***convert to request body is:***\n",
    "    ```json\n",
    "    {\"instances\": [ [1, 2, 3, 4], [5, 6, 7, 8] ]}\n",
    "    ```\n",
    "\n",
    "- JSON Lines file where each line contains an **object**:\n",
    "\n",
    "    ***data is:***\n",
    "    ```text\n",
    "    { \"values\": [1, 2, 3, 4], \"key\": 1 }\n",
    "    { \"values\": [5, 6, 7, 8], \"key\": 2 }\n",
    "    ```\n",
    "\n",
    "    ***convert to request body is:***\n",
    "    ```json\n",
    "    {\"instances\": [\n",
    "      { \"values\": [1, 2, 3, 4], \"key\": 1 },\n",
    "      { \"values\": [5, 6, 7, 8], \"key\": 2 }\n",
    "    ]}\n",
    "    ```\n",
    "\n",
    "> Note: For **PyTorch prebuilt containers**, Vertex AI wraps each instance in a `data` field before sending it to the prediction container. This is because TorchServe's default handlers expect each instance to be wrapped in a `data` field.\n",
    "\n",
    "---\n",
    "**2. Bigquery**\n",
    "\n",
    "Vertex AI transforms each row from the table to a JSON instance.\n",
    "\n",
    "***data table is:***\n",
    "\n",
    "| column 1 | column 2 | column 3|\n",
    "|----------|----------|---------|\n",
    "| 1.0 | 3.0 | \"Cat1\"|\n",
    "| 2.0 | 4.0 | \"Cat2\"|\n",
    "\n",
    "***convert to request body is:***\n",
    "```json\n",
    "{\"instances\": [ [1.0,3.0,\"cat1\"], [2.0,4.0,\"cat2\"] ]}\n",
    "```\n",
    "\n",
    "**Convert datatype from bigquery to body request**\n",
    "\n",
    "| **BigQuery Type** | **JSON Type** | **Example value**                  |\n",
    "| ----------------- | ------------- | ---------------------------------- |\n",
    "| String            | String        | \"abc\"                              |\n",
    "| Integer           | Integer       | 1                                  |\n",
    "| Float             | Float         | 1.2                                |\n",
    "| Numeric           | Float         | 4925.000000000                     |\n",
    "| Boolean           | Boolean       | true                               |\n",
    "| TimeStamp         | String        | \"2019-01-01 23:59:59.999999+00:00\" |\n",
    "| Date              | String        | \"2018-12-31\"                       |\n",
    "| Time              | String        | \"23:59:59.999999\"                  |\n",
    "| DateTime          | String        | \"2019-01-01T00:00:00\"              |\n",
    "| Record            | Object        | { \"A\": 1,\"B\": 2}                   |\n",
    "| Repeated Type     | Array[Type]   | [1, 2]                             |\n",
    "| Nested Record     | Object        | {\"A\": {\"a\": 0}, \"B\": 1}            |\n",
    "\n",
    "---\n",
    "**3. CSV**\n",
    "\n",
    "- One input instance per row in a CSV file. \n",
    "- The first row must be a header row. \n",
    "- Enclose all strings in double quotation marks (\"). \n",
    "- Vertex AI doesn't accept cell values that contain newlines. \n",
    "- Non-quoted values are read as floating point numbers.\n",
    "\n",
    "***data in csv:***\n",
    "\n",
    "```text\n",
    "\"input1\",\"input2\",\"input3\"\n",
    "0.1,1.2,\"cat1\"\n",
    "4.0,5.0,\"cat2\"\n",
    "```\n",
    "\n",
    "***convert to request body is:***\n",
    "```json\n",
    "{\"instances\": [ [0.1,1.2,\"cat1\"], [4.0,5.0,\"cat2\"] ]}\n",
    "```\n",
    "\n",
    "---\n",
    "**4. File list**\n",
    "\n",
    "---\n",
    "**5. TFRecord**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partition data\n",
    "\n",
    "**MapReduce** được sử dụng để chia data thành các replica, với yêu cầu data có khả năng partition:\n",
    "- Automatically partitions **BigQuery**, **file list**, and **JSON lines** input (sử dụng mỗi partition thành 1 replica)\n",
    "- **CSV** not able to partition\n",
    "- **TFRecord** partition by file (mỗi file là 1 replica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter and transformation data\n",
    "\n",
    "- **Filter**: cấu hình subset field would be selected or exclude to input data\n",
    "\n",
    "- **Transform**: config to `instanceType` is `array` or `object` format\n",
    "\n",
    "Ví dụ: cấu hình [`instanceConfig`](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#instanceconfig) trong [`BatchPredictionJob`](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"batchJob1\",\n",
    "  ...\n",
    "  \"instanceConfig\": {\n",
    "    \"excludedFields\":[\"customerId\"] # remove 'customerID' column\n",
    "    \"instanceType\":\"object\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"batchJob1\",\n",
    "  ...\n",
    "  \"instanceConfig\": {\n",
    "    \"includedFields\": [\"col1\",\"col2\"] # include col1 + col2\n",
    "    \"instanceType\":\"object\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request a batch prediction\n",
    "\n",
    "1. [Request by Google Cloud Console](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#google-cloud-console)\n",
    "2. [Request by Python API](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#aiplatform_batch_predict_custom_trained-python_vertex_ai_sdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_JOB_NAME = \"penguins-test\"\n",
    "MODEL_URI = model.resource_name\n",
    "INPUT_FORMAT = \"bigquery\"\n",
    "INPUT_URI = f\"bq://{TABLE_ID}\"\n",
    "OUTPUT_FORMAT = \"bigquery\"\n",
    "OUTPUT_URI = f\"bq://{PROJECT_ID}\"\n",
    "MACHINE_TYPE = \"n1-standard-2\"\n",
    "EXCLUDED_FIELDS = [ID_COLUMN_NAME]\n",
    "\n",
    "# Create a list of columns to be included\n",
    "ALL_COLUMNS = list(df_x_with_id.columns)\n",
    "INCLUDED_FIELDS = ALL_COLUMNS.copy()\n",
    "INCLUDED_FIELDS.remove(ID_COLUMN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Exclude fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON body requests - Exclude fields\n",
    "import json\n",
    "\n",
    "request_with_excluded_fields = {\n",
    "    \"displayName\": f\"{BATCH_JOB_NAME}-excluded_fields\",\n",
    "    \"model\": MODEL_URI,\n",
    "    \"inputConfig\": {\n",
    "        \"instancesFormat\": INPUT_FORMAT,\n",
    "        \"bigquerySource\": {\"inputUri\": INPUT_URI},\n",
    "    },\n",
    "    \"outputConfig\": {\n",
    "        \"predictionsFormat\": OUTPUT_FORMAT,\n",
    "        \"bigqueryDestination\": {\"outputUri\": OUTPUT_URI},\n",
    "    },\n",
    "    \"dedicatedResources\": {\n",
    "        \"machineSpec\": {\n",
    "            \"machineType\": MACHINE_TYPE,\n",
    "        }\n",
    "    },\n",
    "    \"instanceConfig\": {\"excludedFields\": EXCLUDED_FIELDS},\n",
    "}\n",
    "\n",
    "with open(\"request_with_excluded_fields.json\", \"w\") as outfile:\n",
    "    json.dump(request_with_excluded_fields, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to run job\n",
    "! curl \\\n",
    "  -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d @request_with_excluded_fields.json \\\n",
    "  https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/batchPredictionJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Include fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON body requests - Include fields\n",
    "request_with_included_fields = {\n",
    "    \"displayName\": f\"{BATCH_JOB_NAME}-included_fields\",\n",
    "    \"model\": MODEL_URI,\n",
    "    \"inputConfig\": {\n",
    "        \"instancesFormat\": INPUT_FORMAT,\n",
    "        \"bigquerySource\": {\"inputUri\": INPUT_URI},\n",
    "    },\n",
    "    \"outputConfig\": {\n",
    "        \"predictionsFormat\": OUTPUT_FORMAT,\n",
    "        \"bigqueryDestination\": {\"outputUri\": OUTPUT_URI},\n",
    "    },\n",
    "    \"dedicatedResources\": {\n",
    "        \"machineSpec\": {\n",
    "            \"machineType\": MACHINE_TYPE,\n",
    "        }\n",
    "    },\n",
    "    \"instanceConfig\": {\"includedFields\": INCLUDED_FIELDS},\n",
    "}\n",
    "\n",
    "with open(\"request_with_included_fields.json\", \"w\") as outfile:\n",
    "    json.dump(request_with_included_fields, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to run job\n",
    "! curl \\\n",
    "  -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d @request_with_included_fields.json \\\n",
    "  https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/batchPredictionJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or use Python API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input data source: **GCS JSON Lines**\n",
    "- Output data source: **GCS JSON Lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def run_vertex_ai_batch_prediction(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_resource_name: str,\n",
    "    job_display_name: str,\n",
    "    gcs_source: Union[str, Sequence[str]],\n",
    "    gcs_destination: str,\n",
    "    instances_format: str = \"jsonl\",\n",
    "    machine_type: str = \"n1-standard-2\",\n",
    "    accelerator_count: int = 1,\n",
    "    accelerator_type: Union[\n",
    "        str, aiplatform_v1.AcceleratorType\n",
    "    ] = \"NVIDIA_TESLA_K80\",\n",
    "    starting_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    sync: bool = True,\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = my_model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        instances_format=instances_format,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        accelerator_type=accelerator_type,\n",
    "        starting_replica_count=starting_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    batch_prediction_job.wait()\n",
    "\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.resource_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job\n",
    "\n",
    "\n",
    "# Example usage\n",
    "run_vertex_ai_batch_prediction(\n",
    "    project=\"your-gcp-project-id\",\n",
    "    region=\"asia-southeast1\",\n",
    "    model_resource_name=\"projects/your-project/locations/us-central1/models/your-model-id\",\n",
    "    input_bq_uri=\"bq://your-project.your_dataset.your_input_table\",\n",
    "    output_bq_uri=\"bq://your-project.your_dataset.your_output_table\",\n",
    "    job_display_name=\"your_batch_prediction_job_name\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input data source: **Bigquery**\n",
    "- Output data source: **Bigquery**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def run_vertex_ai_batch_prediction(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    model_resource_name: str,\n",
    "    input_bq_uri: str,\n",
    "    output_bq_uri: str,\n",
    "    job_display_name: str,\n",
    "    instance_format: str = \"bigquery\",\n",
    "    predictions_format: str = \"bigquery\",\n",
    "    machine_type: str = \"n1-standard-4\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a batch prediction job with a Vertex AI model, using BigQuery tables as input and output.\n",
    "\n",
    "    Args:\n",
    "        project: GCP project ID.\n",
    "        region: GCP region where the Vertex AI model is hosted.\n",
    "        model_resource_name: Full resource name of the Vertex AI model.\n",
    "        input_bq_uri: BigQuery table URI for the input data (e.g., bq://project.dataset.table).\n",
    "        output_bq_uri: BigQuery table URI for the output data.\n",
    "        job_display_name: Name to display for the batch prediction job.\n",
    "        instance_format: The format for instance data, typically 'bigquery'.\n",
    "        predictions_format: The format for predictions, typically 'bigquery'.\n",
    "        machine_type: Machine type for the batch prediction job.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Vertex AI client\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    # Create the batch prediction job\n",
    "    batch_prediction_job = aiplatform.BatchPredictionJob.create(\n",
    "        job_display_name=job_display_name,\n",
    "        model=model_resource_name,\n",
    "        bigquery_source=input_bq_uri,\n",
    "        bigquery_destination_prefix=output_bq_uri,\n",
    "        instances_format=instance_format,\n",
    "        predictions_format=predictions_format,\n",
    "        machine_type=machine_type,\n",
    "        sync=True,  # Set to False if you don't want to wait for the job to finish\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Batch prediction job created: {batch_prediction_job.resource_name}\"\n",
    "    )\n",
    "    print(f\"Check the job's status in the Vertex AI console.\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "run_vertex_ai_batch_prediction(\n",
    "    project=\"your-gcp-project-id\",\n",
    "    region=\"asia-southeast1\",\n",
    "    model_resource_name=\"projects/your-project/locations/us-central1/models/your-model-id\",\n",
    "    input_bq_uri=\"bq://your-project.your_dataset.your_input_table\",\n",
    "    output_bq_uri=\"bq://your-project.your_dataset.your_output_table\",\n",
    "    job_display_name=\"your_batch_prediction_job_name\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine type & replica count\n",
    "\n",
    "- **Batch**: Tất cả dữ liệu sẽ được chia thành nhiều nhóm (batches), mỗi batch với kích thước batch_size và lần lượt sẽ được đưa vào model để dự đoán.\n",
    "- **Replica**: Replica là một bản sao của machine được cấu hình để thực hiện prediction. Hệ thống sẽ tạo ra nhiều replica giống nhau, 1 replica sẽ xử lý 1 batch nhất định song song với replica khác. Hệ thống sẽ tự động phân chia dữ liệu và gán cho từng replica để xử lý, điều này giúp tăng tốc đáng kể cho các công việc lớn.\n",
    "\n",
    "Trong batch prediction, toàn bộ dữ liệu đầu vào được chia thành các batch và mỗi replica sẽ xử lý một số lượng batch nhất định. Hệ thống sử dụng một cơ chế tương tự như MapReduce để phân chia dữ liệu cho các replicas.\n",
    "\n",
    "Quá trình phân chia này hoạt động hiệu quả nếu dữ liệu có thể được partitioned, tức là có thể chia thành các phần độc lập để các replicas xử lý song song. \n",
    "- Dữ liệu đầu vào từ BigQuery, danh sách file, và file JSON lines là những loại có thể tự động được phân chia. \n",
    "- Tuy nhiên, dữ liệu CSV không phải là định dạng phù hợp cho việc này do nó không dễ chia nhỏ một cách an toàn và hiệu quả.\n",
    "\n",
    "**Recommendation**: ***specify the smallest machine type possible for your job and increase the number of replicas.*** Với mỗi 1 batch sẽ được chạy bởi 1 replica\n",
    "\n",
    "**Thời gian chạy mỗi replica**\n",
    "\n",
    "Để tối ưu chi phí, nên lựa chọn số replica sao cho thời gian chạy mỗi replica tối thiểu là 10 phút. Do billed được tính dựa trên per replica node hour, mà trong đó đã mất khoảng 5 phút để khởi động cho mỗi 1 replica, nếu thời gian chạy predition mỗi replica quá thấp thì chi phí chủ yếu chỉ là thời gian khởi tạo replica.\n",
    "\n",
    "Cụ thể:\n",
    "\n",
    "- Mỗi replica được phân chia một tập hợp các batch dữ liệu khi công việc dự đoán bắt đầu. Hệ thống sẽ tự động gán các batch này cho từng replica.\n",
    "- Quá trình hoạt động liên tục: Sau khi một replica xử lý xong một batch, nó sẽ chuyển sang batch tiếp theo trong danh sách được gán cho nó mà không cần khởi động lại.\n",
    "- Một replica chỉ dừng lại khi đã hoàn thành tất cả các batch mà nó được phân công hoặc khi toàn bộ công việc dự đoán kết thúc.\n",
    "\n",
    "**Số lượng replicas**\n",
    "\n",
    "- Với batch size <= 1 triệu bản ghi: Nên set `starting_replica_count` khoảng vài chục\n",
    "- Với batch size >= 1 triệu bản ghi: Nên set `starting_replica_count` khoảng vài trăm\n",
    "- Hoặc theo công thức: `Số replica` = `Số batches` / (`Số phút kỳ vọng hoàn thành batch prediction job` * `60` / `Số giây cần thiết để 1 replica hoàn thành được 1 batch`)\n",
    "    > `Số batches` = `Số bản ghi` / `batch_size`\n",
    "\n",
    "**Cấu hình resource**\n",
    "- Khác với online prediction, Batch prediction không autoscale resource\n",
    "- Sử dụng `starting_replica_count`, không sử dụng đến tham số `max_replica_count` (như online prediction)\n",
    "- Nếu sử dụng GPU, GPU machine types take more time to startup (10 minutes), nên khuyến nghị là kéo dài thời gian batch prediction để tận dụng được thời gian 1 lần khởi tạo replica không chiếm đa số thời gian của việc chạy job\n",
    "- Vertex AI tự động set partition (batch_size, tương đương với số lượng batches) tuỳ thuộc vào machine, data input and model type để đảm bảo về performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch prediction output\n",
    "---\n",
    "1. Nếu request body nhận được là **list of array**\n",
    "\n",
    "***request contains:***\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instances\": [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "]}\n",
    "```\n",
    "\n",
    "***The prediction container returns:***\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"predictions\": [\n",
    "    [0.1,0.9],\n",
    "    [0.7,0.3]\n",
    "  ],\n",
    "}\n",
    "```\n",
    "\n",
    "***Then the JSON Lines output file is:***\n",
    "\n",
    "```json\n",
    "{ \"instance\": [1, 2, 3, 4], \"prediction\": [0.1,0.9]}\n",
    "{ \"instance\": [5, 6, 7, 8], \"prediction\": [0.7,0.3]}\n",
    "```\n",
    "\n",
    "---\n",
    "2. Nếu request body nhận được là **list of object**\n",
    "\n",
    "***request contains:***\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instances\": [\n",
    "    {\"values\": [1, 2, 3, 4], \"key\": 1},\n",
    "    {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "]}\n",
    "```\n",
    "\n",
    "***The prediction container returns:***\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"predictions\": [\n",
    "    {\"result\":1},\n",
    "    {\"result\":0}\n",
    "  ],\n",
    "}\n",
    "```\n",
    "\n",
    "  ***Then the JSON Lines output file is:***\n",
    "\n",
    "  ```json\n",
    "  { \"instance\": {\"values\": [1, 2, 3, 4], \"key\": 1}, \"prediction\": {\"result\":1}}\n",
    "  { \"instance\": {\"values\": [5, 6, 7, 8], \"key\": 2}, \"prediction\": {\"result\":0}}\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve Vertex model by online prediction (endpoint - realtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model enpoint\n",
    "\n",
    "Trước khi deploy model, ta cần tạo **endpoint** để từ đó sẽ đẩy online request và nhận lại real time prediction\n",
    "\n",
    "**Endpoint** là nơi mà model có thể deploy vào đó. 1 **Endpoint** có thể chứa nhiều model version hoặc model type khác nhau hoặc có thể tái sử dụng cho model version mới hơn, tuỳ thuộc vào cách chia %traffic cho từng deployed model trong endpoint thì sẽ có tỷ lệ 1 request sẽ run bởi model nào --> Phù hợp cho việc testing và chuyển đổi giữa nhiều model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud ai endpoints create \\\n",
    "  --project=ext-pinetree-dw \\\n",
    "  --region=asia-southeast1 \\\n",
    "  --display-name=sentiment-fast-api-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy model to an enpoint\n",
    "\n",
    "Deploy model to exited endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gcloud ai endpoints deploy-model <endpoint_id> \\\n",
    "  --project=ext-pinetree-dw \\\n",
    "  --region=asia-southeast1 \\\n",
    "  --model=<model_id> \\\n",
    "  --display-name=sentiment-fast-api-model-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "project = \"ext-pinetree-dw\"\n",
    "location = \"asia-southeast1\"\n",
    "project_model_id = \"234439745674\"\n",
    "endpoint_id = \"7608484124768075776\"\n",
    "\n",
    "aiplatform.init(project=project, location=location)\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    f\"projects/{project_model_id}/locations/{location}/endpoints/{endpoint_id}\"\n",
    ")\n",
    "\n",
    "instances = [\n",
    "    {\"text\": \"DoiT is a great company.\"},\n",
    "    {\"text\": \"The beach was nice but overall the hotel was very bad.\"},\n",
    "]\n",
    "\n",
    "prediction = endpoint.predict(instances=instances)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving model container - Cloud Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
