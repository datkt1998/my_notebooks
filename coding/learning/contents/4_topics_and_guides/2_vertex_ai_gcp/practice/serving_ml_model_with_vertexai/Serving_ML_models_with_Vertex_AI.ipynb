{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMWhF8u7wLze"
      },
      "source": [
        "# Serving Machine Learning models with Google Vertex AI\n",
        "\n",
        "Great to have you here, this is the code for the following article:\n",
        "\n",
        "* https://medium.com/google-cloud/serving-machine-learning-models-with-google-vertex-ai-5d9644ededa3\n",
        "\n",
        "Your feedback and questions are highly appreciated. <br>You can find me on Twitter [@HeyerSascha](https://twitter.com/HeyerSascha) or connect with me via [LinkedIn](https://www.linkedin.com/in/saschaheyer/). <br>Even better, subscribe to my [YouTube](https://www.youtube.com/channel/UC--Sm3D-rqCUeLXmraypdPQ) channel ❤️."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "mb4l1zDsKZZP",
        "outputId": "ccef66ea-0755-4b72-fa9b-2fb8a8a0da65"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/brNMT7Snlh0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/brNMT7Snlh0\" frameborder=\"0\" allowfullscreen></iframe>'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJH4O6qUBEPQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9l9QSaRk09q",
        "outputId": "98777aee-689f-4876-faa2-d701b64db89d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project joyas-vietnam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your browser has been opened to visit:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=7mOLKckXlVY9JOuUG5cReF1Se1FHaN&access_type=offline&code_challenge=DfMbxtUPqVNvcwm7Q7D2rPuMOJYkofCHLWDr5M0UWao&code_challenge_method=S256\n",
            "\n",
            "\n",
            "Credentials saved to file: [/Users/datkhong/.config/gcloud/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"joyas-vietnam\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
            "\n",
            "\n",
            "Updates are available for some Google Cloud CLI components.  To install them,\n",
            "please run:\n",
            "  $ gcloud components update\n",
            "\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7oubOpffyzr"
      },
      "source": [
        "## Custom Prediction Container with FastAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = \"joyas-vietnam\"\n",
        "PIPELINE_ROOT = \"gs://dev-joyas-recommendation/\"\n",
        "LOCATION = \"asia\"\n",
        "# use this instead\n",
        "aiplatform.init(project=PROJECT_ID, location=\"asia-southeast1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE6YY_8xFwsC",
        "outputId": "8871b76f-6077-46d7-d144-bbc061edfd40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import uvicorn\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from fastapi import Request, FastAPI, Response\n",
        "from fastapi.responses import JSONResponse\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\"../sentiment\")\n",
        "\n",
        "app = FastAPI(title=\"Sentiment Analysis\")\n",
        "\n",
        "AIP_HEALTH_ROUTE = os.environ.get(\"AIP_HEALTH_ROUTE\", \"/health\")\n",
        "AIP_PREDICT_ROUTE = os.environ.get(\"AIP_PREDICT_ROUTE\", \"/predict\")\n",
        "\n",
        "\n",
        "class Prediction(BaseModel):\n",
        "    sentiment: str\n",
        "    confidence: Optional[float]\n",
        "\n",
        "\n",
        "class Predictions(BaseModel):\n",
        "    predictions: List[Prediction]\n",
        "\n",
        "\n",
        "# instad of creating a class we could have also loaded this information\n",
        "# from the model configuration. Better if you introduce new labels over time\n",
        "class Sentiment(Enum):\n",
        "    NEGATIVE = 0\n",
        "    POSITIVE = 1\n",
        "\n",
        "\n",
        "@app.get(AIP_HEALTH_ROUTE, status_code=200)\n",
        "async def health():\n",
        "    return {\"health\": \"ok\"}\n",
        "\n",
        "\n",
        "@app.post(\n",
        "    AIP_PREDICT_ROUTE,\n",
        "    response_model=Predictions,\n",
        "    response_model_exclude_unset=True,\n",
        ")\n",
        "async def predict(request: Request):\n",
        "    body = await request.json()\n",
        "    print(body)\n",
        "\n",
        "    instances = body[\"instances\"]\n",
        "    print(instances)\n",
        "    print(type(instances))\n",
        "    instances = [x[\"text\"] for x in instances]\n",
        "    print(instances)\n",
        "\n",
        "    tf_batch = tokenizer(\n",
        "        instances,\n",
        "        max_length=128,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"tf\",\n",
        "    )\n",
        "\n",
        "    print(tf_batch)\n",
        "\n",
        "    tf_outputs = model(tf_batch)\n",
        "\n",
        "    print(tf_outputs)\n",
        "\n",
        "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
        "    print(tf_predictions)\n",
        "\n",
        "    indices = np.argmax(tf_predictions, axis=-1)\n",
        "    confidences = np.max(tf_predictions, axis=-1)\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for index, confidence in zip(indices, confidences):\n",
        "        sentiment = Sentiment(index).name\n",
        "        print(index)\n",
        "        print(confidence)\n",
        "        outputs.append(Prediction(sentiment=sentiment, confidence=confidence))\n",
        "\n",
        "    return Predictions(predictions=outputs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True, host=\"0.0.0.0\", port=8080)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eB6Ro6IFwAh",
        "outputId": "1a968b7d-0f45-4865-d236-734fab9facf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile Dockerfile\n",
        "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
        "RUN pip install --no-cache-dir transformers==4.1.1 tensorflow==2.9.1 numpy==1.23.1 pydantic==1.9.1\n",
        "COPY main.py ./main.py\n",
        "# COPY ./sentiment /sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "#0 building with \"desktop-linux\" instance using docker driver\n",
            "\n",
            "#1 [internal] load build definition from Dockerfile\n",
            "#1 transferring dockerfile: 249B done\n",
            "#1 DONE 0.0s\n",
            "\n",
            "#2 [internal] load metadata for docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
            "#2 DONE 0.9s\n",
            "\n",
            "#3 [internal] load .dockerignore\n",
            "#3 transferring context: 2B done\n",
            "#3 DONE 0.0s\n",
            "\n",
            "#4 [internal] load build context\n",
            "#4 transferring context: 29B done\n",
            "#4 DONE 0.0s\n",
            "\n",
            "#5 [1/3] FROM docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim@sha256:cce370ade672f3bfcac80d0c80314fc6b6530d3c623dab384af12da76cd2db6b\n",
            "#5 resolve docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim@sha256:cce370ade672f3bfcac80d0c80314fc6b6530d3c623dab384af12da76cd2db6b 0.0s done\n",
            "#5 sha256:bb068c84195eeb57b82fe8388fea53eb6fa847d8ba240f27c3a3866b34f9669c 0B / 11.67MB 0.1s\n",
            "#5 sha256:a2318d6c47ec9cac5acc500c47c79602bcf953cec711a18bc898911a0984365b 0B / 29.13MB 0.1s\n",
            "#5 sha256:fdb547ee6440a47755ef9d71a7c5d0f0686966305c2d17667ea5658657a7ef6b 3.51kB / 3.51kB done\n",
            "#5 sha256:9fa212a4bc4d1281e681d4e150160929f2a2b860628b2c3e738a0d851d78ab79 10.63kB / 10.63kB done\n",
            "#5 sha256:dfc24c282fc2066991671359355a31d97a99135b0fb0277cfe82c1d310f51f20 0B / 3.51MB 0.1s\n",
            "#5 sha256:cce370ade672f3bfcac80d0c80314fc6b6530d3c623dab384af12da76cd2db6b 1.61kB / 1.61kB done\n",
            "#5 sha256:bb068c84195eeb57b82fe8388fea53eb6fa847d8ba240f27c3a3866b34f9669c 1.05MB / 11.67MB 0.5s\n",
            "#5 sha256:bb068c84195eeb57b82fe8388fea53eb6fa847d8ba240f27c3a3866b34f9669c 8.39MB / 11.67MB 0.7s\n",
            "#5 sha256:bb068c84195eeb57b82fe8388fea53eb6fa847d8ba240f27c3a3866b34f9669c 11.67MB / 11.67MB 0.8s done\n",
            "#5 sha256:0c5f125bc464dcadb5d5dd9f6360332c3455cec2dc8c20790b3c7e84854f383f 0B / 232B 0.9s\n",
            "#5 sha256:a2318d6c47ec9cac5acc500c47c79602bcf953cec711a18bc898911a0984365b 2.10MB / 29.13MB 1.0s\n",
            "#5 sha256:dfc24c282fc2066991671359355a31d97a99135b0fb0277cfe82c1d310f51f20 3.15MB / 3.51MB 1.1s\n",
            "#5 sha256:a2318d6c47ec9cac5acc500c47c79602bcf953cec711a18bc898911a0984365b 7.34MB / 29.13MB 1.2s\n",
            "#5 sha256:dfc24c282fc2066991671359355a31d97a99135b0fb0277cfe82c1d310f51f20 3.51MB / 3.51MB 1.1s done\n",
            "#5 sha256:0c5f125bc464dcadb5d5dd9f6360332c3455cec2dc8c20790b3c7e84854f383f 232B / 232B 1.2s done\n",
            "#5 sha256:cecfeb14b6d5f3b4a72b0138527b584ea1807591ce200f6f6a5a817f28c2f1b4 0B / 2.78MB 1.2s\n",
            "#5 sha256:ceaf8b78a51268f0b4077a092b12582be31c9799a74e14bbeedefaa0c29f273d 0B / 180B 1.2s\n",
            "#5 sha256:a2318d6c47ec9cac5acc500c47c79602bcf953cec711a18bc898911a0984365b 9.44MB / 29.13MB 1.3s\n",
            "#5 sha256:a2318d6c47ec9cac5acc500c47c79602bcf953cec711a18bc898911a0984365b 13.63MB / 29.13MB 1.5s\n",
            "#5 sha256:a2318d6c47ec9cac5acc500c47c79602bcf953cec711a18bc898911a0984365b 20.97MB / 29.13MB 1.7s\n",
            "#5 sha256:cecfeb14b6d5f3b4a72b0138527b584ea1807591ce200f6f6a5a817f28c2f1b4 2.10MB / 2.78MB 1.7s\n",
            "#5 sha256:a2318d6c47ec9cac5acc500c47c79602bcf953cec711a18bc898911a0984365b 29.13MB / 29.13MB 1.9s done\n",
            "#5 sha256:cecfeb14b6d5f3b4a72b0138527b584ea1807591ce200f6f6a5a817f28c2f1b4 2.78MB / 2.78MB 1.7s done\n",
            "#5 sha256:ceaf8b78a51268f0b4077a092b12582be31c9799a74e14bbeedefaa0c29f273d 180B / 180B 1.8s done\n",
            "#5 sha256:daa1471c8892485d2ab1da91095f9cd19a41c139449b799ededdadc7dd362801 0B / 7.06MB 1.9s\n",
            "#5 sha256:545d0994c72389fa96aad5d4755301d1543a3700909464c96cdb729b9ccb11d5 0B / 542B 1.9s\n",
            "#5 sha256:128b6b92c07b119f952b99f75993d2cc0df5492d9031f1dc7b1c47d9d5bd0037 0B / 541B 1.9s\n",
            "#5 extracting sha256:a2318d6c47ec9cac5acc500c47c79602bcf953cec711a18bc898911a0984365b 0.1s\n",
            "#5 sha256:545d0994c72389fa96aad5d4755301d1543a3700909464c96cdb729b9ccb11d5 542B / 542B 2.1s done\n",
            "#5 sha256:eee2e1fd69e6b598169cafd1bb7f886460427ee9e6ee9b182ed3badc9ee072e8 0B / 833B 2.1s\n",
            "#5 sha256:128b6b92c07b119f952b99f75993d2cc0df5492d9031f1dc7b1c47d9d5bd0037 541B / 541B 2.2s\n",
            "#5 sha256:128b6b92c07b119f952b99f75993d2cc0df5492d9031f1dc7b1c47d9d5bd0037 541B / 541B 2.2s done\n",
            "#5 sha256:1fc5023671bfae72a62bdf5a7dea209ecb9715bc74e4f959edfaa78ef0807298 0B / 527B 2.3s\n",
            "#5 sha256:daa1471c8892485d2ab1da91095f9cd19a41c139449b799ededdadc7dd362801 2.10MB / 7.06MB 2.5s\n",
            "#5 sha256:eee2e1fd69e6b598169cafd1bb7f886460427ee9e6ee9b182ed3badc9ee072e8 833B / 833B 2.4s done\n",
            "#5 sha256:b2e6d11788a42cc9c50fec6459055485bf62a18658b9618f5aa1558692d85049 0B / 527B 2.5s\n",
            "#5 sha256:daa1471c8892485d2ab1da91095f9cd19a41c139449b799ededdadc7dd362801 3.15MB / 7.06MB 2.6s\n",
            "#5 sha256:1fc5023671bfae72a62bdf5a7dea209ecb9715bc74e4f959edfaa78ef0807298 527B / 527B 2.5s done\n",
            "#5 sha256:ef368acf80b4a600c1cd9dfce6160b2f140c584f8316453f09f6cc487bb360e9 0B / 600B 2.6s\n",
            "#5 sha256:daa1471c8892485d2ab1da91095f9cd19a41c139449b799ededdadc7dd362801 7.06MB / 7.06MB 2.8s done\n",
            "#5 sha256:b2e6d11788a42cc9c50fec6459055485bf62a18658b9618f5aa1558692d85049 527B / 527B 2.7s done\n",
            "#5 sha256:24511625530368a5027532e4ba6160e1a165722ae77ca67e9ebc177820cd55dd 0B / 193B 2.8s\n",
            "#5 sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 0B / 32B 2.8s\n",
            "#5 sha256:ef368acf80b4a600c1cd9dfce6160b2f140c584f8316453f09f6cc487bb360e9 600B / 600B 2.8s done\n",
            "#5 sha256:795cd148a1e77329b3bf7af6296c4743ea2d6f0025974374a2b84f3cd1f9f1e3 0B / 5.32MB 2.9s\n",
            "#5 sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 32B / 32B 3.0s done\n",
            "#5 sha256:e469111d2a7657ee51522dfb8f3bdadf839bc0a019052eecd269ae806ee9f648 0B / 318B 3.1s\n",
            "#5 extracting sha256:a2318d6c47ec9cac5acc500c47c79602bcf953cec711a18bc898911a0984365b 1.3s done\n",
            "#5 extracting sha256:dfc24c282fc2066991671359355a31d97a99135b0fb0277cfe82c1d310f51f20 0.1s\n",
            "#5 sha256:24511625530368a5027532e4ba6160e1a165722ae77ca67e9ebc177820cd55dd 193B / 193B 3.3s done\n",
            "#5 extracting sha256:dfc24c282fc2066991671359355a31d97a99135b0fb0277cfe82c1d310f51f20 0.1s done\n",
            "#5 extracting sha256:bb068c84195eeb57b82fe8388fea53eb6fa847d8ba240f27c3a3866b34f9669c\n",
            "#5 sha256:795cd148a1e77329b3bf7af6296c4743ea2d6f0025974374a2b84f3cd1f9f1e3 1.05MB / 5.32MB 3.6s\n",
            "#5 sha256:e469111d2a7657ee51522dfb8f3bdadf839bc0a019052eecd269ae806ee9f648 318B / 318B 3.5s done\n",
            "#5 sha256:795cd148a1e77329b3bf7af6296c4743ea2d6f0025974374a2b84f3cd1f9f1e3 4.19MB / 5.32MB 3.9s\n",
            "#5 sha256:795cd148a1e77329b3bf7af6296c4743ea2d6f0025974374a2b84f3cd1f9f1e3 5.32MB / 5.32MB 4.0s done\n",
            "#5 extracting sha256:bb068c84195eeb57b82fe8388fea53eb6fa847d8ba240f27c3a3866b34f9669c 0.5s done\n",
            "#5 extracting sha256:0c5f125bc464dcadb5d5dd9f6360332c3455cec2dc8c20790b3c7e84854f383f done\n",
            "#5 extracting sha256:cecfeb14b6d5f3b4a72b0138527b584ea1807591ce200f6f6a5a817f28c2f1b4 0.1s\n",
            "#5 extracting sha256:cecfeb14b6d5f3b4a72b0138527b584ea1807591ce200f6f6a5a817f28c2f1b4 0.2s done\n",
            "#5 extracting sha256:ceaf8b78a51268f0b4077a092b12582be31c9799a74e14bbeedefaa0c29f273d done\n",
            "#5 extracting sha256:daa1471c8892485d2ab1da91095f9cd19a41c139449b799ededdadc7dd362801 0.1s\n",
            "#5 extracting sha256:daa1471c8892485d2ab1da91095f9cd19a41c139449b799ededdadc7dd362801 0.3s done\n",
            "#5 extracting sha256:545d0994c72389fa96aad5d4755301d1543a3700909464c96cdb729b9ccb11d5 done\n",
            "#5 extracting sha256:128b6b92c07b119f952b99f75993d2cc0df5492d9031f1dc7b1c47d9d5bd0037 done\n",
            "#5 extracting sha256:eee2e1fd69e6b598169cafd1bb7f886460427ee9e6ee9b182ed3badc9ee072e8\n",
            "#5 extracting sha256:eee2e1fd69e6b598169cafd1bb7f886460427ee9e6ee9b182ed3badc9ee072e8 done\n",
            "#5 extracting sha256:1fc5023671bfae72a62bdf5a7dea209ecb9715bc74e4f959edfaa78ef0807298 done\n",
            "#5 extracting sha256:b2e6d11788a42cc9c50fec6459055485bf62a18658b9618f5aa1558692d85049 done\n",
            "#5 extracting sha256:ef368acf80b4a600c1cd9dfce6160b2f140c584f8316453f09f6cc487bb360e9\n",
            "#5 extracting sha256:ef368acf80b4a600c1cd9dfce6160b2f140c584f8316453f09f6cc487bb360e9 done\n",
            "#5 extracting sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 done\n",
            "#5 extracting sha256:24511625530368a5027532e4ba6160e1a165722ae77ca67e9ebc177820cd55dd done\n",
            "#5 extracting sha256:795cd148a1e77329b3bf7af6296c4743ea2d6f0025974374a2b84f3cd1f9f1e3\n",
            "#5 extracting sha256:795cd148a1e77329b3bf7af6296c4743ea2d6f0025974374a2b84f3cd1f9f1e3 0.3s done\n",
            "#5 extracting sha256:e469111d2a7657ee51522dfb8f3bdadf839bc0a019052eecd269ae806ee9f648\n",
            "#5 extracting sha256:e469111d2a7657ee51522dfb8f3bdadf839bc0a019052eecd269ae806ee9f648 done\n",
            "#5 DONE 5.8s\n",
            "\n",
            "#6 [2/3] RUN pip install --no-cache-dir transformers==4.1.1 tensorflow==2.9.1 numpy==1.23.1 pydantic==1.9.1\n",
            "#6 1.568 Collecting transformers==4.1.1\n",
            "#6 1.709   Downloading transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
            "#6 1.871      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 9.6 MB/s eta 0:00:00\n",
            "#6 2.216 Collecting tensorflow==2.9.1\n",
            "#6 2.248   Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "#6 44.58      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 511.7/511.7 MB 16.5 MB/s eta 0:00:00\n",
            "#6 45.47 Collecting numpy==1.23.1\n",
            "#6 45.50   Downloading numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "#6 46.65      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 15.3 MB/s eta 0:00:00\n",
            "#6 46.99 Collecting pydantic==1.9.1\n",
            "#6 47.02   Downloading pydantic-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "#6 47.95      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.7/12.7 MB 13.4 MB/s eta 0:00:00\n",
            "#6 48.86 Collecting regex!=2019.12.17\n",
            "#6 48.89   Downloading regex-2024.9.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
            "#6 48.96      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 785.0/785.0 kB 11.8 MB/s eta 0:00:00\n",
            "#6 48.96 Requirement already satisfied: packaging in /usr/local/lib/python3.8/site-packages (from transformers==4.1.1) (24.1)\n",
            "#6 49.04 Collecting sacremoses\n",
            "#6 49.07   Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "#6 49.14      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 897.5/897.5 kB 13.3 MB/s eta 0:00:00\n",
            "#6 49.28 Collecting requests\n",
            "#6 49.31   Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "#6 49.32      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 21.9 MB/s eta 0:00:00\n",
            "#6 49.74 Collecting tokenizers==0.9.4\n",
            "#6 49.85   Downloading tokenizers-0.9.4-cp38-cp38-manylinux2010_x86_64.whl (2.9 MB)\n",
            "#6 50.09      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 12.0 MB/s eta 0:00:00\n",
            "#6 50.21 Collecting filelock\n",
            "#6 50.24   Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "#6 50.37 Collecting tqdm>=4.27\n",
            "#6 50.40   Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "#6 50.41      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 103.9 MB/s eta 0:00:00\n",
            "#6 50.56 Collecting h5py>=2.9.0\n",
            "#6 50.59   Downloading h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "#6 51.01      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 12.6 MB/s eta 0:00:00\n",
            "#6 51.10 Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "#6 51.13   Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "#6 51.16      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 438.7/438.7 kB 14.0 MB/s eta 0:00:00\n",
            "#6 51.24 Collecting termcolor>=1.1.0\n",
            "#6 51.27   Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "#6 51.38 Collecting libclang>=13.0.0\n",
            "#6 51.41   Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "#6 53.54      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/24.5 MB 10.8 MB/s eta 0:00:00\n",
            "#6 53.66 Collecting keras<2.10.0,>=2.9.0rc0\n",
            "#6 53.69   Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "#6 53.82      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 11.9 MB/s eta 0:00:00\n",
            "#6 53.83 Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from tensorflow==2.9.1) (57.5.0)\n",
            "#6 55.02 Collecting grpcio<2.0,>=1.24.3\n",
            "#6 55.05   Downloading grpcio-1.66.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "#6 55.54      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 11.8 MB/s eta 0:00:00\n",
            "#6 55.63 Collecting google-pasta>=0.1.1\n",
            "#6 55.66   Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "#6 55.66      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 167.2 MB/s eta 0:00:00\n",
            "#6 55.74 Collecting keras-preprocessing>=1.1.1\n",
            "#6 55.77   Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "#6 55.78      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 kB 44.9 MB/s eta 0:00:00\n",
            "#6 55.86 Collecting absl-py>=1.0.0\n",
            "#6 55.89   Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "#6 55.90      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 32.8 MB/s eta 0:00:00\n",
            "#6 55.97 Collecting opt-einsum>=2.3.2\n",
            "#6 56.00   Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "#6 56.01      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 34.7 MB/s eta 0:00:00\n",
            "#6 56.09 Collecting six>=1.12.0\n",
            "#6 56.12   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "#6 56.12 Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.9.1) (4.12.2)\n",
            "#6 56.53 Collecting protobuf<3.20,>=3.9.2\n",
            "#6 56.57   Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "#6 56.65      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 13.8 MB/s eta 0:00:00\n",
            "#6 56.73 Collecting astunparse>=1.6.0\n",
            "#6 56.75   Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "#6 56.83 Collecting flatbuffers<2,>=1.12\n",
            "#6 56.86   Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "#6 56.99 Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "#6 57.02   Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "#6 57.22      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 12.4 MB/s eta 0:00:00\n",
            "#6 57.46 Collecting wrapt>=1.11.0\n",
            "#6 57.49   Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "#6 57.50      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.4/83.4 kB 53.5 MB/s eta 0:00:00\n",
            "#6 57.58 Collecting gast<=0.4.0,>=0.2.1\n",
            "#6 57.61   Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "#6 57.72 Collecting tensorboard<2.10,>=2.9\n",
            "#6 57.75   Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "#6 58.20      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 12.8 MB/s eta 0:00:00\n",
            "#6 58.23 Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.44.0)\n",
            "#6 58.62 Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "#6 58.65   Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "#6 59.03      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 13.1 MB/s eta 0:00:00\n",
            "#6 59.13 Collecting markdown>=2.6.8\n",
            "#6 59.16   Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "#6 59.16      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.3/106.3 kB 21.4 MB/s eta 0:00:00\n",
            "#6 59.25 Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "#6 59.28   Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "#6 59.40 Collecting werkzeug>=1.0.1\n",
            "#6 59.43   Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
            "#6 59.44      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 24.9 MB/s eta 0:00:00\n",
            "#6 59.61 Collecting google-auth<3,>=1.6.3\n",
            "#6 59.64   Downloading google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\n",
            "#6 59.65      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.9/200.9 kB 24.6 MB/s eta 0:00:00\n",
            "#6 59.91 Collecting tensorboard-plugin-wit>=1.6.0\n",
            "#6 59.94   Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "#6 59.99      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 14.7 MB/s eta 0:00:00\n",
            "#6 60.01 Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.1.1) (2024.8.30)\n",
            "#6 60.23 Collecting charset-normalizer<4,>=2\n",
            "#6 60.26   Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "#6 60.27      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.1/141.1 kB 13.2 MB/s eta 0:00:00\n",
            "#6 60.39 Collecting urllib3<3,>=1.21.1\n",
            "#6 60.42   Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "#6 60.43      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.3/126.3 kB 16.0 MB/s eta 0:00:00\n",
            "#6 60.44 Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.1.1) (3.8)\n",
            "#6 60.59 Collecting joblib\n",
            "#6 60.62   Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "#6 60.64      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.8/301.8 kB 20.2 MB/s eta 0:00:00\n",
            "#6 60.64 Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from sacremoses->transformers==4.1.1) (8.1.7)\n",
            "#6 60.75 Collecting cachetools<6.0,>=2.0.0\n",
            "#6 60.78   Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
            "#6 60.86 Collecting rsa<5,>=3.1.4\n",
            "#6 60.89   Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "#6 60.98 Collecting pyasn1-modules>=0.2.1\n",
            "#6 61.02   Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
            "#6 61.03      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.5/181.5 kB 25.3 MB/s eta 0:00:00\n",
            "#6 61.14 Collecting requests-oauthlib>=0.7.0\n",
            "#6 61.17   Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "#6 61.33 Collecting importlib-metadata>=4.4\n",
            "#6 61.35   Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "#6 61.38 Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.1.5)\n",
            "#6 61.52 Collecting zipp>=3.20\n",
            "#6 61.55   Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "#6 61.66 Collecting pyasn1<0.7.0,>=0.4.6\n",
            "#6 61.69   Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "#6 61.70      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.1/83.1 kB 49.4 MB/s eta 0:00:00\n",
            "#6 61.80 Collecting oauthlib>=3.0.0\n",
            "#6 61.83   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "#6 61.84      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 16.3 MB/s eta 0:00:00\n",
            "#6 62.40 Installing collected packages: tokenizers, tensorboard-plugin-wit, libclang, keras, flatbuffers, zipp, wrapt, werkzeug, urllib3, tqdm, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, regex, pydantic, pyasn1, protobuf, oauthlib, numpy, joblib, grpcio, gast, filelock, charset-normalizer, cachetools, absl-py, sacremoses, rsa, requests, pyasn1-modules, opt-einsum, keras-preprocessing, importlib-metadata, h5py, google-pasta, astunparse, transformers, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
            "#6 64.17   Attempting uninstall: pydantic\n",
            "#6 64.17     Found existing installation: pydantic 1.10.18\n",
            "#6 64.18     Uninstalling pydantic-1.10.18:\n",
            "#6 64.20       Successfully uninstalled pydantic-1.10.18\n",
            "#6 78.67 Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.5.0 charset-normalizer-3.3.2 filelock-3.16.1 flatbuffers-1.12 gast-0.4.0 google-auth-2.34.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.66.1 h5py-3.11.0 importlib-metadata-8.5.0 joblib-1.4.2 keras-2.9.0 keras-preprocessing-1.1.2 libclang-18.1.1 markdown-3.7 numpy-1.23.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-1.9.1 regex-2024.9.11 requests-2.32.3 requests-oauthlib-2.0.0 rsa-4.9 sacremoses-0.1.1 six-1.16.0 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 tokenizers-0.9.4 tqdm-4.66.5 transformers-4.1.1 urllib3-2.2.3 werkzeug-3.0.4 wrapt-1.16.0 zipp-3.20.2\n",
            "#6 78.67 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "#6 78.98 \n",
            "#6 78.98 [notice] A new release of pip is available: 23.0.1 -> 24.2\n",
            "#6 78.98 [notice] To update, run: pip install --upgrade pip\n",
            "#6 DONE 79.9s\n",
            "\n",
            "#7 [3/3] COPY main.py ./main.py\n",
            "#7 DONE 0.1s\n",
            "\n",
            "#8 exporting to image\n",
            "#8 exporting layers\n",
            "#8 exporting layers 3.6s done\n",
            "#8 writing image sha256:7602b56bfd271611888243d1457ab6e5c6a79edd11b4e5cbdbba066de80ae3c9 done\n",
            "#8 naming to docker.io/library/sentiment-fast-api done\n",
            "#8 DONE 3.6s\n",
            "\n",
            "View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/ms0pf5dzuh6g0e1clprymxxxg\n"
          ]
        }
      ],
      "source": [
        "!docker build -t sentiment-fast-api ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kIFMOMaF4aA",
        "outputId": "b09169ae-403a-451e-a750-d19681b25b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cloudbuild.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile cloudbuild.yaml\n",
        "steps:\n",
        "# Download the model to embed it into the image\n",
        "- name: 'gcr.io/cloud-builders/gsutil'\n",
        "  args: ['cp','-r', 'gs://dev-joyas-recommendation/models/sentiment', '.']\n",
        "  id: 'download-model'\n",
        "# Build the container image\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['build', '-t', 'gcr.io/joyas-vietnam/sentiment-fast-api', '.']\n",
        "  waitFor: ['download-model']\n",
        "# Push the container image to Container Registry\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['push', 'gcr.io/joyas-vietnam/sentiment-fast-api']\n",
        "\n",
        "images:\n",
        "- gcr.io/joyas-vietnam/sentiment-fast-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ivofO1oGAXk",
        "outputId": "98fabdc0-bcce-480b-e0db-70bae28e721e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!gcloud builds submit --config cloudbuild.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKHyt8o7xDml"
      },
      "source": [
        "## Upload and deploy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVOxBxd7nU-w",
        "outputId": "34293ee0-6360-42d7-cca3-2c3e85cb839f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n"
          ]
        }
      ],
      "source": [
        "!gcloud ai models upload \\\n",
        "  --container-ports=80 \\\n",
        "  --container-predict-route=\"/predict\" \\\n",
        "  --container-health-route=\"/health\" \\\n",
        "  --region=us-central1 \\\n",
        "  --display-name=sentiment-fast-api \\\n",
        "  --container-image-uri=gcr.io/sascha-playground-doit/sentiment-fast-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG4u0OsVnhyy",
        "outputId": "b76c2d73-d45b-49aa-c961-1be853a99085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
            "Created Vertex AI endpoint: projects/234439745674/locations/us-central1/endpoints/7608484124768075776.\n"
          ]
        }
      ],
      "source": [
        "!gcloud ai endpoints create \\\n",
        "  --project=sascha-playground-doit \\\n",
        "  --region=us-central1 \\\n",
        "  --display-name=sentiment-fast-api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk_qyK39NJRq"
      },
      "source": [
        "get model and endpoint IDs from previous steps\n",
        "deployment takes 15 min aprox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_pme6FpoUWQ",
        "outputId": "326ff26a-358f-4d10-8f3f-a2240f62a008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
            "Deployed a model to the endpoint 7608484124768075776. Id of the deployed model: 1352896281420234752.\n"
          ]
        }
      ],
      "source": [
        "!gcloud ai endpoints deploy-model 7608484124768075776 \\\n",
        "  --project=sascha-playground-doit \\\n",
        "  --region=us-central1 \\\n",
        "  --model=8709323962590429184 \\\n",
        "  --traffic-split=0=100 \\\n",
        "  --machine-type=\"n1-standard-2\" \\\n",
        "  --display-name=sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnF8bW8QXT42"
      },
      "source": [
        "## Predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxHBVFaBnOP8"
      },
      "source": [
        "### gcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqQRMSltXWz9",
        "outputId": "cc542d66-4350-4e61-b337-e0a14e3277bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting request.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile request.json\n",
        "{\n",
        "    \"instances\": [\n",
        "        {\"text\": \"DoiT is a great company.\"},\n",
        "        {\"text\": \"The beach was nice but overall the hotel was very bad.\"},\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ygJcK3HXUw_",
        "outputId": "0f14d656-e83d-4927-a246-f415130c824b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n",
            "[{'confidence': 0.9409326314926147, 'sentiment': 'POSITIVE'}, {'confidence': 0.9964427351951599, 'sentiment': 'NEGATIVE'}]\n"
          ]
        }
      ],
      "source": [
        "!gcloud ai endpoints predict 4078442670165327872 \\\n",
        "  --region=us-central1 \\\n",
        "  --json-request=request.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeDjc4SPnL1c"
      },
      "source": [
        "### Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "84yulqfiSoju",
        "outputId": "717ba75b-13f2-4e40-aa90-346a9bfd25ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-cloud-aiplatform==1.14.0\n",
            "  Downloading google_cloud_aiplatform-1.14.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting protobuf<4.0.0dev,>=3.19.0\n",
            "  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform==1.14.0) (1.21.0)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform==1.14.0) (1.31.6)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 62.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform==1.14.0) (21.3)\n",
            "Collecting proto-plus<2.0.0dev,>=1.15.0\n",
            "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.35.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.56.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2022.5)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.50.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (4.2.4)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (1.0.3)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.6.2-py2.py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 41.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.6.1-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 45.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.6.0-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 47.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.5.1-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 46.2 MB/s \n",
            "\u001b[?25hCollecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
            "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 46.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.3.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 45.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 45.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.2.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 45.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.1.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 48.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.0.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 59.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 50.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.43.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 55.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 56.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.2-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 47.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 37.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.0-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 45.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-1.41.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 46.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 66.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.39.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 67.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.38.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 57.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.1-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 46.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 40.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.1-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.34.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.33.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 10.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.32.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 10.3 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.5.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 65.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.4.1-py2.py3-none-any.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 47.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.4.0-py2.py3-none-any.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 61.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.3.3-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[K     |████████████████████████████████| 286 kB 55.1 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.4-py2.py3-none-any.whl (27 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_cloud_core-1.4.3-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.2-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.4.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading google_cloud_core-1.2.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.1.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.0.3-py2.py3-none-any.whl (26 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
            "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.8.2)\n",
            "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
            "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
            "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
            "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform==1.14.0) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2.10)\n",
            "Installing collected packages: protobuf, google-crc32c, proto-plus, grpc-google-iam-v1, google-resumable-media, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "  Attempting uninstall: google-cloud-bigquery\n",
            "    Found existing installation: google-cloud-bigquery 1.21.0\n",
            "    Uninstalling google-cloud-bigquery-1.21.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.34.4 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\u001b[0m\n",
            "Successfully installed google-cloud-aiplatform-1.14.0 google-cloud-bigquery-2.34.4 google-cloud-core-2.3.2 google-cloud-resource-manager-1.5.1 google-cloud-storage-2.5.0 google-crc32c-1.5.0 google-resumable-media-2.4.0 grpc-google-iam-v1-0.12.4 proto-plus-1.22.1 protobuf-3.20.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install google-cloud-aiplatform==1.14.0 --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75x3lL2dSsqk"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "project = \"sascha-playground-doit\"\n",
        "location = \"us-central1\"\n",
        "\n",
        "aiplatform.init(project=project, location=location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krH5rUMznRQx",
        "outputId": "1da883e1-1600-47b1-bffa-d8dbb41a7fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction(predictions=[{'sentiment': 'POSITIVE', 'confidence': 0.9409326314926147}, {'confidence': 0.9964427351951599, 'sentiment': 'NEGATIVE'}], deployed_model_id='1352896281420234752', explanations=None)\n"
          ]
        }
      ],
      "source": [
        "instances = [\n",
        "    {\"text\": \"DoiT is a great company.\"},\n",
        "    {\"text\": \"The beach was nice but overall the hotel was very bad.\"},\n",
        "]\n",
        "\n",
        "\n",
        "endpoint = aiplatform.Endpoint(\n",
        "    \"projects/234439745674/locations/us-central1/endpoints/7608484124768075776\"\n",
        ")\n",
        "\n",
        "prediction = endpoint.predict(instances=instances)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tcaud2GWTX82"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
