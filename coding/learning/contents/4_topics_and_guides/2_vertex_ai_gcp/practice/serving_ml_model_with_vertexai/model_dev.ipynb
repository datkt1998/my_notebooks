{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xóa toàn bộ cache trong: C:\\Users\\datkt/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Lấy đường dẫn tới thư mục cache của Hugging Face\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "\n",
    "# Xóa toàn bộ cache mô hình đã tải\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(f\"Đã xóa toàn bộ cache trong: {cache_dir}\")\n",
    "else:\n",
    "    print(f\"Thư mục cache không tồn tại: {cache_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\datkt\\AppData\\Local\\miniconda3\\envs\\textsumma\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 308/308 [00:00<00:00, 308kB/s]\n",
      "Downloading: 100%|██████████| 874k/874k [00:00<00:00, 3.57MB/s]\n",
      "Downloading: 100%|██████████| 1.08M/1.08M [00:00<00:00, 2.65MB/s]\n",
      "Downloading: 100%|██████████| 22.0/22.0 [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 167/167 [00:00<00:00, 292kB/s]\n",
      "Downloading: 100%|██████████| 993/993 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 515M/515M [01:36<00:00, 5.60MB/s]    \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "checkpoint = \"mr4/phobert-base-vi-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "storage_path = \"models/\"\n",
    "tokenizer.save_pretrained(storage_path + \"tokenizer/\")\n",
    "model.save_pretrained(storage_path + \"model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictions(predictions=[Prediction(sentiment='Tích cực', confidence=0.9900698065757751), Prediction(sentiment='Tiêu cực', confidence=0.9821959733963013)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test prediction\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Pydantic models for prediction results\n",
    "class Prediction(BaseModel):\n",
    "    sentiment: str\n",
    "    confidence: Optional[float]\n",
    "\n",
    "\n",
    "class Predictions(BaseModel):\n",
    "    predictions: List[Prediction]\n",
    "    \n",
    "# # Load tokenizer và model\n",
    "storage_path = \"models/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(storage_path + \"tokenizer/\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    storage_path + \"model/\"\n",
    ")\n",
    "    \n",
    "def predict(body):\n",
    "    # Extract the instances (texts) from the request\n",
    "    instances = [x[\"text\"] for x in body[\"instances\"]]\n",
    "\n",
    "    # Tokenize văn bản cho mô hình\n",
    "    tf_batch = tokenizer(\n",
    "        instances,\n",
    "        # max_length=128,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",  # Chuyển thành tensor Pytorch\n",
    "    )\n",
    "\n",
    "    # Lấy kết quả dự đoán từ mô hình\n",
    "    with torch.no_grad():\n",
    "        tf_outputs = model(**tf_batch)\n",
    "\n",
    "    # Áp dụng hàm softmax để lấy xác suất (điểm tự tin)\n",
    "    softmax = torch.nn.functional.softmax(tf_outputs.logits, dim=-1).numpy()\n",
    "\n",
    "    # Tìm chỉ số của xác suất cao nhất (dự đoán cảm xúc)\n",
    "    indices = np.argmax(softmax, axis=-1)\n",
    "\n",
    "    # Lấy giá trị confidence cao nhất cho mỗi dự đoán\n",
    "    confidences = np.max(softmax, axis=-1)\n",
    "\n",
    "    # Prepare the output\n",
    "    outputs = []\n",
    "    for index, confidence in zip(indices, confidences):\n",
    "        sentiment = model.config.id2label[index]\n",
    "        outputs.append(\n",
    "            Prediction(sentiment=sentiment, confidence=float(confidence))\n",
    "        )\n",
    "\n",
    "    # Return the predictions\n",
    "    return Predictions(predictions=outputs)\n",
    "\n",
    "body = {\n",
    "    \"instances\":[\n",
    "        {\"text\": \"Trời hôm nay đẹp quá!\"},\n",
    "        {\"text\": \"món ăn này không ngon\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "predict(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% writefile main.py\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import uvicorn  # noqa: F401\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# # Load tokenizer và model\n",
    "storage_path = \"models/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(storage_path + \"tokenizer/\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    storage_path + \"model/\"\n",
    ")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Sentiment Analysis API\")\n",
    "\n",
    "# Define health and prediction routes based on environment variables or defaults\n",
    "AIP_HEALTH_ROUTE = os.environ.get(\"AIP_HEALTH_ROUTE\", \"/health\")\n",
    "AIP_PREDICT_ROUTE = os.environ.get(\"AIP_PREDICT_ROUTE\", \"/predict\")\n",
    "\n",
    "\n",
    "# Pydantic models for prediction results\n",
    "class Prediction(BaseModel):\n",
    "    sentiment: str\n",
    "    confidence: Optional[float]\n",
    "\n",
    "\n",
    "class Predictions(BaseModel):\n",
    "    predictions: List[Prediction]\n",
    "\n",
    "\n",
    "# Health check route\n",
    "@app.get(AIP_HEALTH_ROUTE, status_code=200)\n",
    "async def health():\n",
    "    return {\"health\": \"ok\"}\n",
    "\n",
    "\n",
    "# Prediction route to handle batch requests\n",
    "@app.post(\n",
    "    AIP_PREDICT_ROUTE,\n",
    "    response_model=Predictions,\n",
    "    response_model_exclude_unset=True,\n",
    ")\n",
    "async def predict(request: Request):\n",
    "    # Extract the JSON body from the request\n",
    "    body = await request.json()\n",
    "\n",
    "    # Extract the instances (texts) from the request\n",
    "    instances = [x[\"text\"] for x in body[\"instances\"]]\n",
    "\n",
    "    # Tokenize văn bản cho mô hình\n",
    "    tf_batch = tokenizer(\n",
    "        instances,\n",
    "        # max_length=128,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",  # Chuyển thành tensor Pytorch\n",
    "    )\n",
    "\n",
    "    # Lấy kết quả dự đoán từ mô hình\n",
    "    with torch.no_grad():\n",
    "        tf_outputs = model(**tf_batch)\n",
    "\n",
    "    # Áp dụng hàm softmax để lấy xác suất (điểm tự tin)\n",
    "    softmax = torch.nn.functional.softmax(tf_outputs.logits, dim=-1).numpy()\n",
    "\n",
    "    # Tìm chỉ số của xác suất cao nhất (dự đoán cảm xúc)\n",
    "    indices = np.argmax(softmax, axis=-1)\n",
    "\n",
    "    # Lấy giá trị confidence cao nhất cho mỗi dự đoán\n",
    "    confidences = np.max(softmax, axis=-1)\n",
    "\n",
    "    # Prepare the output\n",
    "    outputs = []\n",
    "    for index, confidence in zip(indices, confidences):\n",
    "        sentiment = model.config.id2label[index]\n",
    "        outputs.append(\n",
    "            Prediction(sentiment=sentiment, confidence=float(confidence))\n",
    "        )\n",
    "\n",
    "    # Return the predictions\n",
    "    return Predictions(predictions=outputs)\n",
    "\n",
    "\n",
    "# Main function to run the FastAPI app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY main.py ./main.py\n",
    "COPY requirements.txt ./requirements.txt\n",
    "COPY models ./models\n",
    "\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r ./requirements.txt\n",
    "\n",
    "EXPOSE 8080\n",
    "CMD [\"python\", \"main.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = \"asia-southeast1\"\n",
    "PROJECT = \"joyas-vietnam\"\n",
    "REPOSITORY_NAME = \"dev-aiml-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "steps:\n",
    "# Download the model to embed it into the image\n",
    "# - name: 'gcr.io/cloud-builders/gsutil'\n",
    "#   args: ['cp', '-r', 'gs://dev-joyas-recommendation/models/sentiment', '.']\n",
    "#   id: 'download-model'\n",
    "\n",
    "# Build the container image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'asia-southeast1-docker.pkg.dev/joyas-vietnam/dev-aiml-model/sentiment-fast-api', '.']\n",
    "  # waitFor: ['download-model']\n",
    "\n",
    "# Push the container image to Artifact Registry\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['push', 'asia-southeast1-docker.pkg.dev/joyas-vietnam/dev-aiml-model/sentiment-fast-api']\n",
    "\n",
    "images:\n",
    "- asia-southeast1-docker.pkg.dev/joyas-vietnam/dev-aiml-model/sentiment-fast-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=1T3Qk9s5eHVpuz53ibnkXfQ9kFSCGP&access_type=offline&code_challenge=_KVBAaKJGkilFzXi78d8TNjFFEIBKf8G2EPS9xr6l3o&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [datkt.joyas@gmail.com].\n",
      "Your current project is [ext-pinetree-dw].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Your active project does not match the quota project in your local Application Default Credentials file. This might result in unexpected quota issues.\n",
      "\n",
      "To update your Application Default Credentials quota project, use the `gcloud auth application-default set-quota-project` command.\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project joyas-vietnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create request issued for: [dev-aiml-model]\n",
      "Waiting for operation [projects/joyas-vietnam/locations/asia-southeast1/operations/c10a00b3-2f8b-45bf-b923-45e42675f358] to complete...\n",
      ".................done.\n",
      "Created repository [dev-aiml-model].\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create dev-aiml-model \\\n",
    "  --repository-format=docker \\\n",
    "  --location=asia-southeast1 \\\n",
    "  --description=\"My Docker repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --config cloudbuild.yaml ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 278B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
      "#2 ...\n",
      "\n",
      "#3 [auth] tiangolo/uvicorn-gunicorn-fastapi:pull token for registry-1.docker.io\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
      "#2 DONE 2.2s\n",
      "\n",
      "#4 [internal] load .dockerignore\n",
      "#4 transferring context: 2B done\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [1/5] FROM docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim@sha256:cce370ade672f3bfcac80d0c80314fc6b6530d3c623dab384af12da76cd2db6b\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [internal] load build context\n",
      "#6 transferring context: 574B done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "#7 [2/5] COPY main.py ./main.py\n",
      "#7 CACHED\n",
      "\n",
      "#8 [3/5] COPY requirements.txt ./requirements.txt\n",
      "#8 CACHED\n",
      "\n",
      "#9 [4/5] COPY models ./models\n",
      "#9 DONE 5.7s\n",
      "\n",
      "#10 [5/5] RUN pip install --no-cache-dir -r ./requirements.txt\n",
      "#10 4.266 Collecting fastapi==0.115.0\n",
      "#10 4.405   Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "#10 4.444      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.6/94.6 kB 2.5 MB/s eta 0:00:00\n",
      "#10 5.004 ERROR: Ignored the following versions that require a different python version: 1.25.0 Requires-Python >=3.9; 1.25.1 Requires-Python >=3.9; 1.25.2 Requires-Python >=3.9; 1.26.0 Requires-Python <3.13,>=3.9; 1.26.1 Requires-Python <3.13,>=3.9; 1.26.2 Requires-Python >=3.9; 1.26.3 Requires-Python >=3.9; 1.26.4 Requires-Python >=3.9; 2.0.0 Requires-Python >=3.9; 2.0.1 Requires-Python >=3.9; 2.0.2 Requires-Python >=3.9; 2.1.0 Requires-Python >=3.10; 2.1.0rc1 Requires-Python >=3.10; 2.1.1 Requires-Python >=3.10\n",
      "#10 5.005 ERROR: Could not find a version that satisfies the requirement numpy==1.26.4 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "#10 5.006 ERROR: No matching distribution found for numpy==1.26.4\n",
      "#10 5.313 \n",
      "#10 5.313 [notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "#10 5.313 [notice] To update, run: pip install --upgrade pip\n",
      "#10 ERROR: process \"/bin/sh -c pip install --no-cache-dir -r ./requirements.txt\" did not complete successfully: exit code: 1\n",
      "------\n",
      " > [5/5] RUN pip install --no-cache-dir -r ./requirements.txt:\n",
      "4.266 Collecting fastapi==0.115.0\n",
      "4.405   Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "4.444      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.6/94.6 kB 2.5 MB/s eta 0:00:00\n",
      "5.004 ERROR: Ignored the following versions that require a different python version: 1.25.0 Requires-Python >=3.9; 1.25.1 Requires-Python >=3.9; 1.25.2 Requires-Python >=3.9; 1.26.0 Requires-Python <3.13,>=3.9; 1.26.1 Requires-Python <3.13,>=3.9; 1.26.2 Requires-Python >=3.9; 1.26.3 Requires-Python >=3.9; 1.26.4 Requires-Python >=3.9; 2.0.0 Requires-Python >=3.9; 2.0.1 Requires-Python >=3.9; 2.0.2 Requires-Python >=3.9; 2.1.0 Requires-Python >=3.10; 2.1.0rc1 Requires-Python >=3.10; 2.1.1 Requires-Python >=3.10\n",
      "5.005 ERROR: Could not find a version that satisfies the requirement numpy==1.26.4 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "5.006 ERROR: No matching distribution found for numpy==1.26.4\n",
      "5.313 \n",
      "5.313 [notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "5.313 [notice] To update, run: pip install --upgrade pip\n",
      "------\n",
      "Dockerfile:6\n",
      "--------------------\n",
      "   4 |     COPY requirements.txt ./requirements.txt\n",
      "   5 |     COPY models ./models\n",
      "   6 | >>> RUN pip install --no-cache-dir -r ./requirements.txt\n",
      "   7 |     \n",
      "   8 |     EXPOSE 8080\n",
      "--------------------\n",
      "ERROR: failed to solve: process \"/bin/sh -c pip install --no-cache-dir -r ./requirements.txt\" did not complete successfully: exit code: 1\n",
      "\n",
      "View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/wwaxbjtwht1okupgtdbzci9l3\n"
     ]
    }
   ],
   "source": [
    "!docker build -t asia-southeast1-docker.pkg.dev/joyas-vietnam/dev-aiml-model/sentiment-fast-api ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
