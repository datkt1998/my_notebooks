{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f287a848",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Operational level monitoring\n",
    "\n",
    "<img src=\"_images/03op_om.jpg\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb9b309",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## System performance and reliability\n",
    "\n",
    "The system/application performance metrics to monitor that will give you an idea of model performance include:\n",
    "\n",
    "- CPU/GPU utilization when the model is computing predictions on incoming data from each API call; tells you how much your model is consuming per request.\n",
    "- Memory utilization for when the model caches data or input data is cached in memory for faster I/O performance.\n",
    "- Number of failed requests by an event/operation.\n",
    "- Total number of API calls.\n",
    "- Response time of the model server or prediction service.\n",
    "- System reliability: infrastructure and network uptime,...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "646b967c-9c67-44e6-9192-ebd002605dc0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "## Pipelines\n",
    "\n",
    "Monitor the health of your data and model pipeline. Unhealthy data pipelines can affect data quality, and your model pipeline leakages or unexpected changes can easily generate negative value.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "210d1c1d-598e-462d-b616-2d669b895853",
   "metadata": {},
   "source": [
    "### Data pipelines\n",
    "\n",
    "Monitoring the health of data pipelines is extremely crucial because data quality issues can arise from bad or unhealthy data pipelines. This especially is extremely tricky to monitor for your IT Ops/DevOps team and may require empowering your data engineering/DataOps team to monitor and troubleshoot issues.\n",
    "\n",
    "It also has to be a shared responsibility. Work with your DataOps team, communicate what your model expects, and the team will tell you what the output of their data pipeline is—this can help you tighten up your system and drive positive results.\n",
    "\n",
    "If you’re charged with the responsibility of monitoring your data pipeline, here are some metrics and factors you may want to track:\n",
    "\n",
    "- __Input data__ – are the data and files in the pipeline with the appropriate structure, schema, and completeness? Are there data validation tests and checks in place so that the team can be alerted in case of an oddity in ingested data? Monitor what comes into the data pipeline to keep it healthy.\n",
    "- __Intermediate workflow steps__ – are the inputs and outputs of every task and flow in the DAG as expected, in terms of the number of files and file types? How long does a task take to run in the pipeline? This could be the data preprocessing task, or the validation task, or even the data distribution monitoring task.\n",
    "- __Output data__ – is the output data schema as expected by the machine learning model in terms of features and feature embeddings? What’s the typical file size expected from an output file?\n",
    "- __Data quality metrics__ – tracking the statistical metrics according to the data that flows in. This could be basic statistical properties of the data such as mean, standard deviation, correlation, and so on, or distance metrics (such as KL divergence, Kolmogorov-Smirnov statistic). The statistical metric used will be mostly dependent on the dimension of data expected; a couple of features or several features.\n",
    "- __Scheduled run time__ of a job, actual run time, how long it took to run, and the state of the job (successful, or failed job?).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10f66f6c-7f24-4fea-86c1-4a0d65669199",
   "metadata": {},
   "source": [
    "### Model pipeline\n",
    "\n",
    "You want to track crucial factors that can cause your model to break in production after retraining and being redeployed. This includes:\n",
    "\n",
    "- Dependencies – you don’t want a situation where your model was built with Tensorflow 2.0 and a recent dependency update by someone else on your team that’s bundled with Tensorflow 2.4 causes part of your retraining script to fail. Validate the versions of each dependency your model runs on and log that as your pipeline metadata, so dependency updates that cause failure can be easier to debug.\n",
    "- The actual time a retraining job was triggered, how long it took the retraining job to run, resources usage of the job, and the state of the job (successfully retrained and redeployed model, or failed?).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "261c6266-7973-4291-9285-7dfd76c4e92e",
   "metadata": {},
   "source": [
    "## Cost\n",
    "\n",
    "You need to keep an eye out for how much it’s costing you and your organization to host your entire machine learning application, including data storage and compute costs, retraining, or other types of orchestrated jobs. These costs can add up fast, especially if they’re not being tracked. Also, it takes computational power for your models to make predictions for every request, so you also need to track inference costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
