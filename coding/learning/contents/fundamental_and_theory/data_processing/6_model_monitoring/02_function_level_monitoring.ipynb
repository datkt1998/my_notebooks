{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a665342-89a3-4eb4-bf72-ae84e777f6a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Functional level monitoring\n",
    "\n",
    "<img src=\"_images/02f_fm.jpg\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9e0cbc0-15e4-4988-a0d7-a761a0e432ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6e435d9-abfd-4246-8a6a-0c2deaa10557",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data quality issues\n",
    "Tính toàn vẹn của Data input bị thay đổi. Để validate tính toàn vẹn của data trước khi đưa vào model, cần kiểm tra một số metrics liên quan đến data properties/ datatypes\n",
    "\n",
    "__Nguyên nhân:__\n",
    "- Break in data preprocesing pipelines\n",
    "- Change source of data\n",
    "- Data bị loss in source\n",
    "\n",
    "__Detection techniques:__\n",
    "- Testing input data for duplicates,\n",
    "- Testing input data for missing values,\n",
    "- Catching syntax errors,\n",
    "- Catching data type and format errors,\n",
    "- Kiểm tra source dữ liệu của feature bị detect có issue ,\n",
    "\n",
    "__Possible solutions after detecting data quality issues:__\n",
    "- Tạo alert khi data source thay đổi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca6f2083",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data/feature drift\n",
    "Sự thay đổi distribution/histogram của dữ liệu training và production sét trên level features/variables\n",
    "\n",
    "__Nguyên nhân:__\n",
    "- Data quality issue\n",
    "- Change in data properies in real world\n",
    "\n",
    "__Detection techniques:__\n",
    "- Testing __statistic estimator__ of input features: mean, STD, median, variance, range,...\n",
    "- For __continuous features__: use divergence and distance test the distribution: KL divergence, KS statistic, Population Stability Index (PSI), Hellinger distance,...\n",
    "- For __categorical features__: use chi-square test, entropy, number of distance, mode,...\n",
    "- Boxsplot\n",
    "\n",
    "_(if there are a lot of features, can be use dimmensionality reducetion techniques (such as PCA,...) before test)_\n",
    "\n",
    "__Possible solutions after detecting data drift:__\n",
    "- Tạo alert và gửi notif khi phát hiện data drift vượt threshold\n",
    "- Retrain set of new data collection in model periodically"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "969128bc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### PSI\n",
    "__Rules__:\n",
    "- `PSI` < 0.1 - No change. You can continue using existing model.\n",
    "- `PSI` >=0.1 but less than 0.2 - Slight change is required.\n",
    "- `PSI` >=0.2 - Significant change is required. Ideally, you should not use this model any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e70ceb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _psi(expected: np.ndarray, actual: np.ndarray, bucket_type: str = \"bins\", n_bins: int = 10) -> float:\n",
    "    \"\"\"Calculate PSI metric for two arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        expected : list-like\n",
    "            Array of expected values\n",
    "        actual : list-like\n",
    "            Array of actual values\n",
    "        bucket_type : str\n",
    "            Binning strategy. Accepts two options: 'bins' and 'quantiles'. Defaults to 'bins'.\n",
    "            'bins': input arrays are split into bins with equal\n",
    "                and fixed steps based on 'expected' array\n",
    "            'quantiles': input arrays are binned according to 'expected' array\n",
    "                with given number of n_bins\n",
    "        n_bins : int\n",
    "            Number of buckets for binning. Defaults to 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A single float number\n",
    "    \"\"\"\n",
    "    breakpoints = np.arange(0, n_bins + 1) / (n_bins) * 100\n",
    "    if bucket_type == \"bins\":\n",
    "        breakpoints = np.histogram(expected, n_bins)[1]\n",
    "    elif bucket_type == \"quantiles\":\n",
    "        breakpoints = np.percentile(expected, breakpoints)\n",
    "\n",
    "    # Calculate frequencies\n",
    "    expected_percents = np.histogram(expected, breakpoints)[0] / len(expected)\n",
    "    actual_percents = np.histogram(actual, breakpoints)[0] / len(actual)\n",
    "    # Clip frequencies to avoid zero division\n",
    "    expected_percents = np.clip(expected_percents, a_min=0.0001, a_max=None)\n",
    "    actual_percents = np.clip(actual_percents, a_min=0.0001, a_max=None)\n",
    "    # Calculate PSI\n",
    "    psi_value = (expected_percents - actual_percents) * np.log(expected_percents / actual_percents)\n",
    "    psi_value = sum(psi_value)\n",
    "\n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def calculate_psi(\n",
    "        expected: np.ndarray, actual: np.ndarray, bucket_type: str = \"bins\", n_bins: int = 10, axis: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Apply PSI calculation to 2 1-d or 2-d arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expected : list-like\n",
    "        Array of expected values\n",
    "    actual : list-like\n",
    "        Array of actual values\n",
    "    bucket_type : str\n",
    "        Binning strategy. Accepts two options: 'bins' and 'quantiles'. Defaults to 'bins'.\n",
    "            'bins' - input arrays are split into bins with equal\n",
    "                and fixed steps based on ’expected' array\n",
    "            'quantiles' - input arrays are binned according to ’expected’ array\n",
    "                with given number of n_bins\n",
    "    n_bins : int\n",
    "        Number of buckets for binning. Defaults to 10.\n",
    "    axis : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        np.ndarray\n",
    "\n",
    "    Args:\n",
    "        axis:\n",
    "        axis:\n",
    "    \"\"\"\n",
    "    if len(expected.shape) == 1:\n",
    "        psi_values = np.empty(len(expected.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(expected.shape[axis])\n",
    "\n",
    "    for i in range(0, len(psi_values)):\n",
    "        if len(psi_values) == 1:\n",
    "            psi_values = _psi(expected, actual, bucket_type, n_bins)\n",
    "        elif axis == 0:\n",
    "            psi_values[i] = _psi(expected[:, i], actual[:, i], bucket_type, n_bins)\n",
    "        elif axis == 1:\n",
    "            psi_values[i] = _psi(expected[i, :], actual[i, :], bucket_type, n_bins)\n",
    "        return np.array(psi_values)\n",
    "\n",
    "\n",
    "calculate_psi(feature_train_proba, feature_produ_proba, bucket_type=\"bins\", n_bins=10, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3f4b06f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Outliers\n",
    "Sự xuất hiện với tần xuất nhiều các outlies có thể ảnh hưởng tới hiệu suất của mô hình, hoặc dấu hiệu chỉ ra 1 pattern mới mà dữ liệu train trước đó chưa được học.\n",
    "\n",
    "__Detection techniques:__\n",
    "- Determine how far/how often from outlier to training dataset\n",
    "\n",
    "__Possible solutions after detecting outliers:__\n",
    "- Tạo subset mới chứa outlier và retrain new model, đánh giá sự khác biệt giữa new model và primary model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1378e5e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "730f92a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model drift\n",
    "Hiện tượng thay đổi relationship giữa biến Y và các biến X (supervised) hoặc giữa các biến X (unsupervised) với nhau, thậm trí không còn mối tương quan, dẫn tới kết quả model giảm tính chính xác overtime so với benchmark/KPIs\n",
    "\n",
    "__Cause:__\n",
    "- The real-world data changes naturally or sudden as stress events\n",
    "\n",
    "__Model drift detection__\n",
    "- Catching the change of correlation/auc/... between X and Y or between Xs\n",
    "- Detect by predictive performance is reduce overtime by setting a predictive metrics threshold\n",
    "- Detect by label drift (change the distribution)\n",
    "\n",
    "__Possible solutions after detecting model/concept drift__\n",
    "- If your business objectives and environment change frequently, you may want to consider automating your system to schedule and execute retraining at predefined intervals compared to more stable businesses\n",
    "- If retraining your models doesn’t improve performance, you may want to consider remodeling or redeveloping models from scratch.\n",
    "- If you’re working on larger scale projects with a good budget and little trade-off between cost and performance (in terms of how well your model catches up with a very dynamic business climate), you may want to consider __online learning algorithms__ for your project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9050a45",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model configuration and artifacts, version\n",
    "The model configuration file and artifacts contain all the components that were used to build that model, including:\n",
    "\n",
    "- Training dataset location and version,\n",
    "- Test dataset location and version,\n",
    "- Model version\n",
    "- Hyperparameters used,\n",
    "- Default feature values,\n",
    "- Dependencies and their versions; you want to monitor changes in dependency versions to easily find them for root cause analysis when model failure is caused by dependency changes,\n",
    "- Environment variables,\n",
    "- Model type (classification vs regression),\n",
    "- Model author,\n",
    "- Target variable name,\n",
    "- Features to select from the data,\n",
    "- Code and data for testing scenarios,\n",
    "- Code for the model and its preprocessing.\n",
    "\n",
    "Track the configurations for relevance—especially the hyperparameter values used by the model during retraining for any abnormality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f2a0bb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Protect model by attack\n",
    "\n",
    "Monitor your system for adversarial attacks by using the same steps you use to flag inputs with outlier events because adversarial threats don’t follow a pattern, they’re atypical events."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c31cccc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predictions (Output)\n",
    "\n",
    "Monitoring model output in production is not just the best indicator of model performance, but it also tells us if business KPIs are being met. In terms of model predictions, the most important thing to monitor is model performance in line with business metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46f4f238",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model evaluation metrics\n",
    "\n",
    "(Scoring models when ground truth is available)\n",
    "\n",
    "Using metrics to evaluate model performance is a big part of monitoring your model in production. Different metrics can be used here, such as classification, regression, clustering, reinforcement learning, and so on.\n",
    "\n",
    "We typically evaluate the model using predefined model scoring metrics (accuracy, AUC, precision, etc) when you have a ground truth/label to compare your model with.\n",
    "\n",
    "<img src=\"_images/02f_avai_y.png\">\n",
    "\n",
    "At `1`, a part of the production data (input data) is channeled to the ground truth service which typically involves real-time ground truth generated by your system (for example, logging if a user clicked on an ad when the model predicted they would), a human label annotator, or other data labeling vendors for more complicated tasks (such as confirming if a customer repaid a loan at the stipulated time, or confirming if a transaction was fraudulent or legitimate after contacting a customer).\n",
    "\n",
    "The event id that tracks prediction and model details is tagged with that ground truth event and logged to a data store. The data is then ingested into the monitoring platform, which computes the model performance metric given the model’s prediction and the actual label.\n",
    "\n",
    "- As you probably already know, metrics for a classification model include:\n",
    "    - Accuracy\n",
    "    - Confusion Matrix,\n",
    "    - ROC-AUC Score,\n",
    "    - Precision and Recall Scores,\n",
    "    - F1-Score.\n",
    "\n",
    "- Metrics for a regression model include:\n",
    "    - Root Mean Square Error (RMSE),\n",
    "    - R-Squared and Adjusted R-Square Metrics,\n",
    "    - Mean Absolute Error (MAE),\n",
    "    - Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "Calculating the model metrics above is only possible when you have the ground truth available."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdbe581c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prediction Drift\n",
    "\n",
    "(Scoring models when ground truth is NOT available)\n",
    "\n",
    "<img src=\"_images/02f_not_avai_y.png\">\n",
    "\n",
    "- Metrics:\n",
    "    - Hellinger Distance (HDDDM)\n",
    "    - Kullback-Leibler Divergence: đo sự khác biệt giữa 2 phân phối rời rạc\n",
    "    - Population Stability Index (PSI): đo sự khác biệt giữa 2 phân phối liên tục\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1422ef3-01ae-429f-bacb-9015a3addb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSI\n",
    "import numpy as np\n",
    "\n",
    "def _psi(expected: np.ndarray, actual: np.ndarray, bucket_type: str = \"bins\", n_bins: int = 10) -> float:\n",
    "    \"\"\"Calculate PSI metric for two arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        expected : list-like\n",
    "            Array of expected values\n",
    "        actual : list-like\n",
    "            Array of actual values\n",
    "        bucket_type : str\n",
    "            Binning strategy. Accepts two options: 'bins' and 'quantiles'. Defaults to 'bins'.\n",
    "            'bins': input arrays are split into bins with equal\n",
    "                and fixed steps based on 'expected' array\n",
    "            'quantiles': input arrays are binned according to 'expected' array\n",
    "                with given number of n_bins\n",
    "        n_bins : int\n",
    "            Number of buckets for binning. Defaults to 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A single float number\n",
    "    \"\"\"\n",
    "    breakpoints = np.arange(0, n_bins + 1) / (n_bins) * 100\n",
    "    if bucket_type == \"bins\":\n",
    "        breakpoints = np.histogram(expected, n_bins)[1]\n",
    "    elif bucket_type == \"quantiles\":\n",
    "        breakpoints = np.percentile(expected, breakpoints)\n",
    "\n",
    "    # Calculate frequencies\n",
    "    expected_percents = np.histogram(expected, breakpoints)[0] / len(expected)\n",
    "    actual_percents = np.histogram(actual, breakpoints)[0] / len(actual)\n",
    "    # Clip frequencies to avoid zero division\n",
    "    expected_percents = np.clip(expected_percents, a_min=0.0001, a_max=None)\n",
    "    actual_percents = np.clip(actual_percents, a_min=0.0001, a_max=None)\n",
    "    # Calculate PSI\n",
    "    psi_value = (expected_percents - actual_percents) * np.log(expected_percents / actual_percents)\n",
    "    psi_value = sum(psi_value)\n",
    "\n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def calculate_psi(\n",
    "        expected: np.ndarray, actual: np.ndarray, bucket_type: str = \"bins\", n_bins: int = 10, axis: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Apply PSI calculation to 2 1-d or 2-d arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expected : list-like\n",
    "        Array of expected values\n",
    "    actual : list-like\n",
    "        Array of actual values\n",
    "    bucket_type : str\n",
    "        Binning strategy. Accepts two options: 'bins' and 'quantiles'. Defaults to 'bins'.\n",
    "            'bins' - input arrays are split into bins with equal\n",
    "                and fixed steps based on ’expected' array\n",
    "            'quantiles' - input arrays are binned according to ’expected’ array\n",
    "                with given number of n_bins\n",
    "    n_bins : int\n",
    "        Number of buckets for binning. Defaults to 10.\n",
    "    axis : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        np.ndarray\n",
    "\n",
    "    Args:\n",
    "        axis:\n",
    "        axis:\n",
    "    \"\"\"\n",
    "    if len(expected.shape) == 1:\n",
    "        psi_values = np.empty(len(expected.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(expected.shape[axis])\n",
    "\n",
    "    for i in range(0, len(psi_values)):\n",
    "        if len(psi_values) == 1:\n",
    "            psi_values = _psi(expected, actual, bucket_type, n_bins)\n",
    "        elif axis == 0:\n",
    "            psi_values[i] = _psi(expected[:, i], actual[:, i], bucket_type, n_bins)\n",
    "        elif axis == 1:\n",
    "            psi_values[i] = _psi(expected[i, :], actual[i, :], bucket_type, n_bins)\n",
    "        return np.array(psi_values)\n",
    "\n",
    "\n",
    "calculate_psi(y_train_proba, y_produ_proba, bucket_type=\"bins\", n_bins=10, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5686e5b-50ce-4b42-b4a7-71a8d1670ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.589885181619163"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kullback\n",
    "from scipy.special import rel_entr\n",
    "P = [.05, .1, .2, .05, .15, .25, .08, .12]\n",
    "Q = [.3, .1, .2, .1, .1, .02, .08, .1]\n",
    "#calculate (P || Q)\n",
    "sum(rel_entr(P, Q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
