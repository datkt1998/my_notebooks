{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f787a73-8b81-4e0a-af3b-19ff1f93d85c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NLP Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ce658",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "__Some example of NLP:__\n",
    "- Text Classification\n",
    "- Text Generation\n",
    "- Machine Translation\n",
    "- Voice Assistants\n",
    "\n",
    "__Type of NLP's data:__\n",
    "- Text (email, blog, tweet, book,...)\n",
    "- Speech (Voice record, conversation,...)\n",
    "> NLP's datatype thường mô tả là sequences (a sequence of words), seq2seq đề cập tới việc tìm kiếm các info trong 1 sequence để tạo ra 1 sequence khác (Ví dụ như convert giọng nói thành text)\n",
    "\n",
    "\n",
    "__Model workflow__\n",
    "```\n",
    "Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)\n",
    "```\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-text-classification-inputs-and-outputs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8a9ee-38fb-48de-831a-0a829fcf6350",
   "metadata": {},
   "source": [
    "**Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff5abf-687a-468c-80ad-f4f86008ec70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN       deeds reason earthquake may allah forgive us   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  residents asked shelter place notified officer...   \n",
       "3   6     NaN      NaN  people receive wildfires evacuation orders cal...   \n",
       "4   7     NaN      NaN  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dataprep.clean import clean_text\n",
    "\n",
    "datafol = \"Datasets/nlp_getting_started\"\n",
    "train_df = pd.read_csv(datafol + \"/train.csv\").sample(frac=1, random_state=1)\n",
    "train_df = clean_text(train_df, \"text\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5baad6-06cc-45f4-9ef7-147ac151a056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input: 'text' column\n",
    "# Output: 'target' column ( 1 - diaster , 0 - not diaster)\n",
    "train_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb0954-40cb-4b92-ab3b-ced4e583e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"text\"],\n",
    "    train_df[\"target\"],\n",
    "    test_size=0.1,\n",
    "    stratify=train_df[\"target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb2c75-0743-452a-a5d5-b68c9b239924",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Converting text into numbers\n",
    "Có 2 term trong __NLP__ để turn text into numbers:\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n",
    "\n",
    "__1. Tokenization__\n",
    "\n",
    "Ánh xạ character/word/subword sang giá trị số numberical value. Có 3 level của tokenization\n",
    "- _Word-level tokenization_: Mỗi từ sẽ đại diện bởi 1 numerical value. Ví dụ: \"I love yout\" ---> [0,1,2]\n",
    "- _Character-level tokenization_: Mỗi character sẽ đại diện cho 1 token. Ví dụ như convert A-Z to 1-26\n",
    "- _Sub-word tokenization_: break từng từ thành các phần và tokenization nó, khi đó mỗi word có thể thành nhiều tokens. \n",
    "\n",
    "> Tuỳ thuộc vào problem mà nên chọn level tokenization cho phù hợp, hoặc có thể thử các level và kiểm tra performance, hoặc có thể sử dụng `tf.keras.layers.concatenate` để combine/stacking chúng lại với nhau.\n",
    "\n",
    "__2. Embeddings__\n",
    "\n",
    "Đại diện cho natural language được học được biểu diễn dưới dạng vector, bổ sung phần relationships giữa các tokens và có thể learning trong lúc train thay vì chỉ là những con số static. Ví dụ như từ \"Anh\" được đại diện bởi 1 vector [0.1, 0.3, 0.4] có độ dài bằng 3. Chú ý là size of vector có thể cần được tunning cho phù hợp. \n",
    "\n",
    "Các từ đồng nghĩa/sát nghĩa thì sau khi chạy qua Embeddings layers/model (ví dụ __word2vec__) sẽ trả ra các vector gần bằng nhau, thể hiện nếu biểu diễn trong không gian thì các từ này sẽ gần nhau.\n",
    "\n",
    "> Có thể upload các `vector output` kèm `labels` (các words) lên [__TensorFlow Embedding Projector__](http://projector.tensorflow.org/?_gl=1*1jn4yxx*_ga*NTEzMDY1NzcwLjE2ODIxNTU3Mjg.*_ga_W0YLR4190T*MTY4Mzc4MzIwNy41Mi4xLjE2ODM3ODMzMjQuMC4wLjA.) để visualize tính relationships giữa các words\n",
    "\n",
    "Có 2 hướng tiếp cận Embeddings:\n",
    "- ___Create your own embedding___: Để sử dụng được embedding, text cần phải được turn into numbers, sau đó sử dụng `embedding layer` (such as `tf.keras.layers.Embedding`) để learn trong quá trình training\n",
    "- ___Reuse a pre-learned embedding___: Sử dụng các tham số embedding trong các pre-train model để fine-tune lại own model. Các pre-train này học từ một số lượng lớn text ( Ví dụ như all of Wikipedia) nên có khả năng đại diện chính xác mỗi quan hệ giữa các từ \n",
    "\n",
    "> Sử dụng cách search tương tự như computer vision pretrain model trên [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) để lựa chọn ra một số pre-train model hiệu quả như  [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/),.. \n",
    "\n",
    "Có 2 cách thức embedding:\n",
    "- ___Continuous Bag-of-Words (CBOW)___:\n",
    "\n",
    "Phương pháp này lấy inputs đầu vào là một/nhiều từ context word và cố gắng dự đoán output là target word thông qua một tầng neural đơn giản. Nhờ việc đánh giá output error với target word ở dạng one-hot, mô hình có thể điều chỉnh weight, học được vector biểu diễn cho target word. Ví dụ ta có một câu tiếng anh như sau : \"I love you\". Ta có Input context word là \"love\" và Output target word là \"you\". Ta biến đổi input context đầu vào dưới dạng one-hot đi qua một tầng hidden layer và thực hiện softmax phân loại để dự đoán ra từ tiếp theo là gì.\n",
    "![](https://images.viblo.asia/1df2b1bc-c823-4b36-be92-46755788506a.png)\n",
    "\n",
    "- ___Skip-gram___:\n",
    "\n",
    "Nếu như __CBOW__ sử dụng input là __context word__ và cố gắng dự đoán từ đầu ra (__target word__) thì ngược lại, mô hình __Skip-gram__ sử dụng input là __target word__ và cố gắng dự đoán ra các từ hàng xóm của nó. Chúng định nghĩa các từ là hàng xóm (__neightbor word__) của nó thông qua tham số __window size__. _Ví dụ nếu bạn có một câu như sau: \"Tôi thích ăn cua hoàng đế\". Và input target word ban đầu là từ cua. Với kích thước window size = 2, ta sẽ có các neighbor word (thích, ăn, hoàng, đế ). Và chúng ta sẽ có 4 cặp input-output như sau: (cua, thích ), (cua, hoàng ), (cua, đế ), (cua, ăn )_. Các __neightbor word__ được coi như nhau trong quá trình training.\n",
    "\n",
    "![](https://images.viblo.asia/d6dd1927-085e-45a7-89d0-0bd3ea152827.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae903d-eb56-4ea4-9ed2-3b20f48b6478",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)\n",
    "\n",
    "Sử dụng [`tf.keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) với một số params như sau:\n",
    "- `max_tokens` - Số lượng word tối đa trong vocabulary (e.g. 20000 or the number of unique words in your text), bao gồm 1 slot cho OOV (out of vocabulary) tokens.\n",
    "- `standardize` - Phương thức để standardizing text. Default is \"lower_and_strip_punctuation\" nghĩa là lowers text and removes all punctuation marks.\n",
    "- `split` - How to split text, default is \"whitespace\" which splits on spaces.\n",
    "- `ngrams` - How many words to contain per token split (create groups of n-words?), for example, ngrams=2 splits tokens into continuous sequences of 2.\n",
    "- `output_mode` - How to output tokens:\n",
    "    - \"int\" (integer mapping): map theo index của từ trong vocab\n",
    "    - \"multi_hot\" : mapping theo kiểu one-hot nếu từ đó có xuất hiện trong text\n",
    "    - \"count\": mapping theo số lần từ đó xuất hiện trong text\n",
    "    - \"tf-idf\"\n",
    "- `output_sequence_length` - Sử dụng trong `output_mode=int`, quy định độ dài của mỗi sequence output gồm bao nhiêu tokens, nếu sequence có độ dài hơn `output_sequence_length` thì sẽ được truncated, nếu ít hơn thì được padded để đảm bảo độ dài chính xác của output tensor là `shape = (batch_size, output_sequence_length)`\n",
    "- `pad_to_max_tokens` - Defaults to False, if True, the output feature axis sẽ được padded/mở rộng tới độ dài bằng max_tokens ngay cả khi số lượng unique tokens in the vocabulary nhỏ hơn max_tokens. Chỉ có tác dụng trong `output_mode` là `multi_hot`, `count`, `tf_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "864aaced-267e-4e24-a3c0-0b44242a41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce7dc8-4f81-4ea9-b2f5-16fe36505b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 21)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kiểm tra số lượng words trung bình mỗi text ---> set for 'output_sequence_length'\n",
    "avg_len = round(x_train.map(lambda x: len(x.split(\" \"))).sum() / len(x_train))\n",
    "max_len = x_train.map(lambda x: len(x.split(\" \"))).max()\n",
    "avg_len, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c22b2844-8628-487e-b64b-1cf3abfafd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.map(lambda x: len(x.split(\" \"))).quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e9062-4dcd-4782-9633-822c980b8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "max_vocab_length = 10000  # or 20k, 30k or 32,179\n",
    "max_sequence_length = 15\n",
    "\n",
    "\n",
    "tvect = layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_sequence_length=max_sequence_length,\n",
    "    output_mode=\"int\",\n",
    "    name=\"text_vectorization\",\n",
    ")\n",
    "\n",
    "# fit tvect to datatrain\n",
    "tvect.adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f21ca5-e77d-4c01-9b7d-d24cf4d0152f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'video slain mexican journalist unknowingly predicted death via breitbartnews'\n",
      "----> [ 12, 7864, 1, 5051, 6652, 2646, 78, 7, 1, 0, 0, 0, 0, 0, 0 ]\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "import random\n",
    "\n",
    "\n",
    "def check_vectorize(sen, tvect=tvect):\n",
    "    res = tvect(sen)\n",
    "    print(\n",
    "        f\"'{sen}'\\n---->\", \"[\", \", \".join([str(i) for i in res.numpy()]), \"]\"\n",
    "    )\n",
    "\n",
    "\n",
    "check_vectorize(random.choice(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd594e-8e5a-4b8c-974d-181707d5dccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'water main break disrupts'\n",
      "----> [ 92, 1260, 856, 2349, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]\n",
      "'water main break disrupts trolley service sandiego water main break disrupts trolley service sandiego abc cde main'\n",
      "----> [ 92, 1260, 856, 2349, 2494, 239, 3290, 92, 1260, 856, 2349, 2494, 239, 3290, 440 ]\n",
      "'water main break disrupts trolley service sandiego water main break disrupts trolley service sandiego abc'\n",
      "----> [ 92, 1260, 856, 2349, 2494, 239, 3290, 92, 1260, 856, 2349, 2494, 239, 3290, 440 ]\n"
     ]
    }
   ],
   "source": [
    "# nếu độ dài ngắn hơn 15 thì được pad bằng 0\n",
    "check_vectorize(\"water main break disrupts\")\n",
    "\n",
    "# nếu độ dài text dài hơn 15 thì chỉ lấy 15 word đầu tiên\n",
    "check_vectorize(\n",
    "    \"water main break disrupts trolley service sandiego water main break disrupts trolley service sandiego abc cde main\"\n",
    ")\n",
    "check_vectorize(\n",
    "    \"water main break disrupts trolley service sandiego water main break disrupts trolley service sandiego abc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74791d94-a2c7-4308-9d27-1b8eed67edd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common (the most occurrence) words: ['', '[UNK]', 'like', 'fire', 'new']\n",
      "Bottom 5 least common words: ['monarchy', 'mon', 'momneedscoffee', 'mommys', 'mommyisbomb']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary order by commom (most occurrence)\n",
    "words_in_vocab = tvect.get_vocabulary()\n",
    "\n",
    "# most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "top_5_words = words_in_vocab[:5]\n",
    "\n",
    "# least common tokens\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common (the most occurrence) words: {top_5_words}\")\n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921038c8-5151-42dc-92a3-f32c3d18f825",
   "metadata": {},
   "source": [
    "> '[UNK]' = unknown = Out-of-Vocabolary (index = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157f5ae-0ae3-4147-ad22-d335fd5b2781",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "Sử dụng [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) với một số params như sau:\n",
    "- `input_dim` - The size of the vocabulary (e.g. len(text_vectorizer.get_vocabulary()).\n",
    "- `output_dim` - The size of the output embedding vector, for example, a value of 100 outputs a feature vector of size 100 for each word.\n",
    "- `embeddings_initializer` - How to initialize the embeddings matrix, default is `\"uniform\"` which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n",
    "- `input_length` - Length of sequences being passed to embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea0c9b-aab9-49a7-8e17-30ef8f7331b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.02957509,  0.03379121,  0.04258261, ..., -0.04872347,\n",
       "         -0.00656006,  0.01681853],\n",
       "        [-0.02690672, -0.02275765, -0.01154822, ...,  0.02936261,\n",
       "          0.04035657,  0.01228539],\n",
       "        [ 0.0207009 ,  0.00322819, -0.02702802, ..., -0.01606815,\n",
       "          0.00701294,  0.04634335],\n",
       "        ...,\n",
       "        [ 0.0172099 , -0.03784468,  0.03696242, ..., -0.03041265,\n",
       "         -0.03056842,  0.0070639 ],\n",
       "        [ 0.0172099 , -0.03784468,  0.03696242, ..., -0.03041265,\n",
       "         -0.03056842,  0.0070639 ],\n",
       "        [ 0.0172099 , -0.03784468,  0.03696242, ..., -0.03041265,\n",
       "         -0.03056842,  0.0070639 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# mỗi 1 token sẽ được embedding thành 1 vector có chiều dài là output_dim\n",
    "# mỗi 1 sequence được embedding thành 1 tensor có shape là (input_length, output_dim)\n",
    "# số lượng weights params là (output_dim * input_dim)\n",
    "\n",
    "embedding = layers.Embedding(\n",
    "    input_dim=max_vocab_length,  # set input shape\n",
    "    output_dim=128,  # set size of embedding vector\n",
    "    embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "    input_length=max_sequence_length,  # how long is each input\n",
    "    name=\"embedding_1\",\n",
    ")\n",
    "\n",
    "embedding(tvect([\"water main break disrupts trolley service\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d74188-9c48-4c3f-9d47-9c44a65c70d1",
   "metadata": {},
   "source": [
    "> Các giá trị embedding này được khởi tạo bàn đầu ngẫu nhiên, trong quá trình train thì sẽ được learn để tạo ra relationship phù hợp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e68dcf-98c9-4d7b-8b3c-e7ff6a79d700",
   "metadata": {},
   "source": [
    "**Visualize the embedding on Embedding Projector**\n",
    "\n",
    "To visualize on [__Embedding Projector__](http://projector.tensorflow.org/_), cần chuẩn bị 2 file bao gồm:\n",
    "- The embedding vector (embedding weights)\n",
    "- the meta data of vector (vocabulary)\n",
    "\n",
    "Then:\n",
    "1. Go to http://projector.tensorflow.org/\n",
    "2. Click on \"Load data\"\n",
    "3. Upload the two files you downloaded (embedding_vectors.tsv and embedding_metadata.tsv)\n",
    "4. Explore\n",
    "5. Optional: You can share the data you've created by clicking \"Publish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27861c41-ba8a-449f-b9a7-9a3478c8efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding vector and metadata\n",
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# get vocab and embedding_weight from model\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "words_in_vocab = model_1.get_layer(\"text_vectorization_1\").get_vocabulary()\n",
    "\n",
    "\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "    if num == 0:\n",
    "        continue  # skip padding token\n",
    "    vec = embed_weights[num]\n",
    "    out_m.write(word + \"\\n\")  # write words to file\n",
    "    out_v.write(\n",
    "        \"\\t\".join([str(x) for x in vec]) + \"\\n\"\n",
    "    )  # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72d0e3-88b9-4384-b722-96a1c75d0254",
   "metadata": {},
   "source": [
    "## Core Techniques Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9421d-d11b-4439-bff9-5720f9b83596",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN's)\n",
    "\n",
    "Khi đọc và hiểu 1 words thì để hiểu từ đó trong cả câu/ngữ cảnh đó, hay nói cách khác cần phải hiểu bối cảnh của câu hoặc các words trước đó. Ví dụ chúng ta có 2 câu là \"Bố ăn tối chưa ?\" và \"Bố chưa ăn tối\". Mặc dù cả 2 sequence đều có các từ giống nhau nhưng khác nhau về mặt ý nghĩa. Thứ tự các words quyết định ý nghĩa của sentence.\n",
    "\n",
    "Khi RNN looks at 1 sequence of text (dưới dạng numeric), các pattern sẽ được học 1 cách liên tục dựa vào order của sequence đó. RNN's là mạng đặc trưng bởi việc lấy những input(x) + previous inputs để compute Y, helpful when dealing with sequences data, such as natural language text.\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc)\n",
    "\n",
    "Với mỗi timestep t, the activation $a^{<t>}$ và output $y^{t}$ được tính theo công thức\n",
    "> $$a^{<t>}=g_{1}(W_{a a}a^{<t-1>}+W_{a x}x^{<t>}+b_{a})$$\n",
    "\n",
    "> $$y^{<t>}=g_{2}(W_{y a}a^{<t>}+b_{y})$$\n",
    "\n",
    "Trong đó các hệ số `W` và `b` là các weights sẽ được update trong quá trình learn, còn `g1`, `g2` là các hàm activation functions\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn-ltr.png?74e25518f882f8758439bcb3637715e5)\n",
    "\n",
    "__Ưu điểm của RNN__:\n",
    "- Xử lý input có bất kỳ độ dài như thế nào\n",
    "- Model size not increasing with size of input\n",
    "- Computation takes into account history information\n",
    "- Weights được share theo thời gian\n",
    "\n",
    "__Hạn chế của RNN__:\n",
    "- Phải thực hiện tuần tự data, nên ko tuận dụng được sức mạnh tính toán song song (CPU/GPU), tính toán lâu\n",
    "- Khó trong việc accessing những long-history information vào thời điểm hiện tại\n",
    "- Cannot consider any future input for the current state\n",
    "- Vanishing gradient: do các hàm activation trong RNN thường là `tanh` (có output y [-1,1] và đạo hàm [0,1]) và `sigmoid` (có output y [0,1] và đạo hàm [0,0.25]), rất dễ gây ra đạo hàm = 0 với các giá trị activation_input lớn khiến các weights phía xa đều không được update, tức là các node phía xa không còn tác dụng nhiều tới node hiện tại nữa. Có một số cách khắc phục bằng việc:\n",
    "    - Sử dụng activation là __ReLU__ hoặc các biến thể\n",
    "    - Sử dụng 1 số mạng biến thể như __GRU__ hay __LSTM__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aad948-f6ab-404c-a550-09cb38361032",
   "metadata": {},
   "source": [
    "##### Một số các triển khai của RNN's\n",
    "\n",
    "![](https://3863425935-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LIA3amopGH9NC6Rf0mA%2F-LIA3mTJltflw3MVKAEQ%2F-LIA3nSKrqNJpLgASeso%2Fsequence.png?generation=1532415397328022&alt=media)\n",
    "\n",
    "- ___One to one___: one input, one output. Ví dụ: image classification,...\n",
    "- ___One to many___: one input, many output. Ví dụ: image captioning (input 1 image, output ra 1 sequence of text as image caption)\n",
    "- ___Many to one___: many input, one output. Ví dụ: text classification,...\n",
    "- ___Many to many___: many input, many output. Ví dụ: machine translation, speech to text,...\n",
    "\n",
    "Ví dụ __Many to many__:\n",
    "\n",
    "![](https://images.viblo.asia/4a1049be-e04c-482b-b8f4-775a7bd55c15.png)\n",
    "\n",
    "Nếu như mạng NN chỉ là input_layer $x$ đi qua hidden_layer $h$ và cho ra output_layer $y$ với __fully connected__ giữa các layers thì trong RNN, các input $x_t$ sẽ được kết hợp với hidden layer $h_{t-1}$ chạy qua activation function $g_1$ để tính toán ra hidden layer $h_t$. Thường hàm $g_1$ là hàm $\\tanh$ kết hợp với tập hợp các trọng số W (tính là total Loss từ L1, L2,..Lt). Ngoài ra còn có activation function $g_2$ khi tính toán output $y_t$.\n",
    "$$h_t = g_1(h_{t-1}, x_t) = \\tanh (W_{hh}h_{t-1} + W_{xh}x_{t} + b_h)$$\n",
    "Tính output $y_t$:\n",
    "$$y_t = g_2(W_{hy}h_t + b_y)$$\n",
    "\n",
    "Tổng hợp quá trình tính toán được thể hiện:\n",
    "\n",
    "![](https://images.viblo.asia/4b1cc09d-99fa-422a-9bee-14908aace750.png)\n",
    "\n",
    "Trong mạng NN thì chỉ có 1 matrix $W$ duy nhất, nhưng trong mạng RNN thì có 3 matrix trọng số:\n",
    "- $W_{hh}$: là matrix trọng số của \"bộ nhớ trước\" $h_{t-1}$\n",
    "- $W_{xh}$: là matrix trọng số của \"input hiện tại\" $x_t$\n",
    "- $W_{hy}$: là matrix trọng số của \"bộ nhớ hiện tại\" $h_t$ để tạo ra output $y_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ee7dc-042a-4853-9dfe-a77e366a84ca",
   "metadata": {},
   "source": [
    "##### Applications of RNNs\n",
    "\n",
    "| Loại network | minh hoạ | ứng dụng  | \n",
    "| --------- | ------ |------ |\n",
    "| One-to-One | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-one-to-one-ltr.png?9c8e3b04d222d178d6bee4506cc3f779) | Traditional Neral network |\n",
    "| One-to-many | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-one-to-many-ltr.png?d246c2f0d1e0f43a21a8bd95f579cb3b) | Music generation |\n",
    "| Many-to-one | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-one-ltr.png?c8a442b3ea9f4cb81f929c089b910c9d) |  Sentiment classification |\n",
    "| Many-to-many | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-same-ltr.png?2790431b32050b34b80011afead1f232) |  Name entity recognition |\n",
    "| Many-to-many | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-different-ltr.png?8ca8bafd1eeac4e8c961d9293858407b) | Machine translation |\n",
    "\n",
    "RNN cho phép ta dự đoán xác suất của một từ mới nhờ vào các từ đã biết liền trước nó. Cơ chế này hoạt động giống với ví dụ bên trên, với các đầu ra của cụm này sẽ là đầu vào của cụm tiếp theo cho đến khi ta được một câu hoàn chỉnh. Các input thường được encode dưới dạng 1 vector one hot encoding. Ví dụ với tập dataset gồm 50000 câu ta lấy ra được một dictionary gồm 4000 từ, từ \"hot\" nằm ở vị trí 128 thì vector one hot của từ \"hot\" sẽ là một vector gồm 4000 phần tử đều bằng 0 chỉ có duy nhất vị trí 128 bằng 1. Mô hình này này chính là mô hình Many to Many với số lượng đầu ra, đầu vào và lớp ẩn bằng nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9b1cbe-6d63-416b-b575-cee397e15237",
   "metadata": {},
   "source": [
    "##### Loss function\n",
    "\n",
    "Loss function của RNNs bằng tổng loss tại mỗi output trong mạng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2279e3-9bee-4a5f-8d98-57060011a09a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Handling long term dependencies\n",
    "\n",
    "1. Activation Function\n",
    "\n",
    "Các hàm activation function phổ biến trong RNNs là : __sigmoid, tanh, ReLU__\n",
    "\n",
    "2. Vanishing/exploding gradient\n",
    "\n",
    "Hiện tượng gradient biến mất hoặc bùng nổ thường xuyên xảy ra trong mạng RNN, nên rất khó trong việc capture những yếu tố dài hạn phía trước ảnh hưởng tới hiện tại.\n",
    "\n",
    "__Gradient clipping__ thường được sử dụng để setting max value of gradient trong TH gặp phải vấn đề gradient exploding\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/gradient-clipping-en.png?6c3de441dc56aad634dc1a91accb48f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19389bb8-47dc-4969-acc6-1836269deb47",
   "metadata": {},
   "source": [
    "| Tham số | LSTM | GRU  | \n",
    "| --------- | ------ |------ |\n",
    "| Minh hoạ | ![](https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_768/http://dprogrammer.org/wp-content/uploads/2019/04/LSTM-Core-768x466.png) | ![](https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_768/http://dprogrammer.org/wp-content/uploads/2019/04/GRU-768x502.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d7979-15e1-498a-b8f7-883733b673d8",
   "metadata": {},
   "source": [
    "| Loại gate - state |  Công thức  | Vai trò | minh hoạ |\n",
    "| --------- | ------ |------ |------ |\n",
    "| __Forget gate__ $f_t$ | $$ f_t = \\text{sigmoid}(W_f.[h_{t-1}, x_t] + b_f) $$  |  Forget gate quyết định thông tin nào từ bộ nhớ dài hạn được lưu giữ hoặc loại bỏ | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313eff13-6c41-45b2-95b2-8fb1b1e9bf3a",
   "metadata": {},
   "source": [
    "$C_{t}\\,\\longrightarrow\\,f_{t}\\,*\\,C_{t-1}\\,+\\,\\dot{\\iota}_{t}\\,*\\,\\widetilde C_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc21b0-2461-49a3-a7fb-ff568f92257c",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "[Long short-term memory cells (LSTMs)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) khác RNN ở điểm thay vì 1 tầng mạng neural với hàm `tanh` thì LSTM có 4 tâng (4 cổng gate) tương tác với nhau, nhờ vậy mà có thể bỏ đi hoặc thêm vào các thông tin cần thiết thông qua các gate, một nơi giúp sàng lọc thông tin với activation là 1 hàm sigmoid.\n",
    "> Tầng sigmoid cho output trong khoảng [0,1], mô tả có bao nhiêu thông tin có thể được thông qua. Khi output là 0 có nghĩa là không có thông tin nào, nếu là 1 thì có nghĩa cho tất cả các thông tin đi qua.\n",
    "\n",
    "![](https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_768/http://dprogrammer.org/wp-content/uploads/2019/04/LSTM-Core-768x466.png)\n",
    "\n",
    "\n",
    "| Loại gate - state |  Công thức  | Vai trò | minh hoạ |\n",
    "| --------- | ------ |------ |------ |\n",
    "| __Forget gate__ $f_t$ | $$ f_t = \\text{sigmoid}(W_f.[h_{t-1}, x_t] + b_f) $$  |  Forget gate quyết định thông tin nào từ bộ nhớ dài hạn được lưu giữ hoặc loại bỏ | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png) |\n",
    "| __Input gate__ $i_t$ |  $$ i_t = \\text{sigmoid}(W_i.[h_{t-1}, x_t] + b_i) $$| Cổng đầu vào quyết định thông tin nào sẽ được lưu trữ trong bộ nhớ dài hạn. Nó chỉ hoạt động với thông tin từ đầu vào hiện tại và bộ nhớ ngắn hạn từ bước trước. Tại cổng này, nó lọc ra thông tin từ các biến không hữu ích | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png) |\n",
    "| __Output gate__ ($o_t$) |  $$ o_t = \\text{sigmoid}(W_o.[h_{t-1}, x_t] + b_o) $$ | Output gate sử dụng $x_t$, $h_{t-1}$ và long-term memory mới vừa được tính để tạo ra bộ lọc cho short-term memory $h_t$ (để dùng trong next step) và cấu phần ra the cell state $C_t$ (dùng trong step hiện tại) | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png) |\n",
    "| __Hidden state__ ($\\tilde{C}_{t}$) | $$ \\tilde{C}_{t} = \\text{tanh}(W_c.[h_{t-1}, x_t] + b_c) $$  |  Trạng thái ẩn tạm thời cấu phần ra the cell state $C_t$ (dùng trong step hiện tại)  | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png) |\n",
    "| __Cell state__ ($C_{t}$) | $$ C_{t} = f_t*C_{t-1} + i_t*\\tilde{C}_{t} $$ | là bộ nhớ trong của LSTM được tổng hợp của bộ nhớ trước $C_{t-1}$ đã được lọc qua forget gate $f_t$ và trạng thái ẩn $\\tilde{C}_{t}$ đã được lọc qua Input gate $i_t$, từ đó các thông tin quan trọng (__long-term memory__) sẽ được đi xa hơn và sẽ được dùng khi cần | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png) |\n",
    "| __Short-term memory Output__ ($h_{t}$) | $$ h_{t} = o_t*\\text{tanh}({C}_{t}) $$  |  Cell state $C_{t}$ sau khi qua tanh activation sẽ được lọc 1 lần nữa qua Output gate $o_t$ tạo ra output của step. | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png) |\n",
    "\n",
    "- Nếu nhìn kỹ một chút, ta có thể thấy RNN truyền thống là dạng đặc biệt của LSTM. Nếu thay giá trị đầu ra của input gate = 1 và đầu ra forget gate = 0 (không nhớ trạng thái trước)\n",
    "- LSTM có long-term memory. Tuy nhiên, LSTM khá giống với RNN truyền thống, tức có short-term memory. Nhìn chung, LSTM giải quyết phần nào vanishing gradient so với RNN, nhưng chỉ một phần.\n",
    "- Với lượng tính toán như trên, RNN đã chậm, LSTM nay còn chậm hơn.\n",
    "\n",
    "__Reference:__\n",
    "- https://dominhhai.github.io/vi/2017/10/what-is-lstm/#3-2-bên-trong-lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb2735-67a1-40ec-bd44-087aa34d0225",
   "metadata": {},
   "source": [
    "### GRU\n",
    "Gated Recurrent Unit (GRU) là 1 TH đặc biệt của LSTM. GRU sử dụng less training parameter nên do đó sử dụng less memory and executes faster than LSTM trong khi đó LSTM is more accurate on a larger dataset. \n",
    "- LSTM if you are dealing with large sequences and accuracy is concerned\n",
    "- GRU is used when you have less memory consumption and want faster results. \n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)\n",
    "\n",
    "GRU chỉ có 2 cổng: cổng thiết lập lại `r` và cổng cập nhập `z`. Cổng thiết lập lại sẽ quyết định cách kết hợp giữa đầu vào hiện tại với bộ nhớ trước, còn cổng cập nhập sẽ chỉ định có bao nhiêu thông tin về bộ nhớ trước nên giữa lại. Như vậy RNN thuần cũng là một dạng đặc biệt của GRU, với đầu ra của cổng thiết lập lại là 1 và cổng cập nhập là 0.\n",
    "\n",
    "__What is the difference between GRU & LSTM?__\n",
    "- The GRU has 2 gates, LSTM has 3 gates\n",
    "- GRU không có bộ nhớ trong và output gate như LSTM\n",
    "- 2 cổng vào và cổng quên được kết hợp lại thành cổng cập nhập z và cổng thiết lập lại r sẽ được áp dụng trực tiếp cho trạng thái ẩn trước.\n",
    "- GRU không sử dụng một hàm phi tuyến tính để tính đầu ra như LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16830e1-c646-4036-80ee-60c5f5114c35",
   "metadata": {},
   "source": [
    "### Bidirectional (BRNN)\n",
    "\n",
    "| BRNN |  áp dụng  |  minh hoạ |\n",
    "| --------- | ------ |------ |\n",
    "| BRNN khác RNN một điểm là thay vì process 1 sequense từ trái ---> phải thì BRNN process 2 chiều (thêm cả từ phải ---> trái) | Việc phân tích câu cả 2 chiều có khả năng cải thiện performance tuy nhiên chi phí training và số lượng các tham số sẽ phải x2  | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/bidirectional-rnn-ltr.png?e3e66fae56ea500924825017917b464a) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f24b7-627f-4a00-8c19-26d6520b660e",
   "metadata": {},
   "source": [
    "### Deep (DRNN)\n",
    "\n",
    "| DRNN |  áp dụng  |  minh hoạ |\n",
    "| --------- | ------ |------ |\n",
    "| DRNN | DRNN | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/deep-rnn-ltr.png?f57da6de44ddd4709ad3b696cac6a912) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28fbe7-3f19-4d6d-b32a-d7fac07a2073",
   "metadata": {},
   "source": [
    "## Practice modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411637f0-580a-418c-8aaa-7b4ea5f0a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "\n",
    "def get_performance(y_val, y_pred, model_name=\"baseline\"):\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        y_val, y_pred\n",
    "    )\n",
    "    class_name = [\"class0\", \"class1\"]\n",
    "    df = pd.DataFrame(\n",
    "        [precision, recall, fscore],\n",
    "        columns=class_name,\n",
    "        index=[\"precision\", \"recall\", \"fscore\"],\n",
    "    )\n",
    "    df.loc[\"accuracy\"] = accuracy_score(y_val, y_pred)\n",
    "    df = df.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "    df[\"model_name\"] = model_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3ef28-cb1e-4da4-aa68-cc0cf821ad51",
   "metadata": {},
   "source": [
    "### List models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b29d1-a5b8-4f84-931c-d9e7e31579cb",
   "metadata": {},
   "source": [
    "#### Model0: baseline model\n",
    "Sử dụng [TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) formula để convert words sang dạng numeric và model chúng bằng [Multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB), model này thường được refering cho các model dữ liệu text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c960e26-c541-41eb-97bc-729d54728350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84       435\n",
      "           1       0.83      0.69      0.76       327\n",
      "\n",
      "    accuracy                           0.81       762\n",
      "   macro avg       0.81      0.79      0.80       762\n",
      "weighted avg       0.81      0.81      0.80       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# create model pipeline\n",
    "model_0 = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),  # convert words to numbers using TF-IDF\n",
    "        (\"clf\", MultinomialNB()),  # model the text\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit model\n",
    "model_0.fit(x_train, y_train)\n",
    "\n",
    "# evaluate our model\n",
    "y_pred = model_0.predict(x_val)\n",
    "model_0_res = get_performance(y_val, y_pred, model_name=\"0_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223a387-a65c-40d8-8825-5d711d2fbac8",
   "metadata": {},
   "source": [
    "#### model1: simple dense\n",
    "Với dữ liệu đầu vào, ta thực hiện theo các bước\n",
    "```\n",
    "tokenization ---> embedding ---> average pooling ---> fully connected dence with sigmoid activation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6bc20-9f87-4c5d-b01f-07febcfe4d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_model(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_1_dense\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = build_model(10000, 15)\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac862cd8-ea98-4385-bd6d-e5377a64df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c3f92-26d5-4f3c-a2d5-db40c8bf4927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.6289 - accuracy: 0.6509 - val_loss: 0.5621 - val_accuracy: 0.7493\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.4534 - accuracy: 0.8206 - val_loss: 0.4598 - val_accuracy: 0.7927\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.3400 - accuracy: 0.8673 - val_loss: 0.4364 - val_accuracy: 0.8045\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2745 - accuracy: 0.8956 - val_loss: 0.4397 - val_accuracy: 0.7913\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2277 - accuracy: 0.9177 - val_loss: 0.4600 - val_accuracy: 0.7953\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce195b0-2156-498b-b013-7e6a324b76c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       435\n",
      "           1       0.80      0.69      0.74       327\n",
      "\n",
      "    accuracy                           0.80       762\n",
      "   macro avg       0.80      0.78      0.79       762\n",
      "weighted avg       0.80      0.80      0.79       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_1.predict(x_val).round().squeeze()\n",
    "model_1_res = get_performance(y_val, y_pred, model_name=\"1_simple_dence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e22a1-b83c-4f05-be1a-4b9eaab080ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding vector and metadata\n",
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\n",
    "    \"Datasets/nlp_getting_started/embedding_vectors.tsv\", \"w\", encoding=\"utf-8\"\n",
    ")\n",
    "out_m = io.open(\n",
    "    \"Datasets/nlp_getting_started/embedding_metadata.tsv\",\n",
    "    \"w\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "# get vocab and embedding_weight from model\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "words_in_vocab = model_1.get_layer(\"text_vectorization_1\").get_vocabulary()\n",
    "\n",
    "\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "    if num == 0:\n",
    "        continue  # skip padding token\n",
    "    vec = embed_weights[num]\n",
    "    out_m.write(word + \"\\n\")  # write words to file\n",
    "    out_v.write(\n",
    "        \"\\t\".join([str(x) for x in vec]) + \"\\n\"\n",
    "    )  # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77ebd0-d068-4aca-8ba3-871b4bcc0cc2",
   "metadata": {},
   "source": [
    "#### model2: LSTM model\n",
    "\n",
    "[`tensorflow.keras.layers.LSTM()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2026611-cf2c-4e47-a48b-f47ea5da30a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_lstm(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = keras.layers.LSTM(64)(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_2 = build_lstm(10000, 15)\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37895c2-182d-48a5-92e1-09bf9d57f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 10s 44ms/step - loss: 0.5158 - accuracy: 0.7396 - val_loss: 0.4424 - val_accuracy: 0.8071\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2985 - accuracy: 0.8824 - val_loss: 0.4780 - val_accuracy: 0.7835\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 16ms/step - loss: 0.1931 - accuracy: 0.9295 - val_loss: 0.5891 - val_accuracy: 0.7559\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1324 - accuracy: 0.9486 - val_loss: 0.7432 - val_accuracy: 0.7835\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0908 - accuracy: 0.9631 - val_loss: 0.7791 - val_accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "model_2_history = model_2.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6c235-b6ae-41e8-aac4-a1b23658f1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       435\n",
      "           1       0.74      0.68      0.71       327\n",
      "\n",
      "    accuracy                           0.76       762\n",
      "   macro avg       0.76      0.75      0.75       762\n",
      "weighted avg       0.76      0.76      0.76       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_2.predict(x_val).round().squeeze()\n",
    "model_2_res = get_performance(y_val, y_pred, model_name=\"2_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b62e1-4e2c-4253-88ab-e2ed0466446a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa826449-a764-4d9c-9e1c-89969e744f53",
   "metadata": {},
   "source": [
    "#### model3: GRU model\n",
    "\n",
    "[`tensorflow.keras.layers.GRU()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers_GRU -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2fa4f4-49c9-4d0f-a293-48428265cf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_gru(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = keras.layers.GRU(64)(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_3_GRU\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_3 = build_gru(10000, 15)\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38573217-0e8e-45a8-8caf-555c0e641f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 24ms/step - loss: 0.6505 - accuracy: 0.6179 - val_loss: 0.5060 - val_accuracy: 0.7690\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3814 - accuracy: 0.8348 - val_loss: 0.4461 - val_accuracy: 0.7979\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.2317 - accuracy: 0.9066 - val_loss: 0.5470 - val_accuracy: 0.7848\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1470 - accuracy: 0.9448 - val_loss: 0.7074 - val_accuracy: 0.7520\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1061 - accuracy: 0.9594 - val_loss: 0.7763 - val_accuracy: 0.7559\n"
     ]
    }
   ],
   "source": [
    "model_3_history = model_3.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d64c53-0f80-4eb9-a190-8c94b276b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.79       435\n",
      "           1       0.74      0.66      0.70       327\n",
      "\n",
      "    accuracy                           0.76       762\n",
      "   macro avg       0.75      0.74      0.75       762\n",
      "weighted avg       0.75      0.76      0.75       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_3.predict(x_val).round().squeeze()\n",
    "model_3_res = get_performance(y_val, y_pred, model_name=\"3_GRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79805e1-432d-4cf3-91fa-d00b180ec11d",
   "metadata": {},
   "source": [
    "#### model4: Bidirectonal RNN\n",
    "\n",
    "[`tensorflow.keras.layers.Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) là một cách tiếp cận training 2 chiều thay vì một chiều, do đó có thể wrap với bất kỳ layers nào nếu muốn training 2 chiều"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b8dc1-5bec-4d8d-b6ec-db8561e49876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 25ms/step - loss: 0.6467 - accuracy: 0.6189 - val_loss: 0.4780 - val_accuracy: 0.8005\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3829 - accuracy: 0.8339 - val_loss: 0.4440 - val_accuracy: 0.8031\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.2445 - accuracy: 0.9053 - val_loss: 0.5274 - val_accuracy: 0.7992\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1554 - accuracy: 0.9441 - val_loss: 0.6703 - val_accuracy: 0.7546\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1085 - accuracy: 0.9609 - val_loss: 0.7498 - val_accuracy: 0.7690\n",
      "24/24 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80       435\n",
      "           1       0.75      0.69      0.72       327\n",
      "\n",
      "    accuracy                           0.77       762\n",
      "   macro avg       0.77      0.76      0.76       762\n",
      "weighted avg       0.77      0.77      0.77       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_gru_bidir(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = layers.Bidirectional(keras.layers.GRU(64))(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_4_GRU_bidir\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_4 = build_gru(10000, 15)\n",
    "# model_4.summary()\n",
    "model_4_history = model_4.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n",
    "# prediction and evaluation\n",
    "y_pred = model_4.predict(x_val).round().squeeze()\n",
    "model_4_res = get_performance(y_val, y_pred, model_name=\"model_4_GRU_bidir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb8038-92eb-4113-80f8-666e3a163916",
   "metadata": {},
   "source": [
    "#### model5: Convolutional NN for text\n",
    "\n",
    "Khi sử dụng [__Convolutional layers__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) cho dữ liệu text (sequences) thì điểm khác biệt chính là số chiều sẽ là 1 (thay vì D = 2 khi sử lý dữ liệu dạng image)\n",
    "\n",
    "Các bước chính trong viêc sử dụng CNN for text data: ([chi tiết tại Understanding Convolutional Neural Networks for Text Classification](https://www.aclweb.org/anthology/W18-5408.pdf))\n",
    "1. Sử dụng `Conv1D()` để filter bằng __ngram detectors__, mỗi 1 filter cụ thể sẽ tạo đặc điểm gần nhất với 1 họ ngrams\n",
    "> an ngram là 1 collection of n-words\n",
    "2. Sử dụng Maxpooling trong suốt quá trình extracts những ngrams có liên quan để phục vụ việc ra quyét định\n",
    "3. Model sẽ phân loại dựa trên các thông tin sau lớp maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c69299-07b5-4a24-8696-37d251f8643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,300,545\n",
      "Trainable params: 1,300,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_cnn_text(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(\n",
    "        x\n",
    "    )  # sử dụng ngram với n = 5 words\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_5 = build_cnn_text(10000, 15)\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed7eca-a12d-4991-8649-79428a7cb882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.5586 - accuracy: 0.7254 - val_loss: 0.4585 - val_accuracy: 0.7913\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.3250 - accuracy: 0.8678 - val_loss: 0.4647 - val_accuracy: 0.7940\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.1881 - accuracy: 0.9355 - val_loss: 0.5576 - val_accuracy: 0.7940\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.1173 - accuracy: 0.9581 - val_loss: 0.6164 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.0837 - accuracy: 0.9699 - val_loss: 0.6956 - val_accuracy: 0.7703\n"
     ]
    }
   ],
   "source": [
    "model_5_history = model_5.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680b975-d09e-421c-80b7-8a385f5bfa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       435\n",
      "           1       0.74      0.72      0.73       327\n",
      "\n",
      "    accuracy                           0.77       762\n",
      "   macro avg       0.77      0.76      0.76       762\n",
      "weighted avg       0.77      0.77      0.77       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_5.predict(x_val).round().squeeze()\n",
    "model_5_res = get_performance(y_val, y_pred, model_name=\"5_Conv1D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6df3c-d3b8-4c15-a3af-1ce5a149e147",
   "metadata": {},
   "source": [
    "#### model6: TensorFlow Hub Pretrained Sentence Encoder\n",
    "\n",
    "the __Universal Sentence Encoder__ embedding a whole sentence-level (thay vì word-level như layer Embedding phía trên), với mỗi sentence được encode thành vector có 512 dimentional\n",
    "\n",
    "> 🔑 Note: An __encoder__ is the name for a model which converts raw data such as text into a numerical representation (feature vector), a __decoder__ converts the numerical representation to a desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea564c0e-4941-48fd-a2e5-284c9938e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d026d29-1db4-494e-911d-26f3017eff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 20s 90ms/step - loss: 0.5124 - accuracy: 0.7719 - val_loss: 0.4251 - val_accuracy: 0.8058\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 12s 56ms/step - loss: 0.4283 - accuracy: 0.8067 - val_loss: 0.4156 - val_accuracy: 0.8071\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 11s 50ms/step - loss: 0.4179 - accuracy: 0.8129 - val_loss: 0.4206 - val_accuracy: 0.8097\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 10s 48ms/step - loss: 0.4103 - accuracy: 0.8161 - val_loss: 0.4189 - val_accuracy: 0.8123\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 10s 47ms/step - loss: 0.4049 - accuracy: 0.8196 - val_loss: 0.4201 - val_accuracy: 0.8045\n",
      "24/24 [==============================] - 4s 155ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       435\n",
      "           1       0.77      0.78      0.77       327\n",
      "\n",
      "    accuracy                           0.80       762\n",
      "   macro avg       0.80      0.80      0.80       762\n",
      "weighted avg       0.80      0.80      0.80       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_USE():\n",
    "    # We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "    sentence_encoder_layer = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "        input_shape=[],  # shape of inputs coming to our model\n",
    "        dtype=tf.string,  # data type of inputs coming to the USE layer\n",
    "        trainable=False,  # keep the pretrained weights (we'll create a feature extractor)\n",
    "        name=\"USE\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = tf.keras.layers.Input(shape=[], dtype=\"string\")\n",
    "    x = sentence_encoder_layer(inputs)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_6_USE\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_6 = build_USE()\n",
    "model_6_history = model_6.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n",
    "# prediction and evaluation\n",
    "y_pred = model_6.predict(x_val).round().squeeze()\n",
    "model_6_res = get_performance(y_val, y_pred, model_name=\"model_6_USE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c462a6-6d96-498d-87b3-545d1d22bc65",
   "metadata": {},
   "source": [
    "#### model7: Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f86342e-b31d-4edc-8ef0-437b91c4a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f2dce-6b12-4a27-8065-447fffabfd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7_Transformers\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " keras_layer_3 (KerasLayer)  (None, 50)                48190600  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                3264      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,193,929\n",
      "Trainable params: 48,193,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 0.5330 - accuracy: 0.7453 - val_loss: 0.4240 - val_accuracy: 0.8228\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 12s 58ms/step - loss: 0.3394 - accuracy: 0.8568 - val_loss: 0.4291 - val_accuracy: 0.8228\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 12s 58ms/step - loss: 0.2187 - accuracy: 0.9164 - val_loss: 0.5073 - val_accuracy: 0.7874\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 12s 58ms/step - loss: 0.1378 - accuracy: 0.9512 - val_loss: 0.6148 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 13s 58ms/step - loss: 0.0953 - accuracy: 0.9667 - val_loss: 0.6857 - val_accuracy: 0.7808\n",
      "24/24 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_performance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# prediction and evaluation\u001b[39;00m\n\u001b[1;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_7\u001b[38;5;241m.\u001b[39mpredict(x_val)\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 24\u001b[0m model_7_res \u001b[38;5;241m=\u001b[39m \u001b[43mget_performance\u001b[49m(y_val, y_pred, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_7_Transformers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_performance' is not defined"
     ]
    }
   ],
   "source": [
    "def build_tfm():\n",
    "    embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "    hub_layer = hub.KerasLayer(\n",
    "        embedding, input_shape=[], dtype=tf.string, trainable=True\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = tf.keras.layers.Input(shape=[], dtype=\"string\")\n",
    "    x = hub_layer(inputs)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_7_Transformers\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "model_7 = build_tfm()\n",
    "model_7_history = model_7.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n",
    "# prediction and evaluation\n",
    "y_pred = model_7.predict(x_val).round().squeeze()\n",
    "model_7_res = get_performance(y_val, y_pred, model_name=\"model_7_Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa97f0-649b-47f6-9456-cf4918dbc780",
   "metadata": {},
   "source": [
    "### Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa761405-6c56-45f6-81d0-a666d23f6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "res = pd.concat(\n",
    "    [\n",
    "        model_0_res,\n",
    "        model_1_res,\n",
    "        model_2_res,\n",
    "        model_3_res,\n",
    "        model_4_res,\n",
    "        model_5_res,\n",
    "        model_6_res,\n",
    "        model_7_res,\n",
    "    ]\n",
    ")\n",
    "fig = px.bar(\n",
    "    res,\n",
    "    y=\"class1\",\n",
    "    x=\"model_name\",\n",
    "    color=\"metric\",\n",
    "    barmode=\"group\",\n",
    "    range_y=(0.6, 1),\n",
    "    text_auto=\".2f\",\n",
    ")\n",
    "fig.update_traces(\n",
    "    textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f20fb1",
   "metadata": {},
   "source": [
    "<img src = \"_images/comparisation_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0903e9d-4568-467e-a3b9-454295b95fd9",
   "metadata": {},
   "source": [
    "### Combining our models (model ensembling/stacking)\n",
    "\n",
    "ref: [__Chapter 6 of the Machine Learning Engineering Book__](http://www.mlebook.com/wiki/doku.php)\n",
    "\n",
    "Sử dụng ensemble để kết hợp nhiều model để make prediction với điều kiện là các models có tính chất __uncorrelated__ với nhau, hay nói cách khác là mỗi model có 1 cách tiếp cận/kiến trúc mạng khác nhau, cách tìm ra patterns khác nhau.\n",
    "\n",
    "Các phương pháp combine output:\n",
    "1. average the probabilities\n",
    "2. Majority vote\n",
    "3. Model stacking: sử dụng output của model này để làm input cho model khác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55475ff-bcd3-4848-8b36-09c6c54be8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b1c44-f915-481b-81c1-3c0b59800a4f",
   "metadata": {},
   "source": [
    "### Saving and loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e3bff-e795-4d05-9f56-da97026f7814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use H5 format\n",
    "model_6.save(\"models/NLP/H5/model_6.h5\")\n",
    "\n",
    "# Load model with custom Hub Layer (required with HDF5 format)\n",
    "# do model 6 sử dụng cấu trúc từ nguồn ngoài nên phải khai báo custom_objects\n",
    "loaded_model_6 = tf.keras.models.load_model(\n",
    "    \"models/NLP/H5/model_6.h5\", custom_objects={\"KerasLayer\": hub.KerasLayer}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87082fe-1569-4c3a-995a-4f6fb638df61",
   "metadata": {},
   "source": [
    "### Finding the most wrong examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e7881-63c2-4842-9f4b-e5efb237d808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 3s 107ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>make sure evacuate past fire doors questions y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>foodscare offersgo nestleindia slips loss magg...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>phiddleface theres choking hazard dont die get</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>ruin life</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>blazing elwoods blazingelwoods bother doug son...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  pred  \\\n",
       "3289  make sure evacuate past fire doors questions y...       0   0.0   \n",
       "4221  foodscare offersgo nestleindia slips loss magg...       1   1.0   \n",
       "4186     phiddleface theres choking hazard dont die get       0   0.0   \n",
       "5873                                          ruin life       0   0.0   \n",
       "706   blazing elwoods blazingelwoods bother doug son...       0   0.0   \n",
       "\n",
       "      pred_prob  \n",
       "3289   0.474197  \n",
       "4221   0.526637  \n",
       "4186   0.062413  \n",
       "5873   0.047390  \n",
       "706    0.036600  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe with validation sentences and best performing model predictions\n",
    "y_pred = model_6.predict(x_val)\n",
    "val_df = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": x_val,\n",
    "        \"target\": y_val,\n",
    "        \"pred\": y_pred.round().squeeze(),\n",
    "        \"pred_prob\": y_pred.squeeze(),\n",
    "    }\n",
    ")\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64c2ad-feaf-4cea-acbe-e21a91337ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>general news uae demolition houses waterways b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.921028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>alaska wolves face catastrophe denali wolves p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>madonnamking rspca site multiple story high ri...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3821</th>\n",
       "      <td>juneau empire first responders turn national n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>government concerned population explosion popu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>could die falling sinkhole still blamed</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>fredolsencruise please take faroeislands itine...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.852597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>steveycheese mapmyrun electrocuted way round m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>plan emergency preparedness families children ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>nikostar lakes ohio thought abject desolation ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  pred  \\\n",
       "2345  general news uae demolition houses waterways b...       0   1.0   \n",
       "1491  alaska wolves face catastrophe denali wolves p...       0   1.0   \n",
       "3991  madonnamking rspca site multiple story high ri...       0   1.0   \n",
       "3821  juneau empire first responders turn national n...       0   1.0   \n",
       "3506  government concerned population explosion popu...       0   1.0   \n",
       "6070            could die falling sinkhole still blamed       0   1.0   \n",
       "4832  fredolsencruise please take faroeislands itine...       0   1.0   \n",
       "3111  steveycheese mapmyrun electrocuted way round m...       0   1.0   \n",
       "3193  plan emergency preparedness families children ...       0   1.0   \n",
       "2525  nikostar lakes ohio thought abject desolation ...       0   1.0   \n",
       "\n",
       "      pred_prob  \n",
       "2345   0.921028  \n",
       "1491   0.913840  \n",
       "3991   0.907247  \n",
       "3821   0.894248  \n",
       "3506   0.891692  \n",
       "6070   0.872665  \n",
       "4832   0.852597  \n",
       "3111   0.829169  \n",
       "3193   0.815358  \n",
       "2525   0.805084  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the wrong predictions and sort by prediction probabilities\n",
    "most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\n",
    "    \"pred_prob\", ascending=False\n",
    ")\n",
    "most_wrong[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
