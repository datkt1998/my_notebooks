{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "**Embedding** là cách ánh xạ mỗi **token/từ/câu từ** không gian rời rạc sang không gian vector liên tục (thường là **vector thực chiều d**, ví dụ 100, 300, 768, 1024). Các vector này được thiết kế/huấn luyện sao cho mang thông tin về ngữ nghĩa và ngữ cảnh của từ, bổ sung phần relationships giữa các tokens và có thể learning trong lúc train thay vì chỉ là những con số static\n",
    "\n",
    "Các từ đồng nghĩa/sát nghĩa thì sau khi chạy qua Embeddings layers/model (ví dụ __word2vec__) sẽ trả ra các vector gần bằng nhau, thể hiện nếu biểu diễn trong không gian thì các từ này sẽ gần nhau.\n",
    "\n",
    "- ***Word Embedding (ví dụ: Word2Vec, GloVe, FastText):***\n",
    "    - Mỗi từ tĩnh (static embedding) có một vector cố định.\n",
    "    - Không phân biệt nghĩa khác nhau của từ trong các ngữ cảnh khác nhau (cùng một từ “bank” – “ngân hàng” hoặc “bờ sông” – thường có 1 vector duy nhất).\n",
    "\n",
    "\n",
    "- ***Contextual Embedding (ví dụ: BERT, GPT, ELMo):***\n",
    "    - Vector biểu diễn từ phụ thuộc vào ngữ cảnh xung quanh.\n",
    "    - Từ “bank” trong câu “I put money in the bank” và “he sat on the bank of the river” sẽ có 2 vector khác nhau, nhờ mô hình Transformer/RNN nắm được ngữ cảnh.\n",
    "\n",
    "- ***Sentence Embedding / Document Embedding:***\n",
    "    - Thay vì embedding cho từng từ, mô hình tạo vector cho câu hoặc đoạn văn. Ví dụ: Sentence-BERT, Universal Sentence Encoder, InferSent, v.v.\n",
    "\n",
    "> Có thể upload các `vector output` kèm `labels` (các words) lên [__TensorFlow Embedding Projector__](http://projector.tensorflow.org/?_gl=1*1jn4yxx*_ga*NTEzMDY1NzcwLjE2ODIxNTU3Mjg.*_ga_W0YLR4190T*MTY4Mzc4MzIwNy41Mi4xLjE2ODM3ODMzMjQuMC4wLjA.) để visualize tính relationships giữa các words\n",
    "\n",
    "---\n",
    "\n",
    "**Có 2 hướng tiếp cận Embeddings:**\n",
    "- ___Create your own embedding___: Để sử dụng được embedding, text cần phải được turn into numbers, sau đó sử dụng `embedding layer` (such as `tf.keras.layers.Embedding`) để learn trong quá trình training\n",
    "- ___Reuse a pre-learned embedding___(**prefer**): Sử dụng các tham số embedding trong các pre-train model để fine-tune lại own model. Các pre-train này học từ một số lượng lớn text ( Ví dụ như all of Wikipedia) nên có khả năng đại diện chính xác mỗi quan hệ giữa các từ \n",
    "    > Sử dụng cách search tương tự như computer vision pretrain model trên [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) để lựa chọn ra một số pre-train model hiệu quả như  [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/),.. \n",
    "\n",
    "---\n",
    "\n",
    "**Có 2 cách thức embedding:**\n",
    "\n",
    "- ___Continuous Bag-of-Words (CBOW)___: Phương pháp này lấy inputs đầu vào là một/nhiều từ context word và cố gắng dự đoán output là target word thông qua một tầng neural đơn giản. Nhờ việc đánh giá output error với target word ở dạng one-hot, mô hình có thể điều chỉnh weight, học được vector biểu diễn cho target word. \n",
    "> Ví dụ ta có một câu tiếng anh như sau : \"I love you\". Ta có Input context word là \"love\" và Output target word là \"you\". Ta biến đổi input context đầu vào dưới dạng one-hot đi qua một tầng hidden layer và thực hiện softmax phân loại để dự đoán ra từ tiếp theo là gì.\n",
    "\n",
    "![](https://images.viblo.asia/1df2b1bc-c823-4b36-be92-46755788506a.png)\n",
    "\n",
    "- ___Skip-gram___: Nếu như __CBOW__ sử dụng input là __context word__ và cố gắng dự đoán từ đầu ra (__target word__) thì ngược lại, mô hình __Skip-gram__ sử dụng input là __target word__ và cố gắng dự đoán ra các từ hàng xóm của nó. Chúng định nghĩa các từ là hàng xóm (__neightbor word__) của nó thông qua tham số __window size__. \n",
    "> Ví dụ nếu bạn có một câu như sau: \"Tôi thích ăn cua hoàng đế\". Và input target word ban đầu là từ cua. Với kích thước window size = 2, ta sẽ có các neighbor word (thích, ăn, hoàng, đế ). Và chúng ta sẽ có 4 cặp input-output như sau: (cua, thích ), (cua, hoàng ), (cua, đế ), (cua, ăn ). Các __neightbor word__ được coi như nhau trong quá trình training.\n",
    "\n",
    "![](https://images.viblo.asia/d6dd1927-085e-45a7-89d0-0bd3ea152827.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Ứng dụng**\n",
    "- Phân loại văn bản (sentiment, topic): Dùng vector embedding (của từ hoặc câu) làm đầu vào mô hình.\n",
    "- Dịch máy, tóm tắt, chatbot: Hầu như mọi mô hình sequence-to-sequence (seq2seq) đều yêu cầu embedding tốt.\n",
    "- Truy xuất thông tin, tìm kiếm ngữ nghĩa: Tính tương đồng giữa câu truy vấn và câu trong văn bản thông qua cosine similarity, v.v.\n",
    "- Clustering, topic modeling: Nếu có embedding tốt, ta có thể gom nhóm các văn bản (hoặc từ) về chủ đề.\n",
    "- Recommendation, Phân tích đồ thị, …: Embedding được mở rộng không chỉ cho từ mà còn cho người dùng, item, node trong đồ thị (Graph Embedding).\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practice cho Embedding**\n",
    "\n",
    "***Sử dụng mô hình pre-trained (fine-tuned khi cần):***\n",
    "- Mô hình Transformer hiện đại (BERT, RoBERTa, GPT, XLM-R, PhoBERT, v.v.) thường cung cấp embedding có chất lượng rất tốt.\n",
    "- Nếu dữ liệu bạn chuyên biệt (y tế, tài chính) hoặc tiếng Việt, nên dùng hoặc fine-tune mô hình pre-trained phù hợp (chẳng hạn PhoBERT cho tiếng Việt).\n",
    "\n",
    "***Chọn loại embedding:***\n",
    "- Tĩnh (Word2Vec, GloVe): Đơn giản, dễ dùng, ít tài nguyên; nhưng không xử lý được từ đồng âm đa nghĩa theo ngữ cảnh.\n",
    "- Ngữ cảnh (Contextual, ví dụ BERT): Chính xác hơn, nắm bắt được meaning tùy ngữ cảnh, nhưng nặng về tính toán, phức tạp.\n",
    "\n",
    "***Khai thác “last hidden state” hay “pooler output” của BERT:***\n",
    "Khi muốn lấy embedding từ BERT, ta có thể lấy vector ẩn tại layer cuối, hoặc Mean Pooling hay [CLS] token (tuỳ mô hình). Chẳng hạn:\n",
    "- BERT gốc dùng vector tại [CLS] như “sentence embedding”, nhưng hiệu quả có thể kém hơn so với Mean Pooling.\n",
    "- Sentence-BERT đã tinh chỉnh cách lấy embedding câu.\n",
    "> Khuyến nghị: Thử Mean Pooling toàn bộ token (hoặc fine-tune mô hình “sentence embedding” chuyên dụng) để có câu embedding tốt hơn.\n",
    "\n",
    "***Fine-tune nếu có đủ dữ liệu:***\n",
    "- Nếu bài toán của bạn có sẵn dữ liệu labeled và quan trọng về chất lượng, hãy fine-tune mô hình pre-trained.\n",
    "- Fine-tune có thể cải thiện đáng kể độ chính xác so với chỉ dùng embedding tĩnh hoặc dùng mô hình pre-trained “thô”.\n",
    "\n",
    "***Quản lý OOV (Out-of-Vocabulary):***\n",
    "- Với embedding tĩnh (Word2Vec, GloVe), từ mới không có trong từ điển => mô hình không có vector. Cần kỹ thuật fallback (như sử dụng vector trung bình).\n",
    "- Subword embedding (BERT) giảm thiểu vấn đề OOV, vì từ mới được chia nhỏ thành subword.\n",
    "\n",
    "***Kiểm tra trực quan (phân tích PCA, t-SNE)***\n",
    "- Để đảm bảo embedding phản ánh tốt, bạn có thể trực quan hoá vector trong không gian 2D/3D.\n",
    "> Ví dụ, các từ/câu cùng chủ đề sẽ gần nhau, các từ/câu khác nghĩa sẽ xa nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding by keras\n",
    "Sử dụng [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) với một số params như sau:\n",
    "- `input_dim` - The size of the vocabulary (e.g. len(text_vectorizer.get_vocabulary()).\n",
    "- `output_dim` - The size of the output embedding vector, for example, a value of 100 outputs a feature vector of size 100 for each word.\n",
    "- `embeddings_initializer` - How to initialize the embeddings matrix, default is `\"uniform\"` which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n",
    "- `input_length` - Length of sequences being passed to embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.02957509,  0.03379121,  0.04258261, ..., -0.04872347,\n",
       "         -0.00656006,  0.01681853],\n",
       "        [-0.02690672, -0.02275765, -0.01154822, ...,  0.02936261,\n",
       "          0.04035657,  0.01228539],\n",
       "        [ 0.0207009 ,  0.00322819, -0.02702802, ..., -0.01606815,\n",
       "          0.00701294,  0.04634335],\n",
       "        ...,\n",
       "        [ 0.0172099 , -0.03784468,  0.03696242, ..., -0.03041265,\n",
       "         -0.03056842,  0.0070639 ],\n",
       "        [ 0.0172099 , -0.03784468,  0.03696242, ..., -0.03041265,\n",
       "         -0.03056842,  0.0070639 ],\n",
       "        [ 0.0172099 , -0.03784468,  0.03696242, ..., -0.03041265,\n",
       "         -0.03056842,  0.0070639 ]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# mỗi 1 token sẽ được embedding thành 1 vector có chiều dài là output_dim\n",
    "# mỗi 1 sequence được embedding thành 1 tensor có shape là (input_length, output_dim)\n",
    "# số lượng weights params là (output_dim * input_dim)\n",
    "\n",
    "embedding = layers.Embedding(\n",
    "    input_dim=max_vocab_length,  # set input shape\n",
    "    output_dim=128,  # set size of embedding vector\n",
    "    embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "    input_length=max_sequence_length,  # how long is each input\n",
    "    name=\"embedding_1\",\n",
    ")\n",
    "\n",
    "embedding(tvect([\"water main break disrupts trolley service\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Các giá trị embedding này được khởi tạo bàn đầu ngẫu nhiên, trong quá trình train thì sẽ được learn để tạo ra relationship phù hợp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the embedding on Embedding Projector**\n",
    "\n",
    "To visualize on [__Embedding Projector__](http://projector.tensorflow.org/_), cần chuẩn bị 2 file bao gồm:\n",
    "- The embedding vector (embedding weights)\n",
    "- the meta data of vector (vocabulary)\n",
    "\n",
    "Then:\n",
    "1. Go to http://projector.tensorflow.org/\n",
    "2. Click on \"Load data\"\n",
    "3. Upload the two files you downloaded (embedding_vectors.tsv and embedding_metadata.tsv)\n",
    "4. Explore\n",
    "5. Optional: You can share the data you've created by clicking \"Publish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding vector and metadata\n",
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# get vocab and embedding_weight from model\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "words_in_vocab = model_1.get_layer(\"text_vectorization_1\").get_vocabulary()\n",
    "\n",
    "\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "    if num == 0:\n",
    "        continue  # skip padding token\n",
    "    vec = embed_weights[num]\n",
    "    out_m.write(word + \"\\n\")  # write words to file\n",
    "    out_v.write(\n",
    "        \"\\t\".join([str(x) for x in vec]) + \"\\n\"\n",
    "    )  # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
