{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing in NLP\n",
    "\n",
    "M·ª•c ti√™u c·ªßa to√†n b·ªô qu√° tr√¨nh n√†y l√† gi√∫p m√¥ h√¨nh h·ªçc m√°y (Machine Learning) ho·∫∑c m√¥ h√¨nh h·ªçc s√¢u (Deep Learning) nh·∫≠n ƒë∆∞·ª£c ƒë·∫ßu v√†o ‚Äús·∫°ch‚Äù v√† ‚Äúchu·∫©n ho√°‚Äù, t·ª´ ƒë√≥ ƒë·∫°t hi·ªáu qu·∫£ cao h∆°n trong ph√¢n t√≠ch v√† d·ª± ƒëo√°n.\n",
    "\n",
    "**NLP Python packages:**\n",
    "\n",
    "|NLP Library|\tDescription|\n",
    "|---|---|\n",
    "|NLTK\t|This is one of the most usable and mother of all NLP libraries.|\n",
    "|spaCy\t|This is a completely optimized and highly accurate library widely used in deep learning|\n",
    "|Stanford CoreNLP| Python\tFor client-server-based architecture, this is a good library in NLTK. This is written in JAVA, but it provides modularity to use it in Python.|\n",
    "|TextBlob\t|This is an NLP library which works in Pyhton2 and python3. This is used for processing textual data and provide mainly all type of operation in the form of API.|\n",
    "|Gensim\t|Genism is a robust open source NLP library support in Python. This library is highly efficient and scalable.|\n",
    "|Pattern\t|It is a light-weighted NLP module. This is generally used in Web-mining, crawling or such type of spidering task|\n",
    "|Polyglot\t|For massive multilingual applications, Polyglot is best suitable NLP library. Feature extraction in the way on Identity and Entity.|\n",
    "|PyNLPl\t|PyNLPI also was known as ‚ÄòPineapple‚Äô and supports Python. It provides a parser for many data formats like FoLiA/Giza/Moses/ARPA/Timbl/CQL.|\n",
    "|Vocabulary\t|This library is best to get Semantic type information from the given text.|\n",
    "|pyvi\t| Python Vietnamese Core NLP Toolkit |\n",
    "|underthesea| Underthesea - Vietnamese NLP Toolkit |\n",
    "\n",
    "\n",
    "**The process of NLP Preprocessing is as follows:**\n",
    "1. General cleaning\n",
    "    - Case normalization\n",
    "    - Normalize grammatical structure\n",
    "    - Regular expression handling\n",
    "2. Removing noise from the dataset\n",
    "    - Removing special characters/patterns\n",
    "    - Removing punctuations\n",
    "    - Removing stop words\n",
    "    - Remove unnecessary components: table, image, etc.\n",
    "3. Normalizing text to right-format for the ML Algorithm\n",
    "    - Tagging: Part-of-speech tagging, named-entity recognition\n",
    "    - Stemming / Lemmatization\n",
    "4. Tokenization\n",
    "5. Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Th·ªß t∆∞·ªõng Abe c√∫i ƒë·∫ßu xin l·ªói v√¨ h√†nh ƒë·ªông phi...</td>\n",
       "      <td>binhluan.biz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Th·ªß t∆∞·ªõng Nh·∫≠t c√∫i ƒë·∫ßu xin l·ªói v√¨ tinh th·∫ßn ph...</td>\n",
       "      <td>www.ipick.vn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cho√°ng! C∆° tr∆∞·ªüng ƒëeo khƒÉn qu√†ng qu·∫©y banh n√≥c...</td>\n",
       "      <td>tintucqpvn.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          domain  label\n",
       "0  Th·ªß t∆∞·ªõng Abe c√∫i ƒë·∫ßu xin l·ªói v√¨ h√†nh ƒë·ªông phi...    binhluan.biz      1\n",
       "1  Th·ªß t∆∞·ªõng Nh·∫≠t c√∫i ƒë·∫ßu xin l·ªói v√¨ tinh th·∫ßn ph...    www.ipick.vn      1\n",
       "2  Cho√°ng! C∆° tr∆∞·ªüng ƒëeo khƒÉn qu√†ng qu·∫©y banh n√≥c...  tintucqpvn.net      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# link data: https://github.com/WhySchools/VFND-vietnamese-fake-news-datasets/blob/master/CSV/vn_news_223_tdlfr.csv\n",
    "\n",
    "raw = pd.read_csv(\n",
    "    r\"contents\\theory\\aiml_algorithms\\dl_nlp\\data\\vn_news_223_tdlfr.csv\"\n",
    ")\n",
    "print(raw.shape)\n",
    "raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Cleaning\n",
    "\n",
    "1. **Case Normalization**: Th∆∞·ªùng chuy·ªÉn t·∫•t c·∫£ v·ªÅ ch·ªØ th∆∞·ªùng (lowercase) ƒë·ªÉ gi·∫£m ƒë·ªô ph·ª©c t·∫°p khi so s√°nh t·ª´. However, do remember that **lowercasing can change the meaning of some text** e.g \"US\" vs \"us\".\n",
    "\n",
    "2. **S·ª≠a l·ªói ch√≠nh t·∫£ (n·∫øu c·∫ßn)**: Trong m·ªôt s·ªë b√†i to√°n ph√¢n t√≠ch ng√¥n ng·ªØ, vi·ªác ch√≠nh t·∫£ ch√≠nh x√°c c√≥ √Ω nghƒ©a quan tr·ªçng.\n",
    "\n",
    "3. **X·ª≠ l√Ω c√°c t·ª´ vi·∫øt t·∫Øt, t·ª´ l√≥ng**: V√≠ d·ª•: ‚Äúko‚Äù -> ‚Äúkh√¥ng‚Äù, ‚Äúk‚Äù -> ‚Äúkh√¥ng‚Äù (trong ti·∫øng Vi·ªát), ho·∫∑c ‚Äúu‚Äù -> ‚Äúyou‚Äù (ti·∫øng Anh). --> Vi·ªác nh·∫•t qu√°n ho√° c√°c bi·∫øn th·ªÉ t·ª´ v·ª±ng gi√∫p m√¥ h√¨nh hi·ªÉu r√µ h∆°n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise\n",
    "\n",
    "Text data often contains noise such as punctuation, special characters, and irrelevant symbols. Preprocessing helps remove these elements, making the text cleaner and easier to analyze.\n",
    "\n",
    "1. **Lo·∫°i b·ªè k√Ω t·ª± ho·∫∑c bi·ªÉu t∆∞·ª£ng kh√¥ng mong mu·ªën**: k√Ω t·ª± ƒë·∫∑c bi·ªát, emoji, ƒë∆∞·ªùng d·∫´n (URL), email, k√Ω t·ª± HTML, th·∫ª HTML, v.v. --> gi·∫£m b·ªõt nh·ªØng th√†nh ph·∫ßn kh√¥ng c√≥ gi√° tr·ªã ng·ªØ nghƒ©a ho·∫∑c g√¢y nhi·ªÖu.\n",
    "\n",
    "2. **Lo·∫°i b·ªè kho·∫£ng tr·∫Øng, xu·ªëng d√≤ng th·ª´a, or punctuations**: like `. , ! $( ) * % @` gi√∫p d·ªØ li·ªáu g·ªçn g√†ng, nh·∫•t qu√°n.\n",
    "\n",
    "3. **Lo·∫°i b·ªè ho·∫∑c thay th·∫ø token v√¥ nghƒ©a (Stopwords, t·ª´ v√¥ nghƒ©a trong ng·ªØ c·∫£nh)**: Stopwords (nh∆∞ \"v√†\", \"ho·∫∑c\", \"c·ªßa\" trong ti·∫øng Vi·ªát; \"the\", \"is\", \"at\" trong ti·∫øng Anh, v.v.) th∆∞·ªùng √≠t mang th√¥ng tin ng·ªØ nghƒ©a v√† c√≥ th·ªÉ g√¢y nhi·ªÖu cho m√¥ h√¨nh. T√πy b√†i to√°n m√† quy·∫øt ƒë·ªãnh gi·ªØ hay b·ªè, v√¨ ƒë√¥i khi stopwords c≈©ng quan tr·ªçng trong m·ªôt s·ªë ng·ªØ c·∫£nh.\n",
    "\n",
    "4. **Lo·∫°i b·ªè nh·ªØng ph·∫ßn t·ª≠ kh√¥ng li√™n quan**: V√≠ d·ª•: trong c√°c ƒëo·∫°n vƒÉn b·∫£n c√≥ ch√®n c√°c code snippet, b·∫£ng bi·ªÉu, metadata‚Ä¶ kh√¥ng c·∫ßn thi·∫øt cho ph√¢n t√≠ch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing\n",
    "\n",
    "Different forms of words (e.g., ‚Äúrun,‚Äù ‚Äúrunning,‚Äù ‚Äúran‚Äù) can convey the same meaning but appear in different forms. Preprocessing techniques like stemming and lemmatization help standardize these variations.\n",
    "\n",
    "1. **Tagging**\n",
    "    - **Part-of-speech (POS)**: \n",
    "    - **Named-entity recognition (NER)**:\n",
    "\n",
    "2. **Stemming / Lemmatization (Gi·∫£m bi·∫øn th·ªÉ t·ª´ v·ª±ng - useful for English)**:\n",
    "    - **Stemming**: c·∫Øt b·ªè ph·∫ßn ‚Äúƒëu√¥i‚Äù c·ªßa t·ª´ ƒë·ªÉ ƒë∆∞a v·ªÅ ‚Äúg·ªëc‚Äù (c√≥ th·ªÉ kh√¥ng ph·∫£i l√† t·ª´ ƒë√∫ng trong t·ª´ ƒëi·ªÉn).\n",
    "        ```text\n",
    "        connecting  -->  connect\n",
    "        connected  -->  connect\n",
    "        connectivity  -->  connect\n",
    "        connect  -->  connect\n",
    "        connects  -->  connect\n",
    "        ```\n",
    "    - **Lemmatization**: ƒë∆∞a t·ª´ v·ªÅ d·∫°ng ‚Äúg·ªëc t·ª´ ƒëi·ªÉn‚Äù (ch√≠nh t·∫Øc) d·ª±a v√†o t·ª´ lo·∫°i, ng·ªØ c·∫£nh. **Lemmatization** v·ªÅ c∆° b·∫£n l√† gi·ªëng v·ªõi **stemming** khi n√≥ lo·∫°i b·ªè ph·∫ßn ƒëu√¥i c·ªßa t·ª´ ƒë·ªÉ thu ƒë∆∞·ª£c g·ªëc t·ª´, tuy nhi√™n c√°c g·ªëc t·ª´ ·ªü ƒë√¢y ƒë·ªÅu th·ª±c s·ª± t·ªën t·∫°i ch·ª© kh√¥ng nh∆∞ **stemming** (nh∆∞ v√≠ d·ª• tr√™n th√¨ t·ª´ `moved` sau khi lemmatize s·∫Ω thu ƒë∆∞·ª£c `move`). Trong th∆∞ vi·ªán NLTK s·∫Ω s·ª≠ d·ª•ng t·ª´ ƒëi·ªÉn **Wordnet** ƒë·ªÉ map theo c√°c quy t·∫Øc (theo t√≠nh ch·∫•t c·ªßa t·ª´, t·ª´ l√† danh t·ª´, ƒë·ªông t·ª´, tr·∫°ng t·ª´ hay t√≠nh t·ª´). S·ª≠ d·ª•ng part-of-speech tagging (nltk.pos_tag) ƒë·ªÉ thu ƒë∆∞·ª£c c√°c t√≠nh ch·∫•t c·ªßa t·ª´.\n",
    "    \n",
    "    -> Hai k·ªπ thu·∫≠t n√†y gi√∫p gi·∫£m s·ª± tr√πng l·∫∑p khi c√πng m·ªôt t·ª´ xu·∫•t hi·ªán ·ªü c√°c d·∫°ng bi·∫øn th·ªÉ kh√°c nhau.\n",
    "\n",
    "3. **X·ª≠ l√Ω nh√£n (n·∫øu l√† b√†i to√°n gi√°m s√°t)**:\n",
    "    - Ki·ªÉm tra v√† chu·∫©n ho√° d·ªØ li·ªáu nh√£n (label). V√≠ d·ª•: chuy·ªÉn t·ª´ ‚Äúpositive‚Äù / ‚Äúnegative‚Äù / ‚Äúneutral‚Äù sang 0 / 1 / 2 ho·∫∑c t∆∞∆°ng t·ª±."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Th·ªß t∆∞·ªõng Abe c√∫i ƒë·∫ßu xin l·ªói v√¨ h√†nh ƒë·ªông phi...</td>\n",
       "      <td>th·ªß_t∆∞·ªõng Abe c√∫i ƒë·∫ßu xin_l·ªói v√¨ h√†nh_ƒë·ªông phi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Th·ªß t∆∞·ªõng Nh·∫≠t c√∫i ƒë·∫ßu xin l·ªói v√¨ tinh th·∫ßn ph...</td>\n",
       "      <td>th·ªß_t∆∞·ªõng Nh·∫≠t c√∫i ƒë·∫ßu xin_l·ªói v√¨ tinh_th·∫ßn ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cho√°ng! C∆° tr∆∞·ªüng ƒëeo khƒÉn qu√†ng qu·∫©y banh n√≥c...</td>\n",
       "      <td>cho√°ng ! c∆°_tr∆∞·ªüng ƒëeo khƒÉn_qu√†ng qu·∫©y banh n√≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ch∆∞a bao gi·ªù nh·∫°c Kpop l·∫°i d·ªÖ h√°t ƒë·∫øn th·∫ø!!!\\n...</td>\n",
       "      <td>ch∆∞a bao_gi·ªù nh·∫°c Kpop l·∫°i d·ªÖ h√°t ƒë·∫øn th·∫ø ! ! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ƒê·∫°i h·ªçc Hutech s·∫Ω √°p d·ª•ng c·∫£i c√°ch \"Ti·∫øq Vi·ªát\"...</td>\n",
       "      <td>ƒë·∫°i_h·ªçc Hutech s·∫Ω √°p_d·ª•ng c·∫£i_c√°ch \" Ti·∫øq Vi·ªát...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>‚ÄúSi√™u m√°y bay‚Äù A350 s·∫Ω ch·ªü CƒêV Vi·ªát Nam ƒëi Mal...</td>\n",
       "      <td>‚Äú si√™u m√°y_bay ‚Äù A350 s·∫Ω ch·ªü cƒëv Vi·ªát_Nam ƒëi M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Th∆∞·ªüng 20.000 USD cho ƒë·ªôi tuy·ªÉn c·ªù vua Vi·ªát Na...</td>\n",
       "      <td>th∆∞·ªüng 20.000 usd cho ƒë·ªôi_tuy·ªÉn c·ªù_vua Vi·ªát_Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Tr∆∞·ªùng S∆°n gi√†nh HCV t·∫°i gi·∫£i c·ªù vua ƒë·ªìng ƒë·ªôi ...</td>\n",
       "      <td>Tr∆∞·ªùng_S∆°n gi√†nh hcv t·∫°i gi·∫£i c·ªù_vua ƒë·ªìng_ƒë·ªôi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Chuy·ªán v·ªÅ ch√†ng sinh vi√™n Lu·∫≠t - Ki·ªán t∆∞·ªõng L√™...</td>\n",
       "      <td>chuy·ªán v·ªÅ ch√†ng sinh_vi√™n Lu·∫≠t - ki·ªán_t∆∞·ªõng L√™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Ti·ªÅn ƒë·∫°o Malaysia: ‚ÄúT√¥i ƒë√£ c√≥ c√°ch v∆∞·ª£t qua h√†...</td>\n",
       "      <td>ti·ªÅn_ƒë·∫°o Malaysia : ‚Äú t√¥i ƒë√£ c√≥ c√°ch v∆∞·ª£t qua ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Th·ªß t∆∞·ªõng Abe c√∫i ƒë·∫ßu xin l·ªói v√¨ h√†nh ƒë·ªông phi...   \n",
       "1    Th·ªß t∆∞·ªõng Nh·∫≠t c√∫i ƒë·∫ßu xin l·ªói v√¨ tinh th·∫ßn ph...   \n",
       "2    Cho√°ng! C∆° tr∆∞·ªüng ƒëeo khƒÉn qu√†ng qu·∫©y banh n√≥c...   \n",
       "3    Ch∆∞a bao gi·ªù nh·∫°c Kpop l·∫°i d·ªÖ h√°t ƒë·∫øn th·∫ø!!!\\n...   \n",
       "4    ƒê·∫°i h·ªçc Hutech s·∫Ω √°p d·ª•ng c·∫£i c√°ch \"Ti·∫øq Vi·ªát\"...   \n",
       "..                                                 ...   \n",
       "218  ‚ÄúSi√™u m√°y bay‚Äù A350 s·∫Ω ch·ªü CƒêV Vi·ªát Nam ƒëi Mal...   \n",
       "219  Th∆∞·ªüng 20.000 USD cho ƒë·ªôi tuy·ªÉn c·ªù vua Vi·ªát Na...   \n",
       "220  Tr∆∞·ªùng S∆°n gi√†nh HCV t·∫°i gi·∫£i c·ªù vua ƒë·ªìng ƒë·ªôi ...   \n",
       "221  Chuy·ªán v·ªÅ ch√†ng sinh vi√™n Lu·∫≠t - Ki·ªán t∆∞·ªõng L√™...   \n",
       "222  Ti·ªÅn ƒë·∫°o Malaysia: ‚ÄúT√¥i ƒë√£ c√≥ c√°ch v∆∞·ª£t qua h√†...   \n",
       "\n",
       "                                        processed_text  \n",
       "0    th·ªß_t∆∞·ªõng Abe c√∫i ƒë·∫ßu xin_l·ªói v√¨ h√†nh_ƒë·ªông phi...  \n",
       "1    th·ªß_t∆∞·ªõng Nh·∫≠t c√∫i ƒë·∫ßu xin_l·ªói v√¨ tinh_th·∫ßn ph...  \n",
       "2    cho√°ng ! c∆°_tr∆∞·ªüng ƒëeo khƒÉn_qu√†ng qu·∫©y banh n√≥...  \n",
       "3    ch∆∞a bao_gi·ªù nh·∫°c Kpop l·∫°i d·ªÖ h√°t ƒë·∫øn th·∫ø ! ! ...  \n",
       "4    ƒë·∫°i_h·ªçc Hutech s·∫Ω √°p_d·ª•ng c·∫£i_c√°ch \" Ti·∫øq Vi·ªát...  \n",
       "..                                                 ...  \n",
       "218  ‚Äú si√™u m√°y_bay ‚Äù A350 s·∫Ω ch·ªü cƒëv Vi·ªát_Nam ƒëi M...  \n",
       "219  th∆∞·ªüng 20.000 usd cho ƒë·ªôi_tuy·ªÉn c·ªù_vua Vi·ªát_Na...  \n",
       "220  Tr∆∞·ªùng_S∆°n gi√†nh hcv t·∫°i gi·∫£i c·ªù_vua ƒë·ªìng_ƒë·ªôi ...  \n",
       "221  chuy·ªán v·ªÅ ch√†ng sinh_vi√™n Lu·∫≠t - ki·ªán_t∆∞·ªõng L√™...  \n",
       "222  ti·ªÅn_ƒë·∫°o Malaysia : ‚Äú t√¥i ƒë√£ c√≥ c√°ch v∆∞·ª£t qua ...  \n",
       "\n",
       "[223 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "import string\n",
    "\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "# stop words: https://github.com/stopwords/vietnamese-stopwords/blob/master/vietnamese-stopwords-dash.txt\n",
    "stop_words = (\n",
    "    pd.read_csv(\n",
    "        r\"contents\\theory\\aiml_algorithms\\dl_nlp\\data\\vietnamese-stopwords-dash.txt\",\n",
    "        header=None,\n",
    "    )\n",
    "    .iloc[:, 0]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    # replace parttern \" ko \" by \" kh√¥ng \"\n",
    "    processed_text = re.sub(r\"\\bko\\b\", \"kh√¥ng\", text)\n",
    "\n",
    "    tokens = ViTokenizer.tokenize(text)\n",
    "    pos_tags = ViPosTagger.postagging(tokens)\n",
    "    processed_text = []\n",
    "    for token, pos in zip(pos_tags[0], pos_tags[1]):\n",
    "        # if token not in stop_words:\n",
    "        if pos.startswith(\"Np\"):\n",
    "            processed_text.append(token.title())\n",
    "        else:\n",
    "            processed_text.append(token.lower())\n",
    "\n",
    "    return \" \".join(processed_text)\n",
    "\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "raw[\"processed_text\"] = raw[\"text\"].apply(process_text)\n",
    "raw[[\"text\", \"processed_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n",
    "\n",
    "**Tokenization**: Tokenization (quy tr√¨nh t√°ch t·ª´, chia nh·ªè vƒÉn b·∫£n) l√† b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω (preprocessing) v√¥ c√πng quan tr·ªçng trong x·ª≠ l√Ω vƒÉn b·∫£n. M·ª•c ti√™u c·ªßa tokenization l√† chuy·ªÉn vƒÉn b·∫£n g·ªëc (chu·ªói k√Ω t·ª±) th√†nh danh s√°ch c√°c token (nh·ªØng ƒë∆°n v·ªã c√≥ √Ω nghƒ©a).\n",
    "\n",
    "- T√°ch c√¢u th√†nh c√°c ƒë∆°n v·ªã t·ª´ ho·∫∑c subword.\n",
    "- Trong ti·∫øng Anh th∆∞·ªùng d·ªÖ d√†ng h∆°n (t√°ch theo d·∫•u c√°ch v√† k√Ω t·ª± ƒë·∫∑c bi·ªát), c√≤n ti·∫øng Vi·ªát c·∫ßn s·ª≠ d·ª•ng m√¥ h√¨nh ho·∫∑c th∆∞ vi·ªán t√°ch t·ª´ chuy√™n d·ª•ng (nh∆∞ VnCoreNLP, PyVi, v.v.).\n",
    "\n",
    "√Ånh x·∫° character/word/subword sang gi√° tr·ªã s·ªë numberical value. C√≥ 3 level c·ªßa tokenization\n",
    "- _Word-level tokenization_: M·ªói t·ª´ s·∫Ω ƒë·∫°i di·ªán b·ªüi 1 numerical value. T√°ch theo kho·∫£ng tr·∫Øng ho·∫∑c d·ª±a tr√™n th∆∞ vi·ªán tokenizer (ti·∫øng Vi·ªát: VnCoreNLP, PyVi, RDRSegmenter, ‚Ä¶). V√≠ d·ª•: \"I love yout\" ---> [0,1,2]\n",
    "- _Character-level tokenization_: M·ªói character (ch·ªØ c√°i, d·∫•u c√¢u) s·∫Ω ƒë·∫°i di·ªán cho 1 token. H·ªØu √≠ch trong m·ªôt s·ªë b√†i to√°n (ƒë·∫∑c bi·ªát v·ªõi c√°c m√¥ h√¨nh ng√¥n ng·ªØ c√≥ √Ω ƒë·ªãnh x·ª≠ l√Ω ƒë√°nh v·∫ßn, ho·∫∑c khi d·ªØ li·ªáu c√≥ nhi·ªÅu t·ª´ m·ªõi).\n",
    "- _Sub-word tokenization_: break t·ª´ng t·ª´ th√†nh c√°c ph·∫ßn v√† tokenization n√≥, khi ƒë√≥ m·ªói word c√≥ th·ªÉ th√†nh nhi·ªÅu tokens. K·∫øt h·ª£p ∆∞u ƒëi·ªÉm gi·ªØa word-level v√† character-level, ƒë∆∞·ª£c s·ª≠ d·ª•ng trong BERT, GPT, RoBERTa, PhoBERT, v.v.\n",
    "\n",
    "> Tu·ª≥ thu·ªôc v√†o problem m√† n√™n ch·ªçn level tokenization cho ph√π h·ª£p, ho·∫∑c c√≥ th·ªÉ th·ª≠ c√°c level v√† ki·ªÉm tra performance, ho·∫∑c c√≥ th·ªÉ s·ª≠ d·ª•ng `tf.keras.layers.concatenate` ƒë·ªÉ combine/stacking ch√∫ng l·∫°i v·ªõi nhau.\n",
    "---\n",
    "**T·∫°i sao tokenization quan tr·ªçng?**\n",
    "- C√°c m√¥ h√¨nh NLP c·ªï ƒëi·ªÉn (Bag-of-Words, TF-IDF, v.v.) hay hi·ªán ƒë·∫°i (Deep Learning) ƒë·ªÅu l√†m vi·ªác tr√™n c√°c ƒë∆°n v·ªã r·ªùi r·∫°c (token).\n",
    "- Tokenization quy·∫øt ƒë·ªãnh c√°ch m√¥ h√¨nh nh·∫≠n th·ª©c vƒÉn b·∫£n: sai s√≥t ho·∫∑c thi·∫øu h·ª£p l√Ω trong tokenization ·∫£nh h∆∞·ªüng ƒë√°ng k·ªÉ ƒë·∫øn ch·∫•t l∆∞·ª£ng m√¥ h√¨nh.\n",
    "- V·ªõi c√°c m√¥ h√¨nh ng√¥n ng·ªØ hi·ªán ƒë·∫°i (**Transformer** nh∆∞ `BERT`, `GPT`, `RoBERTa`, v.v.), v·∫´n c·∫ßn **tokenization**, th∆∞·ªùng l√† subword tokenization (v√≠ d·ª• `BPE`, `SentencePiece`). L√Ω do: m√¥ h√¨nh c·∫ßn chia vƒÉn b·∫£n th√†nh c√°c ‚Äúm√£‚Äù (code) ƒë∆∞·ª£c h·ªçc s·∫µn trong t·ª´ v·ª±ng (vocabulary) ƒë·ªÉ √°nh x·∫° m·ªói token sang vector **embedding** ph√π h·ª£p.\n",
    "\n",
    "---\n",
    "**B·∫£n ch·∫•t h√†nh ƒë·ªông c·ªßa Tokenization v√† Embedding:**\n",
    "\n",
    "- **Tokenization**: Chuy·ªÉn ƒë·ªïi t·ª´ vƒÉn b·∫£n th√†nh danh s√°ch c√°c token (d·∫°ng ch·ªØ) (word, subword, character). Sau ƒë√≥ s·∫Ω chuy·ªÉn sang d·∫°ng s·ªë (index) th√¥ng qua t·ª´ ƒëi·ªÉn (vocabulary).\n",
    "- **Embedding**: V·ªõi m·ªói 1 token (d·∫°ng s·ªë - index) th√¨ token chuy·ªÉn ƒë·ªïi t·ª´ d·∫°ng s·ªë (index) sang **vector** (nhi·ªÅu chi·ªÅu) s·ªë th·ª±c (embedding). M·ªói token s·∫Ω ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi m·ªôt vector s·ªë th·ª±c c√≥ s·ªë chi·ªÅu l√† `d` x√°c ƒë·ªãnh tr∆∞·ªõc (v√≠ d·ª•: 100, 200, 300 chi·ªÅu). Th·ªÉ hi·ªán s·ª± t∆∞∆°ng quan gi·ªØa c√°c token trong kh√¥ng gian vector.\n",
    "- Khi m√¥ h√¨nh nh·∫≠n input (list c√°c token ID), n√≥ s·∫Ω tra c·ª©u (lookup) t·ª´ng token ID trong h√†ng t∆∞∆°ng ·ª©ng c·ªßa ma tr·∫≠n embedding `ùëä` ƒë·ªÉ l·∫•y ƒë∆∞·ª£c vector (d chi·ªÅu) t∆∞∆°ng ·ª©ng v·ªõi token ƒë√≥.\n",
    "- K·∫øt qu·∫£: Thay v√¨ list s·ªë ID, m√¥ h√¨nh c√≥ m·ªôt chu·ªói vector (m·ªôt cho m·ªói token), ph·∫£n √°nh th√¥ng tin ng·ªØ nghƒ©a v√† ng·ªØ c·∫£nh (v·ªõi c√°c m√¥ h√¨nh hi·ªán ƒë·∫°i) c·ªßa nh·ªØng token ƒë√≥. Sau ƒë√≥, m√¥ h√¨nh s·∫Ω s·ª≠ d·ª•ng vector n√†y ƒë·ªÉ th·ª±c hi·ªán c√°c ph√©p to√°n (t√≠ch v√¥ h∆∞·ªõng, pooling, attention, v.v.) ƒë·ªÉ h·ªçc c·∫•u tr√∫c ng·ªØ nghƒ©a c·ªßa vƒÉn b·∫£n.\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practice cho Tokenization**\n",
    "\n",
    "***1. Ch·ªçn ph∆∞∆°ng ph√°p tokenization ph√π h·ª£p:***\n",
    "\n",
    "- V·ªõi m√¥ h√¨nh Transformer hi·ªán ƒë·∫°i, th∆∞·ªùng d√πng subword tokenization (BPE, SentencePiece) v√¨ kh·∫£ nƒÉng x·ª≠ l√Ω t·ªët t·ª´ m·ªõi, t·ª´ sai ch√≠nh t·∫£, t·ª´ hi·∫øm, v.v.\n",
    "- N·∫øu l√†m truy·ªÅn th·ªëng (Bag-of-Words, TF-IDF) v·ªõi ti·∫øng Vi·ªát, h√£y s·ª≠ d·ª•ng th∆∞ vi·ªán t√°ch t·ª´ chuy√™n d·ª•ng (VD: VnCoreNLP).\n",
    "\n",
    "***2. Gi·ªØ nguy√™n (ho·∫∑c x·ª≠ l√Ω ph√π h·ª£p) d·∫•u c√¢u, bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c (emoji) n·∫øu ch√∫ng mang √Ω nghƒ©a trong b√†i to√°n.***\n",
    "\n",
    "- Nhi·ªÅu b√†i to√°n ph√¢n t√≠ch c·∫£m x√∫c ·ªü MXH c·∫ßn emoji ƒë·ªÉ hi·ªÉu s·∫Øc th√°i.\n",
    "\n",
    "***3. Ki·ªÉm tra ch·∫•t l∆∞·ª£ng tokenization:***\n",
    "\n",
    "- ƒê·∫∑c bi·ªát v·ªõi ti·∫øng Vi·ªát, tokenization ch∆∞a chu·∫©n c√≥ th·ªÉ g√¢y ‚Äúv·ª° nghƒ©a‚Äù.\n",
    "- Th·ª≠ soi m·ªôt s·ªë vƒÉn b·∫£n sau khi tokenization ƒë·ªÉ ch·∫Øc ch·∫Øn ph√π h·ª£p, tr√°nh t√°ch sai t·ª´ gh√©p (VD: ‚Äúƒëi·ªán tho·∫°i‚Äù, ‚Äúc·∫ßm tay‚Äù th√†nh ‚Äúƒëi·ªán‚Äù, ‚Äútho·∫°i‚Äù, ‚Äúc·∫ßm‚Äù, ‚Äútay‚Äù).\n",
    "\n",
    "***4. X·ª≠ l√Ω t·ª´ ƒë·∫∑c bi·ªát, hashtag, mention:***\n",
    "\n",
    "- Trong b√†i to√°n MXH, token h√≥a hashtag (#myhashtag), mention (@username), link URL, v.v., t√πy xem b·∫°n c√≥ mu·ªën gi·ªØ hay lo·∫°i b·ªè.\n",
    "\n",
    "***5. Chu·∫©n ho√° (normalization):***\n",
    "\n",
    "- Th√¥ng th∆∞·ªùng, chuy·ªÉn vƒÉn b·∫£n v·ªÅ d·∫°ng ch·ªØ th∆∞·ªùng (lowercasing).\n",
    "- V·ªõi ti·∫øng Vi·ªát, c·∫ßn xem c√≥ bi·∫øn ƒë·ªïi d·∫•u kh√¥ng, ho·∫∑c chu·∫©n ho√° k√Ω t·ª± unicode t·ªï h·ª£p.\n",
    "\n",
    "**Note**: M√¥ h√¨nh BERT/PhoBERT g·ªëc c√≥ th·ªÉ kh√¥ng lowercasing ƒë·ªÉ gi·ªØ nguy√™n case. N√™n ki·ªÉm tra m√¥ h√¨nh pre-trained y√™u c·∫ßu g√¨.\n",
    "\n",
    "---\n",
    "\n",
    "**C√≥ c·∫ßn tuning l·∫°i tokenization?**\n",
    "\n",
    "V·ªõi c√°c Large Language Model (LLM) hi·ªán ƒë·∫°i nh∆∞ GPT, BERT-based, T5, RoBERTa, ‚Ä¶ th∆∞·ªùng d√πng m·ªôt vocabulary v√† c∆° ch·∫ø tokenization ƒë√£ hu·∫•n luy·ªán s·∫µn. Khi b·∫°n t·∫£i m√¥ h√¨nh n√†y v·ªÅ, ƒëi k√®m lu√¥n c√≥ tokenizer (v√† embedding matrix t∆∞∆°ng ·ª©ng) ƒë√£ ƒë·ªìng b·ªô v·ªõi m√¥ h√¨nh.\n",
    "\n",
    "> Th·ª±c t·∫ø, ƒëa s·ªë ng∆∞·ªùi d√πng kh√¥ng ph·∫£i t·ª± retrain hay tune l·∫°i tokenizer.\n",
    "> - V√¨ m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c pre-trained tr√™n m·ªôt kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì, tokenizer ban ƒë·∫ßu (subword / BPE / SentencePiece) ƒë√£ t∆∞∆°ng ƒë·ªëi t·ªëi ∆∞u.\n",
    "> - N·∫øu t·ª± √Ω thay ƒë·ªïi tokenizer (th√™m token, b·ªõt token, thay ƒë·ªïi c√°ch t√°ch subword), b·∫°n s·∫Ω ph·∫£i hu·∫•n luy·ªán l·∫°i (ho·∫∑c ƒëi·ªÅu ch·ªânh l·ªõn) embedding, l√†m m·∫•t t√≠nh t∆∞∆°ng th√≠ch v·ªõi c√°c tr·ªçng s·ªë ƒë√£ ƒë∆∞·ª£c pre-trained.\n",
    "\n",
    "\n",
    "***Ngo·∫°i l·ªá: Trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát, b·∫°n c√≥ th·ªÉ re-train / fine-tune tokenizer.***\n",
    "\n",
    "> V√≠ d·ª•: khi b·∫°n c√≥ m·ªôt mi·ªÅn d·ªØ li·ªáu r·∫•t ƒë·∫∑c th√π (nh∆∞ y h·ªçc, h√≥a h·ªçc, t√†i ch√≠nh) v·ªõi nhi·ªÅu thu·∫≠t ng·ªØ, t·ª´ vi·∫øt t·∫Øt, k√Ω hi·ªáu‚Ä¶ ho√†n to√†n kh√¥ng (ho·∫∑c r·∫•t √≠t) xu·∫•t hi·ªán trong b·ªô pre-training.\n",
    "> - Khi ƒë√≥, tokenizer c≈© c√≥ th·ªÉ t·∫°o ra nhi·ªÅu token ‚Äú[UNK]‚Äù (token l·∫°) ho·∫∑c token r·∫•t d√†i do kh√¥ng t√¨m th·∫•y subword ph√π h·ª£p.\n",
    "> - B·∫°n mu·ªën n√¢ng cao kh·∫£ nƒÉng bi·ªÉu di·ªÖn, c√≥ th·ªÉ hu·∫•n luy·ªán l·∫°i tokenizer tr√™n data chuy√™n ng√†nh, r·ªìi fine-tune (ho·∫∑c hu·∫•n luy·ªán l·∫°i t·ª´ ƒë·∫ßu) embedding. Tuy nhi√™n, vi·ªác n√†y ƒë√≤i h·ªèi r·∫•t nhi·ªÅu t√†i nguy√™n v√† kinh nghi·ªám, v√¨ b·∫°n c·∫ßn ƒë·∫£m b·∫£o m√¥ h√¨nh (v√† embedding matrix) t∆∞∆°ng th√≠ch v·ªõi tokenizer m·ªõi.\n",
    "\n",
    "\n",
    "V·ªõi h·∫ßu h·∫øt d·ª± √°n NLP ph·ªï bi·∫øn, tuning l·∫°i tokenizer l√† kh√¥ng c·∫ßn thi·∫øt (th·∫≠m ch√≠ b·∫•t l·ª£i n·∫øu b·∫°n kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán l·∫°i ph·∫ßn embedding). Th√¥ng th∆∞·ªùng, b·∫°n:\n",
    "- D√πng tokenizer g·ªëc k√®m m√¥ h√¨nh.\n",
    "- Fine-tune m√¥ h√¨nh tr√™n d·ªØ li·ªáu b·∫°n quan t√¢m. M·ªçi th·ª© t·ª´ tokenization ƒë·∫øn embedding ƒë∆∞·ª£c k·∫ø th·ª´a s·∫µn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by RegEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by keras\n",
    "S·ª≠ d·ª•ng [`tf.keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) v·ªõi m·ªôt s·ªë params nh∆∞ sau:\n",
    "- `max_tokens` - S·ªë l∆∞·ª£ng word t·ªëi ƒëa trong vocabulary (e.g. 20000 or the number of unique words in your text), bao g·ªìm 1 slot cho OOV (out of vocabulary) tokens.\n",
    "- `standardize` - Ph∆∞∆°ng th·ª©c ƒë·ªÉ standardizing text. Default is \"lower_and_strip_punctuation\" nghƒ©a l√† lowers text and removes all punctuation marks.\n",
    "- `split` - How to split text, default is \"whitespace\" which splits on spaces.\n",
    "- `ngrams` - How many words to contain per token split (create groups of n-words?), for example, ngrams=2 splits tokens into continuous sequences of 2.\n",
    "- `output_mode` - How to output tokens:\n",
    "    - \"int\" (integer mapping): map theo index c·ªßa t·ª´ trong vocab\n",
    "    - \"multi_hot\" : mapping theo ki·ªÉu one-hot n·∫øu t·ª´ ƒë√≥ c√≥ xu·∫•t hi·ªán trong text\n",
    "    - \"count\": mapping theo s·ªë l·∫ßn t·ª´ ƒë√≥ xu·∫•t hi·ªán trong text\n",
    "    - [\"tf-idf\"](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting):  **Term Frequency - Inverse Document Frequency** tr√≠ch xu·∫•t v√† ƒë√°nh tr·ªçng s·ªë cho t·ª´/ng·ªØ (term) trong c√°c t√†i li·ªáu vƒÉn b·∫£n.\n",
    "        - `TF` L√† t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa m·ªôt t·ª´ (term) trong t√†i li·ªáu. cho bi·∫øt m·ªôt t·ª´ c√≥ ‚Äún·ªïi b·∫≠t‚Äù trong t√†i li·ªáu c·ª• th·ªÉ hay kh√¥ng.\n",
    "        - `IDF`  L√† ngh·ªãch ƒë·∫£o t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa m·ªôt t·ª´ trong t·∫≠p c√°c t√†i li·ªáu. cho bi·∫øt t·ª´ ƒë√≥ c√≥ ph·ªï bi·∫øn/hay hi·∫øm trong to√†n b·ªô kho t√†i li·ªáu.\n",
    "        - **TF cao**: T·ª´ th∆∞·ªùng xu·∫•t hi·ªán nhi·ªÅu trong t√†i li·ªáu => c√≥ vai tr√≤ quan tr·ªçng trong t√†i li·ªáu ƒë√≥.\n",
    "        - **IDF cao**: T·ª´ hi·∫øm (√≠t xu·∫•t hi·ªán trong to√†n b·ªô t·∫≠p t√†i li·ªáu) => ƒë·ªô ‚Äúph√¢n bi·ªát‚Äù cao.\n",
    "        - **TF-IDF cao**: T·ª´ r·∫•t quan tr·ªçng (ph√¢n bi·ªát) cho t√†i li·ªáu ƒë√≥ (xu·∫•t hi·ªán nhi·ªÅu trong t√†i li·ªáu v√† hi·∫øm g·∫∑p trong t√†i li·ªáu kh√°c).\n",
    "- `output_sequence_length` - S·ª≠ d·ª•ng trong `output_mode=int`, quy ƒë·ªãnh ƒë·ªô d√†i c·ªßa m·ªói sequence output g·ªìm bao nhi√™u tokens, n·∫øu sequence c√≥ ƒë·ªô d√†i h∆°n `output_sequence_length` th√¨ s·∫Ω ƒë∆∞·ª£c truncated, n·∫øu √≠t h∆°n th√¨ ƒë∆∞·ª£c padded ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô d√†i ch√≠nh x√°c c·ªßa output tensor l√† `shape = (batch_size, output_sequence_length)`\n",
    "- `pad_to_max_tokens` - Defaults to False, if True, the output feature axis s·∫Ω ƒë∆∞·ª£c padded/m·ªü r·ªông t·ªõi ƒë·ªô d√†i b·∫±ng max_tokens ngay c·∫£ khi s·ªë l∆∞·ª£ng unique tokens in the vocabulary nh·ªè h∆°n max_tokens. Ch·ªâ c√≥ t√°c d·ª•ng trong `output_mode` l√† `multi_hot`, `count`, `tf_id`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by transformer pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# T·∫£i tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Preprocessing can involve extracting features from text, such as word frequencies, n-grams, or word embeddings, which are essential for building machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Text data often has a high dimensionality due to the presence of a large vocabulary. Preprocessing techniques like term frequency-inverse document frequency (TF-IDF) or dimensionality reduction methods can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
