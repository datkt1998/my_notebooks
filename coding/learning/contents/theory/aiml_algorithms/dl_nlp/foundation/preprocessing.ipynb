{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing in NLP\n",
    "\n",
    "Mục tiêu của toàn bộ quá trình này là giúp mô hình học máy (Machine Learning) hoặc mô hình học sâu (Deep Learning) nhận được đầu vào “sạch” và “chuẩn hoá”, từ đó đạt hiệu quả cao hơn trong phân tích và dự đoán.\n",
    "\n",
    "**NLP Python packages:**\n",
    "\n",
    "|NLP Library|\tDescription|\n",
    "|---|---|\n",
    "|NLTK\t|This is one of the most usable and mother of all NLP libraries.|\n",
    "|spaCy\t|This is a completely optimized and highly accurate library widely used in deep learning|\n",
    "|Stanford CoreNLP| Python\tFor client-server-based architecture, this is a good library in NLTK. This is written in JAVA, but it provides modularity to use it in Python.|\n",
    "|TextBlob\t|This is an NLP library which works in Pyhton2 and python3. This is used for processing textual data and provide mainly all type of operation in the form of API.|\n",
    "|Gensim\t|Genism is a robust open source NLP library support in Python. This library is highly efficient and scalable.|\n",
    "|Pattern\t|It is a light-weighted NLP module. This is generally used in Web-mining, crawling or such type of spidering task|\n",
    "|Polyglot\t|For massive multilingual applications, Polyglot is best suitable NLP library. Feature extraction in the way on Identity and Entity.|\n",
    "|PyNLPl\t|PyNLPI also was known as ‘Pineapple’ and supports Python. It provides a parser for many data formats like FoLiA/Giza/Moses/ARPA/Timbl/CQL.|\n",
    "|Vocabulary\t|This library is best to get Semantic type information from the given text.|\n",
    "|pyvi\t| Python Vietnamese Core NLP Toolkit |\n",
    "|underthesea| Underthesea - Vietnamese NLP Toolkit |\n",
    "\n",
    "\n",
    "**The process of NLP Preprocessing is as follows:**\n",
    "1. General cleaning\n",
    "    - Case normalization\n",
    "    - Normalize grammatical structure\n",
    "    - Regular expression handling\n",
    "2. Removing noise from the dataset\n",
    "    - Removing special characters/patterns\n",
    "    - Removing punctuations\n",
    "    - Removing stop words\n",
    "    - Remove unnecessary components: table, image, etc.\n",
    "3. Normalizing text to right-format for the ML Algorithm\n",
    "    - Tagging: Part-of-speech tagging, named-entity recognition\n",
    "    - Stemming / Lemmatization\n",
    "4. Tokenization\n",
    "5. Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...</td>\n",
       "      <td>binhluan.biz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...</td>\n",
       "      <td>www.ipick.vn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...</td>\n",
       "      <td>tintucqpvn.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          domain  label\n",
       "0  Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...    binhluan.biz      1\n",
       "1  Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...    www.ipick.vn      1\n",
       "2  Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...  tintucqpvn.net      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# link data: https://github.com/WhySchools/VFND-vietnamese-fake-news-datasets/blob/master/CSV/vn_news_223_tdlfr.csv\n",
    "\n",
    "raw = pd.read_csv(\n",
    "    r\"contents\\theory\\aiml_algorithms\\dl_nlp\\data\\vn_news_223_tdlfr.csv\"\n",
    ")\n",
    "print(raw.shape)\n",
    "raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Cleaning\n",
    "\n",
    "1. **Case Normalization**: Thường chuyển tất cả về chữ thường (lowercase) để giảm độ phức tạp khi so sánh từ. However, do remember that **lowercasing can change the meaning of some text** e.g \"US\" vs \"us\".\n",
    "\n",
    "2. **Sửa lỗi chính tả (nếu cần)**: Trong một số bài toán phân tích ngôn ngữ, việc chính tả chính xác có ý nghĩa quan trọng.\n",
    "\n",
    "3. **Xử lý các từ viết tắt, từ lóng**: Ví dụ: “ko” -> “không”, “k” -> “không” (trong tiếng Việt), hoặc “u” -> “you” (tiếng Anh). --> Việc nhất quán hoá các biến thể từ vựng giúp mô hình hiểu rõ hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise\n",
    "\n",
    "Text data often contains noise such as punctuation, special characters, and irrelevant symbols. Preprocessing helps remove these elements, making the text cleaner and easier to analyze.\n",
    "\n",
    "1. **Loại bỏ ký tự hoặc biểu tượng không mong muốn**: ký tự đặc biệt, emoji, đường dẫn (URL), email, ký tự HTML, thẻ HTML, v.v. --> giảm bớt những thành phần không có giá trị ngữ nghĩa hoặc gây nhiễu.\n",
    "\n",
    "2. **Loại bỏ khoảng trắng, xuống dòng thừa, or punctuations**: like `. , ! $( ) * % @` giúp dữ liệu gọn gàng, nhất quán.\n",
    "\n",
    "3. **Loại bỏ hoặc thay thế token vô nghĩa (Stopwords, từ vô nghĩa trong ngữ cảnh)**: Stopwords (như \"và\", \"hoặc\", \"của\" trong tiếng Việt; \"the\", \"is\", \"at\" trong tiếng Anh, v.v.) thường ít mang thông tin ngữ nghĩa và có thể gây nhiễu cho mô hình. Tùy bài toán mà quyết định giữ hay bỏ, vì đôi khi stopwords cũng quan trọng trong một số ngữ cảnh.\n",
    "\n",
    "4. **Loại bỏ những phần tử không liên quan**: Ví dụ: trong các đoạn văn bản có chèn các code snippet, bảng biểu, metadata… không cần thiết cho phân tích.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing\n",
    "\n",
    "Different forms of words (e.g., “run,” “running,” “ran”) can convey the same meaning but appear in different forms. Preprocessing techniques like stemming and lemmatization help standardize these variations.\n",
    "\n",
    "1. **Tagging**\n",
    "    - **Part-of-speech (POS)**: \n",
    "    - **Named-entity recognition (NER)**:\n",
    "\n",
    "2. **Stemming / Lemmatization (Giảm biến thể từ vựng - useful for English)**:\n",
    "    - **Stemming**: cắt bỏ phần “đuôi” của từ để đưa về “gốc” (có thể không phải là từ đúng trong từ điển).\n",
    "        ```text\n",
    "        connecting  -->  connect\n",
    "        connected  -->  connect\n",
    "        connectivity  -->  connect\n",
    "        connect  -->  connect\n",
    "        connects  -->  connect\n",
    "        ```\n",
    "    - **Lemmatization**: đưa từ về dạng “gốc từ điển” (chính tắc) dựa vào từ loại, ngữ cảnh. **Lemmatization** về cơ bản là giống với **stemming** khi nó loại bỏ phần đuôi của từ để thu được gốc từ, tuy nhiên các gốc từ ở đây đều thực sự tốn tại chứ không như **stemming** (như ví dụ trên thì từ `moved` sau khi lemmatize sẽ thu được `move`). Trong thư viện NLTK sẽ sử dụng từ điển **Wordnet** để map theo các quy tắc (theo tính chất của từ, từ là danh từ, động từ, trạng từ hay tính từ). Sử dụng part-of-speech tagging (nltk.pos_tag) để thu được các tính chất của từ.\n",
    "    \n",
    "    -> Hai kỹ thuật này giúp giảm sự trùng lặp khi cùng một từ xuất hiện ở các dạng biến thể khác nhau.\n",
    "\n",
    "3. **Xử lý nhãn (nếu là bài toán giám sát)**:\n",
    "    - Kiểm tra và chuẩn hoá dữ liệu nhãn (label). Ví dụ: chuyển từ “positive” / “negative” / “neutral” sang 0 / 1 / 2 hoặc tương tự."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...</td>\n",
       "      <td>thủ_tướng Abe cúi đầu xin_lỗi vì hành_động phi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...</td>\n",
       "      <td>thủ_tướng Nhật cúi đầu xin_lỗi vì tinh_thần ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...</td>\n",
       "      <td>choáng ! cơ_trưởng đeo khăn_quàng quẩy banh nó...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chưa bao giờ nhạc Kpop lại dễ hát đến thế!!!\\n...</td>\n",
       "      <td>chưa bao_giờ nhạc Kpop lại dễ hát đến thế ! ! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đại học Hutech sẽ áp dụng cải cách \"Tiếq Việt\"...</td>\n",
       "      <td>đại_học Hutech sẽ áp_dụng cải_cách \" Tiếq Việt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>“Siêu máy bay” A350 sẽ chở CĐV Việt Nam đi Mal...</td>\n",
       "      <td>“ siêu máy_bay ” A350 sẽ chở cđv Việt_Nam đi M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Thưởng 20.000 USD cho đội tuyển cờ vua Việt Na...</td>\n",
       "      <td>thưởng 20.000 usd cho đội_tuyển cờ_vua Việt_Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Trường Sơn giành HCV tại giải cờ vua đồng đội ...</td>\n",
       "      <td>Trường_Sơn giành hcv tại giải cờ_vua đồng_đội ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Chuyện về chàng sinh viên Luật - Kiện tướng Lê...</td>\n",
       "      <td>chuyện về chàng sinh_viên Luật - kiện_tướng Lê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Tiền đạo Malaysia: “Tôi đã có cách vượt qua hà...</td>\n",
       "      <td>tiền_đạo Malaysia : “ tôi đã có cách vượt qua ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...   \n",
       "1    Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...   \n",
       "2    Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...   \n",
       "3    Chưa bao giờ nhạc Kpop lại dễ hát đến thế!!!\\n...   \n",
       "4    Đại học Hutech sẽ áp dụng cải cách \"Tiếq Việt\"...   \n",
       "..                                                 ...   \n",
       "218  “Siêu máy bay” A350 sẽ chở CĐV Việt Nam đi Mal...   \n",
       "219  Thưởng 20.000 USD cho đội tuyển cờ vua Việt Na...   \n",
       "220  Trường Sơn giành HCV tại giải cờ vua đồng đội ...   \n",
       "221  Chuyện về chàng sinh viên Luật - Kiện tướng Lê...   \n",
       "222  Tiền đạo Malaysia: “Tôi đã có cách vượt qua hà...   \n",
       "\n",
       "                                        processed_text  \n",
       "0    thủ_tướng Abe cúi đầu xin_lỗi vì hành_động phi...  \n",
       "1    thủ_tướng Nhật cúi đầu xin_lỗi vì tinh_thần ph...  \n",
       "2    choáng ! cơ_trưởng đeo khăn_quàng quẩy banh nó...  \n",
       "3    chưa bao_giờ nhạc Kpop lại dễ hát đến thế ! ! ...  \n",
       "4    đại_học Hutech sẽ áp_dụng cải_cách \" Tiếq Việt...  \n",
       "..                                                 ...  \n",
       "218  “ siêu máy_bay ” A350 sẽ chở cđv Việt_Nam đi M...  \n",
       "219  thưởng 20.000 usd cho đội_tuyển cờ_vua Việt_Na...  \n",
       "220  Trường_Sơn giành hcv tại giải cờ_vua đồng_đội ...  \n",
       "221  chuyện về chàng sinh_viên Luật - kiện_tướng Lê...  \n",
       "222  tiền_đạo Malaysia : “ tôi đã có cách vượt qua ...  \n",
       "\n",
       "[223 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "import string\n",
    "\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "# stop words: https://github.com/stopwords/vietnamese-stopwords/blob/master/vietnamese-stopwords-dash.txt\n",
    "stop_words = (\n",
    "    pd.read_csv(\n",
    "        r\"contents\\theory\\aiml_algorithms\\dl_nlp\\data\\vietnamese-stopwords-dash.txt\",\n",
    "        header=None,\n",
    "    )\n",
    "    .iloc[:, 0]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    # replace parttern \" ko \" by \" không \"\n",
    "    processed_text = re.sub(r\"\\bko\\b\", \"không\", text)\n",
    "\n",
    "    tokens = ViTokenizer.tokenize(text)\n",
    "    pos_tags = ViPosTagger.postagging(tokens)\n",
    "    processed_text = []\n",
    "    for token, pos in zip(pos_tags[0], pos_tags[1]):\n",
    "        # if token not in stop_words:\n",
    "        if pos.startswith(\"Np\"):\n",
    "            processed_text.append(token.title())\n",
    "        else:\n",
    "            processed_text.append(token.lower())\n",
    "\n",
    "    return \" \".join(processed_text)\n",
    "\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "raw[\"processed_text\"] = raw[\"text\"].apply(process_text)\n",
    "raw[[\"text\", \"processed_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n",
    "\n",
    "**Tokenization**: Tokenization (quy trình tách từ, chia nhỏ văn bản) là bước tiền xử lý (preprocessing) vô cùng quan trọng trong xử lý văn bản. Mục tiêu của tokenization là chuyển văn bản gốc (chuỗi ký tự) thành danh sách các token (những đơn vị có ý nghĩa).\n",
    "\n",
    "- Tách câu thành các đơn vị từ hoặc subword.\n",
    "- Trong tiếng Anh thường dễ dàng hơn (tách theo dấu cách và ký tự đặc biệt), còn tiếng Việt cần sử dụng mô hình hoặc thư viện tách từ chuyên dụng (như VnCoreNLP, PyVi, v.v.).\n",
    "\n",
    "Ánh xạ character/word/subword sang giá trị số numberical value. Có 3 level của tokenization\n",
    "- _Word-level tokenization_: Mỗi từ sẽ đại diện bởi 1 numerical value. Tách theo khoảng trắng hoặc dựa trên thư viện tokenizer (tiếng Việt: VnCoreNLP, PyVi, RDRSegmenter, …). Ví dụ: \"I love yout\" ---> [0,1,2]\n",
    "- _Character-level tokenization_: Mỗi character (chữ cái, dấu câu) sẽ đại diện cho 1 token. Hữu ích trong một số bài toán (đặc biệt với các mô hình ngôn ngữ có ý định xử lý đánh vần, hoặc khi dữ liệu có nhiều từ mới).\n",
    "- _Sub-word tokenization_: break từng từ thành các phần và tokenization nó, khi đó mỗi word có thể thành nhiều tokens. Kết hợp ưu điểm giữa word-level và character-level, được sử dụng trong BERT, GPT, RoBERTa, PhoBERT, v.v.\n",
    "\n",
    "> Tuỳ thuộc vào problem mà nên chọn level tokenization cho phù hợp, hoặc có thể thử các level và kiểm tra performance, hoặc có thể sử dụng `tf.keras.layers.concatenate` để combine/stacking chúng lại với nhau.\n",
    "---\n",
    "**Tại sao tokenization quan trọng?**\n",
    "- Các mô hình NLP cổ điển (Bag-of-Words, TF-IDF, v.v.) hay hiện đại (Deep Learning) đều làm việc trên các đơn vị rời rạc (token).\n",
    "- Tokenization quyết định cách mô hình nhận thức văn bản: sai sót hoặc thiếu hợp lý trong tokenization ảnh hưởng đáng kể đến chất lượng mô hình.\n",
    "- Với các mô hình ngôn ngữ hiện đại (**Transformer** như `BERT`, `GPT`, `RoBERTa`, v.v.), vẫn cần **tokenization**, thường là subword tokenization (ví dụ `BPE`, `SentencePiece`). Lý do: mô hình cần chia văn bản thành các “mã” (code) được học sẵn trong từ vựng (vocabulary) để ánh xạ mỗi token sang vector **embedding** phù hợp.\n",
    "\n",
    "---\n",
    "**Bản chất hành động của Tokenization và Embedding:**\n",
    "\n",
    "- **Tokenization**: Chuyển đổi từ văn bản thành danh sách các token (dạng chữ) (word, subword, character). Sau đó sẽ chuyển sang dạng số (index) thông qua từ điển (vocabulary).\n",
    "- **Embedding**: Với mỗi 1 token (dạng số - index) thì token chuyển đổi từ dạng số (index) sang **vector** (nhiều chiều) số thực (embedding). Mỗi token sẽ được biểu diễn bởi một vector số thực có số chiều là `d` xác định trước (ví dụ: 100, 200, 300 chiều). Thể hiện sự tương quan giữa các token trong không gian vector.\n",
    "- Khi mô hình nhận input (list các token ID), nó sẽ tra cứu (lookup) từng token ID trong hàng tương ứng của ma trận embedding `𝑊` để lấy được vector (d chiều) tương ứng với token đó.\n",
    "- Kết quả: Thay vì list số ID, mô hình có một chuỗi vector (một cho mỗi token), phản ánh thông tin ngữ nghĩa và ngữ cảnh (với các mô hình hiện đại) của những token đó. Sau đó, mô hình sẽ sử dụng vector này để thực hiện các phép toán (tích vô hướng, pooling, attention, v.v.) để học cấu trúc ngữ nghĩa của văn bản.\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practice cho Tokenization**\n",
    "\n",
    "***1. Chọn phương pháp tokenization phù hợp:***\n",
    "\n",
    "- Với mô hình Transformer hiện đại, thường dùng subword tokenization (BPE, SentencePiece) vì khả năng xử lý tốt từ mới, từ sai chính tả, từ hiếm, v.v.\n",
    "- Nếu làm truyền thống (Bag-of-Words, TF-IDF) với tiếng Việt, hãy sử dụng thư viện tách từ chuyên dụng (VD: VnCoreNLP).\n",
    "\n",
    "***2. Giữ nguyên (hoặc xử lý phù hợp) dấu câu, biểu tượng cảm xúc (emoji) nếu chúng mang ý nghĩa trong bài toán.***\n",
    "\n",
    "- Nhiều bài toán phân tích cảm xúc ở MXH cần emoji để hiểu sắc thái.\n",
    "\n",
    "***3. Kiểm tra chất lượng tokenization:***\n",
    "\n",
    "- Đặc biệt với tiếng Việt, tokenization chưa chuẩn có thể gây “vỡ nghĩa”.\n",
    "- Thử soi một số văn bản sau khi tokenization để chắc chắn phù hợp, tránh tách sai từ ghép (VD: “điện thoại”, “cầm tay” thành “điện”, “thoại”, “cầm”, “tay”).\n",
    "\n",
    "***4. Xử lý từ đặc biệt, hashtag, mention:***\n",
    "\n",
    "- Trong bài toán MXH, token hóa hashtag (#myhashtag), mention (@username), link URL, v.v., tùy xem bạn có muốn giữ hay loại bỏ.\n",
    "\n",
    "***5. Chuẩn hoá (normalization):***\n",
    "\n",
    "- Thông thường, chuyển văn bản về dạng chữ thường (lowercasing).\n",
    "- Với tiếng Việt, cần xem có biến đổi dấu không, hoặc chuẩn hoá ký tự unicode tổ hợp.\n",
    "\n",
    "**Note**: Mô hình BERT/PhoBERT gốc có thể không lowercasing để giữ nguyên case. Nên kiểm tra mô hình pre-trained yêu cầu gì.\n",
    "\n",
    "---\n",
    "\n",
    "**Có cần tuning lại tokenization?**\n",
    "\n",
    "Với các Large Language Model (LLM) hiện đại như GPT, BERT-based, T5, RoBERTa, … thường dùng một vocabulary và cơ chế tokenization đã huấn luyện sẵn. Khi bạn tải mô hình này về, đi kèm luôn có tokenizer (và embedding matrix tương ứng) đã đồng bộ với mô hình.\n",
    "\n",
    "> Thực tế, đa số người dùng không phải tự retrain hay tune lại tokenizer.\n",
    "> - Vì mô hình đã được pre-trained trên một khối lượng dữ liệu khổng lồ, tokenizer ban đầu (subword / BPE / SentencePiece) đã tương đối tối ưu.\n",
    "> - Nếu tự ý thay đổi tokenizer (thêm token, bớt token, thay đổi cách tách subword), bạn sẽ phải huấn luyện lại (hoặc điều chỉnh lớn) embedding, làm mất tính tương thích với các trọng số đã được pre-trained.\n",
    "\n",
    "\n",
    "***Ngoại lệ: Trong một số trường hợp đặc biệt, bạn có thể re-train / fine-tune tokenizer.***\n",
    "\n",
    "> Ví dụ: khi bạn có một miền dữ liệu rất đặc thù (như y học, hóa học, tài chính) với nhiều thuật ngữ, từ viết tắt, ký hiệu… hoàn toàn không (hoặc rất ít) xuất hiện trong bộ pre-training.\n",
    "> - Khi đó, tokenizer cũ có thể tạo ra nhiều token “[UNK]” (token lạ) hoặc token rất dài do không tìm thấy subword phù hợp.\n",
    "> - Bạn muốn nâng cao khả năng biểu diễn, có thể huấn luyện lại tokenizer trên data chuyên ngành, rồi fine-tune (hoặc huấn luyện lại từ đầu) embedding. Tuy nhiên, việc này đòi hỏi rất nhiều tài nguyên và kinh nghiệm, vì bạn cần đảm bảo mô hình (và embedding matrix) tương thích với tokenizer mới.\n",
    "\n",
    "\n",
    "Với hầu hết dự án NLP phổ biến, tuning lại tokenizer là không cần thiết (thậm chí bất lợi nếu bạn không có đủ dữ liệu để huấn luyện lại phần embedding). Thông thường, bạn:\n",
    "- Dùng tokenizer gốc kèm mô hình.\n",
    "- Fine-tune mô hình trên dữ liệu bạn quan tâm. Mọi thứ từ tokenization đến embedding được kế thừa sẵn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by RegEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by keras\n",
    "Sử dụng [`tf.keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) với một số params như sau:\n",
    "- `max_tokens` - Số lượng word tối đa trong vocabulary (e.g. 20000 or the number of unique words in your text), bao gồm 1 slot cho OOV (out of vocabulary) tokens.\n",
    "- `standardize` - Phương thức để standardizing text. Default is \"lower_and_strip_punctuation\" nghĩa là lowers text and removes all punctuation marks.\n",
    "- `split` - How to split text, default is \"whitespace\" which splits on spaces.\n",
    "- `ngrams` - How many words to contain per token split (create groups of n-words?), for example, ngrams=2 splits tokens into continuous sequences of 2.\n",
    "- `output_mode` - How to output tokens:\n",
    "    - \"int\" (integer mapping): map theo index của từ trong vocab\n",
    "    - \"multi_hot\" : mapping theo kiểu one-hot nếu từ đó có xuất hiện trong text\n",
    "    - \"count\": mapping theo số lần từ đó xuất hiện trong text\n",
    "    - [\"tf-idf\"](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting):  **Term Frequency - Inverse Document Frequency** trích xuất và đánh trọng số cho từ/ngữ (term) trong các tài liệu văn bản.\n",
    "        - `TF` Là tần suất xuất hiện của một từ (term) trong tài liệu. cho biết một từ có “nổi bật” trong tài liệu cụ thể hay không.\n",
    "        - `IDF`  Là nghịch đảo tần suất xuất hiện của một từ trong tập các tài liệu. cho biết từ đó có phổ biến/hay hiếm trong toàn bộ kho tài liệu.\n",
    "        - **TF cao**: Từ thường xuất hiện nhiều trong tài liệu => có vai trò quan trọng trong tài liệu đó.\n",
    "        - **IDF cao**: Từ hiếm (ít xuất hiện trong toàn bộ tập tài liệu) => độ “phân biệt” cao.\n",
    "        - **TF-IDF cao**: Từ rất quan trọng (phân biệt) cho tài liệu đó (xuất hiện nhiều trong tài liệu và hiếm gặp trong tài liệu khác).\n",
    "- `output_sequence_length` - Sử dụng trong `output_mode=int`, quy định độ dài của mỗi sequence output gồm bao nhiêu tokens, nếu sequence có độ dài hơn `output_sequence_length` thì sẽ được truncated, nếu ít hơn thì được padded để đảm bảo độ dài chính xác của output tensor là `shape = (batch_size, output_sequence_length)`\n",
    "- `pad_to_max_tokens` - Defaults to False, if True, the output feature axis sẽ được padded/mở rộng tới độ dài bằng max_tokens ngay cả khi số lượng unique tokens in the vocabulary nhỏ hơn max_tokens. Chỉ có tác dụng trong `output_mode` là `multi_hot`, `count`, `tf_id`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by transformer pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Tải tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Preprocessing can involve extracting features from text, such as word frequencies, n-grams, or word embeddings, which are essential for building machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Text data often has a high dimensionality due to the presence of a large vocabulary. Preprocessing techniques like term frequency-inverse document frequency (TF-IDF) or dimensionality reduction methods can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
