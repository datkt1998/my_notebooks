{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Techniques Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN's)\n",
    "\n",
    "Khi ƒë·ªçc v√† hi·ªÉu 1 words th√¨ ƒë·ªÉ hi·ªÉu t·ª´ ƒë√≥ trong c·∫£ c√¢u/ng·ªØ c·∫£nh ƒë√≥, hay n√≥i c√°ch kh√°c c·∫ßn ph·∫£i hi·ªÉu b·ªëi c·∫£nh c·ªßa c√¢u ho·∫∑c c√°c words tr∆∞·ªõc ƒë√≥. V√≠ d·ª• ch√∫ng ta c√≥ 2 c√¢u l√† \"B·ªë ƒÉn t·ªëi ch∆∞a ?\" v√† \"B·ªë ch∆∞a ƒÉn t·ªëi\". M·∫∑c d√π c·∫£ 2 sequence ƒë·ªÅu c√≥ c√°c t·ª´ gi·ªëng nhau nh∆∞ng kh√°c nhau v·ªÅ m·∫∑t √Ω nghƒ©a. Th·ª© t·ª± c√°c words quy·∫øt ƒë·ªãnh √Ω nghƒ©a c·ªßa sentence.\n",
    "\n",
    "Khi RNN looks at 1 sequence of text (d∆∞·ªõi d·∫°ng numeric), c√°c pattern s·∫Ω ƒë∆∞·ª£c h·ªçc 1 c√°ch li√™n t·ª•c d·ª±a v√†o order c·ªßa sequence ƒë√≥. RNN's l√† m·∫°ng ƒë·∫∑c tr∆∞ng b·ªüi vi·ªác l·∫•y nh·ªØng input(x) + previous inputs ƒë·ªÉ compute Y, helpful when dealing with sequences data, such as natural language text.\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc)\n",
    "\n",
    "V·ªõi m·ªói timestep t, the activation $a^{<t>}$ v√† output $y^{t}$ ƒë∆∞·ª£c t√≠nh theo c√¥ng th·ª©c\n",
    "> $$a^{<t>}=g_{1}(W_{a a}a^{<t-1>}+W_{a x}x^{<t>}+b_{a})$$\n",
    "\n",
    "> $$y^{<t>}=g_{2}(W_{y a}a^{<t>}+b_{y})$$\n",
    "\n",
    "Trong ƒë√≥ c√°c h·ªá s·ªë `W` v√† `b` l√† c√°c weights s·∫Ω ƒë∆∞·ª£c update trong qu√° tr√¨nh learn, c√≤n `g1`, `g2` l√† c√°c h√†m activation functions\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn-ltr.png?74e25518f882f8758439bcb3637715e5)\n",
    "\n",
    "__∆Øu ƒëi·ªÉm c·ªßa RNN__:\n",
    "- X·ª≠ l√Ω input c√≥ b·∫•t k·ª≥ ƒë·ªô d√†i nh∆∞ th·∫ø n√†o\n",
    "- Model size not increasing with size of input\n",
    "- Computation takes into account history information\n",
    "- Weights ƒë∆∞·ª£c share theo th·ªùi gian\n",
    "\n",
    "__H·∫°n ch·∫ø c·ªßa RNN__:\n",
    "- Ph·∫£i th·ª±c hi·ªán tu·∫ßn t·ª± data, n√™n ko tu·∫≠n d·ª•ng ƒë∆∞·ª£c s·ª©c m·∫°nh t√≠nh to√°n song song (CPU/GPU), t√≠nh to√°n l√¢u\n",
    "- Kh√≥ trong vi·ªác accessing nh·ªØng long-history information v√†o th·ªùi ƒëi·ªÉm hi·ªán t·∫°i\n",
    "- Cannot consider any future input for the current state\n",
    "- Vanishing gradient: do c√°c h√†m activation trong RNN th∆∞·ªùng l√† `tanh` (c√≥ output y [-1,1] v√† ƒë·∫°o h√†m [0,1]) v√† `sigmoid` (c√≥ output y [0,1] v√† ƒë·∫°o h√†m [0,0.25]), r·∫•t d·ªÖ g√¢y ra ƒë·∫°o h√†m = 0 v·ªõi c√°c gi√° tr·ªã activation_input l·ªõn khi·∫øn c√°c weights ph√≠a xa ƒë·ªÅu kh√¥ng ƒë∆∞·ª£c update, t·ª©c l√† c√°c node ph√≠a xa kh√¥ng c√≤n t√°c d·ª•ng nhi·ªÅu t·ªõi node hi·ªán t·∫°i n·ªØa. C√≥ m·ªôt s·ªë c√°ch kh·∫Øc ph·ª•c b·∫±ng vi·ªác:\n",
    "    - S·ª≠ d·ª•ng activation l√† __ReLU__ ho·∫∑c c√°c bi·∫øn th·ªÉ\n",
    "    - S·ª≠ d·ª•ng 1 s·ªë m·∫°ng bi·∫øn th·ªÉ nh∆∞ __GRU__ hay __LSTM__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M·ªôt s·ªë c√°c tri·ªÉn khai c·ªßa RNN's\n",
    "\n",
    "![](https://3863425935-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LIA3amopGH9NC6Rf0mA%2F-LIA3mTJltflw3MVKAEQ%2F-LIA3nSKrqNJpLgASeso%2Fsequence.png?generation=1532415397328022&alt=media)\n",
    "\n",
    "- ___One to one___: one input, one output. V√≠ d·ª•: image classification,...\n",
    "- ___One to many___: one input, many output. V√≠ d·ª•: image captioning (input 1 image, output ra 1 sequence of text as image caption)\n",
    "- ___Many to one___: many input, one output. V√≠ d·ª•: text classification,...\n",
    "- ___Many to many___: many input, many output. V√≠ d·ª•: machine translation, speech to text,...\n",
    "\n",
    "V√≠ d·ª• __Many to many__:\n",
    "\n",
    "![](https://images.viblo.asia/4a1049be-e04c-482b-b8f4-775a7bd55c15.png)\n",
    "\n",
    "N·∫øu nh∆∞ m·∫°ng NN ch·ªâ l√† input_layer $x$ ƒëi qua hidden_layer $h$ v√† cho ra output_layer $y$ v·ªõi __fully connected__ gi·ªØa c√°c layers th√¨ trong RNN, c√°c input $x_t$ s·∫Ω ƒë∆∞·ª£c k·∫øt h·ª£p v·ªõi hidden layer $h_{t-1}$ ch·∫°y qua activation function $g_1$ ƒë·ªÉ t√≠nh to√°n ra hidden layer $h_t$. Th∆∞·ªùng h√†m $g_1$ l√† h√†m $\\tanh$ k·∫øt h·ª£p v·ªõi t·∫≠p h·ª£p c√°c tr·ªçng s·ªë W (t√≠nh l√† total Loss t·ª´ L1, L2,..Lt). Ngo√†i ra c√≤n c√≥ activation function $g_2$ khi t√≠nh to√°n output $y_t$.\n",
    "$$h_t = g_1(h_{t-1}, x_t) = \\tanh (W_{hh}h_{t-1} + W_{xh}x_{t} + b_h)$$\n",
    "T√≠nh output $y_t$:\n",
    "$$y_t = g_2(W_{hy}h_t + b_y)$$\n",
    "\n",
    "T·ªïng h·ª£p qu√° tr√¨nh t√≠nh to√°n ƒë∆∞·ª£c th·ªÉ hi·ªán:\n",
    "\n",
    "![](https://images.viblo.asia/4b1cc09d-99fa-422a-9bee-14908aace750.png)\n",
    "\n",
    "Trong m·∫°ng NN th√¨ ch·ªâ c√≥ 1 matrix $W$ duy nh·∫•t, nh∆∞ng trong m·∫°ng RNN th√¨ c√≥ 3 matrix tr·ªçng s·ªë:\n",
    "- $W_{hh}$: l√† matrix tr·ªçng s·ªë c·ªßa \"b·ªô nh·ªõ tr∆∞·ªõc\" $h_{t-1}$\n",
    "- $W_{xh}$: l√† matrix tr·ªçng s·ªë c·ªßa \"input hi·ªán t·∫°i\" $x_t$\n",
    "- $W_{hy}$: l√† matrix tr·ªçng s·ªë c·ªßa \"b·ªô nh·ªõ hi·ªán t·∫°i\" $h_t$ ƒë·ªÉ t·∫°o ra output $y_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of RNNs\n",
    "\n",
    "| Lo·∫°i network | minh ho·∫° | ·ª©ng d·ª•ng  | \n",
    "| --------- | ------ |------ |\n",
    "| One-to-One | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-one-to-one-ltr.png?9c8e3b04d222d178d6bee4506cc3f779) | Traditional Neral network |\n",
    "| One-to-many | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-one-to-many-ltr.png?d246c2f0d1e0f43a21a8bd95f579cb3b) | Music generation |\n",
    "| Many-to-one | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-one-ltr.png?c8a442b3ea9f4cb81f929c089b910c9d) |  Sentiment classification |\n",
    "| Many-to-many | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-same-ltr.png?2790431b32050b34b80011afead1f232) |  Name entity recognition |\n",
    "| Many-to-many | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-different-ltr.png?8ca8bafd1eeac4e8c961d9293858407b) | Machine translation |\n",
    "\n",
    "RNN cho ph√©p ta d·ª± ƒëo√°n x√°c su·∫•t c·ªßa m·ªôt t·ª´ m·ªõi nh·ªù v√†o c√°c t·ª´ ƒë√£ bi·∫øt li·ªÅn tr∆∞·ªõc n√≥. C∆° ch·∫ø n√†y ho·∫°t ƒë·ªông gi·ªëng v·ªõi v√≠ d·ª• b√™n tr√™n, v·ªõi c√°c ƒë·∫ßu ra c·ªßa c·ª•m n√†y s·∫Ω l√† ƒë·∫ßu v√†o c·ªßa c·ª•m ti·∫øp theo cho ƒë·∫øn khi ta ƒë∆∞·ª£c m·ªôt c√¢u ho√†n ch·ªânh. C√°c input th∆∞·ªùng ƒë∆∞·ª£c encode d∆∞·ªõi d·∫°ng 1 vector one hot encoding. V√≠ d·ª• v·ªõi t·∫≠p dataset g·ªìm 50000 c√¢u ta l·∫•y ra ƒë∆∞·ª£c m·ªôt dictionary g·ªìm 4000 t·ª´, t·ª´ \"hot\" n·∫±m ·ªü v·ªã tr√≠ 128 th√¨ vector one hot c·ªßa t·ª´ \"hot\" s·∫Ω l√† m·ªôt vector g·ªìm 4000 ph·∫ßn t·ª≠ ƒë·ªÅu b·∫±ng 0 ch·ªâ c√≥ duy nh·∫•t v·ªã tr√≠ 128 b·∫±ng 1. M√¥ h√¨nh n√†y n√†y ch√≠nh l√† m√¥ h√¨nh Many to Many v·ªõi s·ªë l∆∞·ª£ng ƒë·∫ßu ra, ƒë·∫ßu v√†o v√† l·ªõp ·∫©n b·∫±ng nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Loss function c·ªßa RNNs b·∫±ng t·ªïng loss t·∫°i m·ªói output trong m·∫°ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling long term dependencies\n",
    "\n",
    "1. Activation Function\n",
    "\n",
    "C√°c h√†m activation function ph·ªï bi·∫øn trong RNNs l√† : __sigmoid, tanh, ReLU__\n",
    "\n",
    "2. Vanishing/exploding gradient\n",
    "\n",
    "Hi·ªán t∆∞·ª£ng gradient bi·∫øn m·∫•t ho·∫∑c b√πng n·ªï th∆∞·ªùng xuy√™n x·∫£y ra trong m·∫°ng RNN, n√™n r·∫•t kh√≥ trong vi·ªác capture nh·ªØng y·∫øu t·ªë d√†i h·∫°n ph√≠a tr∆∞·ªõc ·∫£nh h∆∞·ªüng t·ªõi hi·ªán t·∫°i.\n",
    "\n",
    "__Gradient clipping__ th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ setting max value of gradient trong TH g·∫∑p ph·∫£i v·∫•n ƒë·ªÅ gradient exploding\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/gradient-clipping-en.png?6c3de441dc56aad634dc1a91accb48f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Tham s·ªë | LSTM | GRU  | \n",
    "| --------- | ------ |------ |\n",
    "| Minh ho·∫° | ![](https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_768/http://dprogrammer.org/wp-content/uploads/2019/04/LSTM-Core-768x466.png) | ![](https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_768/http://dprogrammer.org/wp-content/uploads/2019/04/GRU-768x502.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Lo·∫°i gate - state |  C√¥ng th·ª©c  | Vai tr√≤ | minh ho·∫° |\n",
    "| --------- | ------ |------ |------ |\n",
    "| __Forget gate__ $f_t$ | $$ f_t = \\text{sigmoid}(W_f.[h_{t-1}, x_t] + b_f) $$  |  Forget gate quy·∫øt ƒë·ªãnh th√¥ng tin n√†o t·ª´ b·ªô nh·ªõ d√†i h·∫°n ƒë∆∞·ª£c l∆∞u gi·ªØ ho·∫∑c lo·∫°i b·ªè | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C_{t}\\,\\longrightarrow\\,f_{t}\\,*\\,C_{t-1}\\,+\\,\\dot{\\iota}_{t}\\,*\\,\\widetilde C_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "[Long short-term memory cells (LSTMs)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) kh√°c RNN ·ªü ƒëi·ªÉm thay v√¨ 1 t·∫ßng m·∫°ng neural v·ªõi h√†m `tanh` th√¨ LSTM c√≥ 4 t√¢ng (4 c·ªïng gate) t∆∞∆°ng t√°c v·ªõi nhau, nh·ªù v·∫≠y m√† c√≥ th·ªÉ b·ªè ƒëi ho·∫∑c th√™m v√†o c√°c th√¥ng tin c·∫ßn thi·∫øt th√¥ng qua c√°c gate, m·ªôt n∆°i gi√∫p s√†ng l·ªçc th√¥ng tin v·ªõi activation l√† 1 h√†m sigmoid.\n",
    "> T·∫ßng sigmoid cho output trong kho·∫£ng [0,1], m√¥ t·∫£ c√≥ bao nhi√™u th√¥ng tin c√≥ th·ªÉ ƒë∆∞·ª£c th√¥ng qua. Khi output l√† 0 c√≥ nghƒ©a l√† kh√¥ng c√≥ th√¥ng tin n√†o, n·∫øu l√† 1 th√¨ c√≥ nghƒ©a cho t·∫•t c·∫£ c√°c th√¥ng tin ƒëi qua.\n",
    "\n",
    "![](https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_768/http://dprogrammer.org/wp-content/uploads/2019/04/LSTM-Core-768x466.png)\n",
    "\n",
    "\n",
    "| Lo·∫°i gate - state |  C√¥ng th·ª©c  | Vai tr√≤ | minh ho·∫° |\n",
    "| --------- | ------ |------ |------ |\n",
    "| __Forget gate__ $f_t$ | $$ f_t = \\text{sigmoid}(W_f.[h_{t-1}, x_t] + b_f) $$  |  Forget gate quy·∫øt ƒë·ªãnh th√¥ng tin n√†o t·ª´ b·ªô nh·ªõ d√†i h·∫°n ƒë∆∞·ª£c l∆∞u gi·ªØ ho·∫∑c lo·∫°i b·ªè | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png) |\n",
    "| __Input gate__ $i_t$ |  $$ i_t = \\text{sigmoid}(W_i.[h_{t-1}, x_t] + b_i) $$| C·ªïng ƒë·∫ßu v√†o quy·∫øt ƒë·ªãnh th√¥ng tin n√†o s·∫Ω ƒë∆∞·ª£c l∆∞u tr·ªØ trong b·ªô nh·ªõ d√†i h·∫°n. N√≥ ch·ªâ ho·∫°t ƒë·ªông v·ªõi th√¥ng tin t·ª´ ƒë·∫ßu v√†o hi·ªán t·∫°i v√† b·ªô nh·ªõ ng·∫Øn h·∫°n t·ª´ b∆∞·ªõc tr∆∞·ªõc. T·∫°i c·ªïng n√†y, n√≥ l·ªçc ra th√¥ng tin t·ª´ c√°c bi·∫øn kh√¥ng h·ªØu √≠ch | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png) |\n",
    "| __Output gate__ ($o_t$) |  $$ o_t = \\text{sigmoid}(W_o.[h_{t-1}, x_t] + b_o) $$ | Output gate s·ª≠ d·ª•ng $x_t$, $h_{t-1}$ v√† long-term memory m·ªõi v·ª´a ƒë∆∞·ª£c t√≠nh ƒë·ªÉ t·∫°o ra b·ªô l·ªçc cho short-term memory $h_t$ (ƒë·ªÉ d√πng trong next step) v√† c·∫•u ph·∫ßn ra the cell state $C_t$ (d√πng trong step hi·ªán t·∫°i) | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png) |\n",
    "| __Hidden state__ ($\\tilde{C}_{t}$) | $$ \\tilde{C}_{t} = \\text{tanh}(W_c.[h_{t-1}, x_t] + b_c) $$  |  Tr·∫°ng th√°i ·∫©n t·∫°m th·ªùi c·∫•u ph·∫ßn ra the cell state $C_t$ (d√πng trong step hi·ªán t·∫°i)  | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png) |\n",
    "| __Cell state__ ($C_{t}$) | $$ C_{t} = f_t*C_{t-1} + i_t*\\tilde{C}_{t} $$ | l√† b·ªô nh·ªõ trong c·ªßa LSTM ƒë∆∞·ª£c t·ªïng h·ª£p c·ªßa b·ªô nh·ªõ tr∆∞·ªõc $C_{t-1}$ ƒë√£ ƒë∆∞·ª£c l·ªçc qua forget gate $f_t$ v√† tr·∫°ng th√°i ·∫©n $\\tilde{C}_{t}$ ƒë√£ ƒë∆∞·ª£c l·ªçc qua Input gate $i_t$, t·ª´ ƒë√≥ c√°c th√¥ng tin quan tr·ªçng (__long-term memory__) s·∫Ω ƒë∆∞·ª£c ƒëi xa h∆°n v√† s·∫Ω ƒë∆∞·ª£c d√πng khi c·∫ßn | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png) |\n",
    "| __Short-term memory Output__ ($h_{t}$) | $$ h_{t} = o_t*\\text{tanh}({C}_{t}) $$  |  Cell state $C_{t}$ sau khi qua tanh activation s·∫Ω ƒë∆∞·ª£c l·ªçc 1 l·∫ßn n·ªØa qua Output gate $o_t$ t·∫°o ra output c·ªßa step. | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png) |\n",
    "\n",
    "- N·∫øu nh√¨n k·ªπ m·ªôt ch√∫t, ta c√≥ th·ªÉ th·∫•y RNN truy·ªÅn th·ªëng l√† d·∫°ng ƒë·∫∑c bi·ªát c·ªßa LSTM. N·∫øu thay gi√° tr·ªã ƒë·∫ßu ra c·ªßa input gate = 1 v√† ƒë·∫ßu ra forget gate = 0 (kh√¥ng nh·ªõ tr·∫°ng th√°i tr∆∞·ªõc)\n",
    "- LSTM c√≥ long-term memory. Tuy nhi√™n, LSTM kh√° gi·ªëng v·ªõi RNN truy·ªÅn th·ªëng, t·ª©c c√≥ short-term memory. Nh√¨n chung, LSTM gi·∫£i quy·∫øt ph·∫ßn n√†o vanishing gradient so v·ªõi RNN, nh∆∞ng ch·ªâ m·ªôt ph·∫ßn.\n",
    "- V·ªõi l∆∞·ª£ng t√≠nh to√°n nh∆∞ tr√™n, RNN ƒë√£ ch·∫≠m, LSTM nay c√≤n ch·∫≠m h∆°n.\n",
    "\n",
    "__Reference:__\n",
    "- https://dominhhai.github.io/vi/2017/10/what-is-lstm/#3-2-b√™n-trong-lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "Gated Recurrent Unit (GRU) l√† 1 TH ƒë·∫∑c bi·ªát c·ªßa LSTM. GRU s·ª≠ d·ª•ng less training parameter n√™n do ƒë√≥ s·ª≠ d·ª•ng less memory and executes faster than LSTM trong khi ƒë√≥ LSTM is more accurate on a larger dataset. \n",
    "- LSTM if you are dealing with large sequences and accuracy is concerned\n",
    "- GRU is used when you have less memory consumption and want faster results. \n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)\n",
    "\n",
    "GRU ch·ªâ c√≥ 2 c·ªïng: c·ªïng thi·∫øt l·∫≠p l·∫°i `r` v√† c·ªïng c·∫≠p nh·∫≠p `z`. C·ªïng thi·∫øt l·∫≠p l·∫°i s·∫Ω quy·∫øt ƒë·ªãnh c√°ch k·∫øt h·ª£p gi·ªØa ƒë·∫ßu v√†o hi·ªán t·∫°i v·ªõi b·ªô nh·ªõ tr∆∞·ªõc, c√≤n c·ªïng c·∫≠p nh·∫≠p s·∫Ω ch·ªâ ƒë·ªãnh c√≥ bao nhi√™u th√¥ng tin v·ªÅ b·ªô nh·ªõ tr∆∞·ªõc n√™n gi·ªØa l·∫°i. Nh∆∞ v·∫≠y RNN thu·∫ßn c≈©ng l√† m·ªôt d·∫°ng ƒë·∫∑c bi·ªát c·ªßa GRU, v·ªõi ƒë·∫ßu ra c·ªßa c·ªïng thi·∫øt l·∫≠p l·∫°i l√† 1 v√† c·ªïng c·∫≠p nh·∫≠p l√† 0.\n",
    "\n",
    "__What is the difference between GRU & LSTM?__\n",
    "- The GRU has 2 gates, LSTM has 3 gates\n",
    "- GRU kh√¥ng c√≥ b·ªô nh·ªõ trong v√† output gate nh∆∞ LSTM\n",
    "- 2 c·ªïng v√†o v√† c·ªïng qu√™n ƒë∆∞·ª£c k·∫øt h·ª£p l·∫°i th√†nh c·ªïng c·∫≠p nh·∫≠p z v√† c·ªïng thi·∫øt l·∫≠p l·∫°i r s·∫Ω ƒë∆∞·ª£c √°p d·ª•ng tr·ª±c ti·∫øp cho tr·∫°ng th√°i ·∫©n tr∆∞·ªõc.\n",
    "- GRU kh√¥ng s·ª≠ d·ª•ng m·ªôt h√†m phi tuy·∫øn t√≠nh ƒë·ªÉ t√≠nh ƒë·∫ßu ra nh∆∞ LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional (BRNN)\n",
    "\n",
    "| BRNN |  √°p d·ª•ng  |  minh ho·∫° |\n",
    "| --------- | ------ |------ |\n",
    "| BRNN kh√°c RNN m·ªôt ƒëi·ªÉm l√† thay v√¨ process 1 sequense t·ª´ tr√°i ---> ph·∫£i th√¨ BRNN process 2 chi·ªÅu (th√™m c·∫£ t·ª´ ph·∫£i ---> tr√°i) | Vi·ªác ph√¢n t√≠ch c√¢u c·∫£ 2 chi·ªÅu c√≥ kh·∫£ nƒÉng c·∫£i thi·ªán performance tuy nhi√™n chi ph√≠ training v√† s·ªë l∆∞·ª£ng c√°c tham s·ªë s·∫Ω ph·∫£i x2  | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/bidirectional-rnn-ltr.png?e3e66fae56ea500924825017917b464a) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep (DRNN)\n",
    "\n",
    "| DRNN |  √°p d·ª•ng  |  minh ho·∫° |\n",
    "| --------- | ------ |------ |\n",
    "| DRNN | DRNN | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/deep-rnn-ltr.png?f57da6de44ddd4709ad3b696cac6a912) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "\n",
    "def get_performance(y_val, y_pred, model_name=\"baseline\"):\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        y_val, y_pred\n",
    "    )\n",
    "    class_name = [\"class0\", \"class1\"]\n",
    "    df = pd.DataFrame(\n",
    "        [precision, recall, fscore],\n",
    "        columns=class_name,\n",
    "        index=[\"precision\", \"recall\", \"fscore\"],\n",
    "    )\n",
    "    df.loc[\"accuracy\"] = accuracy_score(y_val, y_pred)\n",
    "    df = df.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "    df[\"model_name\"] = model_name\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model0: baseline model\n",
    "S·ª≠ d·ª•ng [TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) formula ƒë·ªÉ convert words sang d·∫°ng numeric v√† model ch√∫ng b·∫±ng [Multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB), model n√†y th∆∞·ªùng ƒë∆∞·ª£c refering cho c√°c model d·ªØ li·ªáu text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84       435\n",
      "           1       0.83      0.69      0.76       327\n",
      "\n",
      "    accuracy                           0.81       762\n",
      "   macro avg       0.81      0.79      0.80       762\n",
      "weighted avg       0.81      0.81      0.80       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# create model pipeline\n",
    "model_0 = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),  # convert words to numbers using TF-IDF\n",
    "        (\"clf\", MultinomialNB()),  # model the text\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit model\n",
    "model_0.fit(x_train, y_train)\n",
    "\n",
    "# evaluate our model\n",
    "y_pred = model_0.predict(x_val)\n",
    "model_0_res = get_performance(y_val, y_pred, model_name=\"0_baseline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1: simple dense\n",
    "V·ªõi d·ªØ li·ªáu ƒë·∫ßu v√†o, ta th·ª±c hi·ªán theo c√°c b∆∞·ªõc\n",
    "```\n",
    "tokenization ---> embedding ---> average pooling ---> fully connected dence with sigmoid activation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_model(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_1_dense\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = build_model(10000, 15)\n",
    "model_1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.keras import TqdmCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.6289 - accuracy: 0.6509 - val_loss: 0.5621 - val_accuracy: 0.7493\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.4534 - accuracy: 0.8206 - val_loss: 0.4598 - val_accuracy: 0.7927\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.3400 - accuracy: 0.8673 - val_loss: 0.4364 - val_accuracy: 0.8045\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2745 - accuracy: 0.8956 - val_loss: 0.4397 - val_accuracy: 0.7913\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2277 - accuracy: 0.9177 - val_loss: 0.4600 - val_accuracy: 0.7953\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       435\n",
      "           1       0.80      0.69      0.74       327\n",
      "\n",
      "    accuracy                           0.80       762\n",
      "   macro avg       0.80      0.78      0.79       762\n",
      "weighted avg       0.80      0.80      0.79       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_1.predict(x_val).round().squeeze()\n",
    "model_1_res = get_performance(y_val, y_pred, model_name=\"1_simple_dence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding vector and metadata\n",
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\n",
    "    \"Datasets/nlp_getting_started/embedding_vectors.tsv\", \"w\", encoding=\"utf-8\"\n",
    ")\n",
    "out_m = io.open(\n",
    "    \"Datasets/nlp_getting_started/embedding_metadata.tsv\",\n",
    "    \"w\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "# get vocab and embedding_weight from model\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "words_in_vocab = model_1.get_layer(\"text_vectorization_1\").get_vocabulary()\n",
    "\n",
    "\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "    if num == 0:\n",
    "        continue  # skip padding token\n",
    "    vec = embed_weights[num]\n",
    "    out_m.write(word + \"\\n\")  # write words to file\n",
    "    out_v.write(\n",
    "        \"\\t\".join([str(x) for x in vec]) + \"\\n\"\n",
    "    )  # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2: LSTM model\n",
    "\n",
    "[`tensorflow.keras.layers.LSTM()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_lstm(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = keras.layers.LSTM(64)(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_2 = build_lstm(10000, 15)\n",
    "model_2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 10s 44ms/step - loss: 0.5158 - accuracy: 0.7396 - val_loss: 0.4424 - val_accuracy: 0.8071\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2985 - accuracy: 0.8824 - val_loss: 0.4780 - val_accuracy: 0.7835\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 16ms/step - loss: 0.1931 - accuracy: 0.9295 - val_loss: 0.5891 - val_accuracy: 0.7559\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1324 - accuracy: 0.9486 - val_loss: 0.7432 - val_accuracy: 0.7835\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0908 - accuracy: 0.9631 - val_loss: 0.7791 - val_accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "model_2_history = model_2.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       435\n",
      "           1       0.74      0.68      0.71       327\n",
      "\n",
      "    accuracy                           0.76       762\n",
      "   macro avg       0.76      0.75      0.75       762\n",
      "weighted avg       0.76      0.76      0.76       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_2.predict(x_val).round().squeeze()\n",
    "model_2_res = get_performance(y_val, y_pred, model_name=\"2_lstm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3: GRU model\n",
    "\n",
    "[`tensorflow.keras.layers.GRU()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers_GRU -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_gru(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = keras.layers.GRU(64)(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_3_GRU\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_3 = build_gru(10000, 15)\n",
    "model_3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 24ms/step - loss: 0.6505 - accuracy: 0.6179 - val_loss: 0.5060 - val_accuracy: 0.7690\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3814 - accuracy: 0.8348 - val_loss: 0.4461 - val_accuracy: 0.7979\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.2317 - accuracy: 0.9066 - val_loss: 0.5470 - val_accuracy: 0.7848\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1470 - accuracy: 0.9448 - val_loss: 0.7074 - val_accuracy: 0.7520\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1061 - accuracy: 0.9594 - val_loss: 0.7763 - val_accuracy: 0.7559\n"
     ]
    }
   ],
   "source": [
    "model_3_history = model_3.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.79       435\n",
      "           1       0.74      0.66      0.70       327\n",
      "\n",
      "    accuracy                           0.76       762\n",
      "   macro avg       0.75      0.74      0.75       762\n",
      "weighted avg       0.75      0.76      0.75       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_3.predict(x_val).round().squeeze()\n",
    "model_3_res = get_performance(y_val, y_pred, model_name=\"3_GRU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model4: Bidirectonal RNN\n",
    "\n",
    "[`tensorflow.keras.layers.Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) l√† m·ªôt c√°ch ti·∫øp c·∫≠n training 2 chi·ªÅu thay v√¨ m·ªôt chi·ªÅu, do ƒë√≥ c√≥ th·ªÉ wrap v·ªõi b·∫•t k·ª≥ layers n√†o n·∫øu mu·ªën training 2 chi·ªÅu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 25ms/step - loss: 0.6467 - accuracy: 0.6189 - val_loss: 0.4780 - val_accuracy: 0.8005\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3829 - accuracy: 0.8339 - val_loss: 0.4440 - val_accuracy: 0.8031\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.2445 - accuracy: 0.9053 - val_loss: 0.5274 - val_accuracy: 0.7992\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1554 - accuracy: 0.9441 - val_loss: 0.6703 - val_accuracy: 0.7546\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1085 - accuracy: 0.9609 - val_loss: 0.7498 - val_accuracy: 0.7690\n",
      "24/24 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80       435\n",
      "           1       0.75      0.69      0.72       327\n",
      "\n",
      "    accuracy                           0.77       762\n",
      "   macro avg       0.77      0.76      0.76       762\n",
      "weighted avg       0.77      0.77      0.77       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_gru_bidir(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = layers.Bidirectional(keras.layers.GRU(64))(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_4_GRU_bidir\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_4 = build_gru(10000, 15)\n",
    "# model_4.summary()\n",
    "model_4_history = model_4.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n",
    "# prediction and evaluation\n",
    "y_pred = model_4.predict(x_val).round().squeeze()\n",
    "model_4_res = get_performance(y_val, y_pred, model_name=\"model_4_GRU_bidir\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model5: Convolutional NN for text\n",
    "\n",
    "Khi s·ª≠ d·ª•ng [__Convolutional layers__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) cho d·ªØ li·ªáu text (sequences) th√¨ ƒëi·ªÉm kh√°c bi·ªát ch√≠nh l√† s·ªë chi·ªÅu s·∫Ω l√† 1 (thay v√¨ D = 2 khi s·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng image)\n",
    "\n",
    "C√°c b∆∞·ªõc ch√≠nh trong vi√™c s·ª≠ d·ª•ng CNN for text data: ([chi ti·∫øt t·∫°i Understanding Convolutional Neural Networks for Text Classification](https://www.aclweb.org/anthology/W18-5408.pdf))\n",
    "1. S·ª≠ d·ª•ng `Conv1D()` ƒë·ªÉ filter b·∫±ng __ngram detectors__, m·ªói 1 filter c·ª• th·ªÉ s·∫Ω t·∫°o ƒë·∫∑c ƒëi·ªÉm g·∫ßn nh·∫•t v·ªõi 1 h·ªç ngrams\n",
    "> an ngram l√† 1 collection of n-words\n",
    "2. S·ª≠ d·ª•ng Maxpooling trong su·ªët qu√° tr√¨nh extracts nh·ªØng ngrams c√≥ li√™n quan ƒë·ªÉ ph·ª•c v·ª• vi·ªác ra quy√©t ƒë·ªãnh\n",
    "3. Model s·∫Ω ph√¢n lo·∫°i d·ª±a tr√™n c√°c th√¥ng tin sau l·ªõp maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,300,545\n",
      "Trainable params: 1,300,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_cnn_text(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(\n",
    "        x\n",
    "    )  # s·ª≠ d·ª•ng ngram v·ªõi n = 5 words\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_5 = build_cnn_text(10000, 15)\n",
    "model_5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.5586 - accuracy: 0.7254 - val_loss: 0.4585 - val_accuracy: 0.7913\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.3250 - accuracy: 0.8678 - val_loss: 0.4647 - val_accuracy: 0.7940\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.1881 - accuracy: 0.9355 - val_loss: 0.5576 - val_accuracy: 0.7940\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.1173 - accuracy: 0.9581 - val_loss: 0.6164 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.0837 - accuracy: 0.9699 - val_loss: 0.6956 - val_accuracy: 0.7703\n"
     ]
    }
   ],
   "source": [
    "model_5_history = model_5.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       435\n",
      "           1       0.74      0.72      0.73       327\n",
      "\n",
      "    accuracy                           0.77       762\n",
      "   macro avg       0.77      0.76      0.76       762\n",
      "weighted avg       0.77      0.77      0.77       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_5.predict(x_val).round().squeeze()\n",
    "model_5_res = get_performance(y_val, y_pred, model_name=\"5_Conv1D\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model6: TensorFlow Hub Pretrained Sentence Encoder\n",
    "\n",
    "the __Universal Sentence Encoder__ embedding a whole sentence-level (thay v√¨ word-level nh∆∞ layer Embedding ph√≠a tr√™n), v·ªõi m·ªói sentence ƒë∆∞·ª£c encode th√†nh vector c√≥ 512 dimentional\n",
    "\n",
    "> üîë Note: An __encoder__ is the name for a model which converts raw data such as text into a numerical representation (feature vector), a __decoder__ converts the numerical representation to a desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 20s 90ms/step - loss: 0.5124 - accuracy: 0.7719 - val_loss: 0.4251 - val_accuracy: 0.8058\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 12s 56ms/step - loss: 0.4283 - accuracy: 0.8067 - val_loss: 0.4156 - val_accuracy: 0.8071\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 11s 50ms/step - loss: 0.4179 - accuracy: 0.8129 - val_loss: 0.4206 - val_accuracy: 0.8097\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 10s 48ms/step - loss: 0.4103 - accuracy: 0.8161 - val_loss: 0.4189 - val_accuracy: 0.8123\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 10s 47ms/step - loss: 0.4049 - accuracy: 0.8196 - val_loss: 0.4201 - val_accuracy: 0.8045\n",
      "24/24 [==============================] - 4s 155ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       435\n",
      "           1       0.77      0.78      0.77       327\n",
      "\n",
      "    accuracy                           0.80       762\n",
      "   macro avg       0.80      0.80      0.80       762\n",
      "weighted avg       0.80      0.80      0.80       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_USE():\n",
    "    # We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "    sentence_encoder_layer = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "        input_shape=[],  # shape of inputs coming to our model\n",
    "        dtype=tf.string,  # data type of inputs coming to the USE layer\n",
    "        trainable=False,  # keep the pretrained weights (we'll create a feature extractor)\n",
    "        name=\"USE\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = tf.keras.layers.Input(shape=[], dtype=\"string\")\n",
    "    x = sentence_encoder_layer(inputs)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_6_USE\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_6 = build_USE()\n",
    "model_6_history = model_6.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n",
    "# prediction and evaluation\n",
    "y_pred = model_6.predict(x_val).round().squeeze()\n",
    "model_6_res = get_performance(y_val, y_pred, model_name=\"model_6_USE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model7: Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7_Transformers\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " keras_layer_3 (KerasLayer)  (None, 50)                48190600  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                3264      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,193,929\n",
      "Trainable params: 48,193,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 0.5330 - accuracy: 0.7453 - val_loss: 0.4240 - val_accuracy: 0.8228\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 12s 58ms/step - loss: 0.3394 - accuracy: 0.8568 - val_loss: 0.4291 - val_accuracy: 0.8228\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 12s 58ms/step - loss: 0.2187 - accuracy: 0.9164 - val_loss: 0.5073 - val_accuracy: 0.7874\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 12s 58ms/step - loss: 0.1378 - accuracy: 0.9512 - val_loss: 0.6148 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 13s 58ms/step - loss: 0.0953 - accuracy: 0.9667 - val_loss: 0.6857 - val_accuracy: 0.7808\n",
      "24/24 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_performance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# prediction and evaluation\u001b[39;00m\n",
      "\u001b[1;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_7\u001b[38;5;241m.\u001b[39mpredict(x_val)\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;32m---> 24\u001b[0m model_7_res \u001b[38;5;241m=\u001b[39m \u001b[43mget_performance\u001b[49m(y_val, y_pred, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_7_Transformers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_performance' is not defined"
     ]
    }
   ],
   "source": [
    "def build_tfm():\n",
    "    embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "    hub_layer = hub.KerasLayer(\n",
    "        embedding, input_shape=[], dtype=tf.string, trainable=True\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = tf.keras.layers.Input(shape=[], dtype=\"string\")\n",
    "    x = hub_layer(inputs)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_7_Transformers\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "model_7 = build_tfm()\n",
    "model_7_history = model_7.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n",
    "# prediction and evaluation\n",
    "y_pred = model_7.predict(x_val).round().squeeze()\n",
    "model_7_res = get_performance(y_val, y_pred, model_name=\"model_7_Transformers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "res = pd.concat(\n",
    "    [\n",
    "        model_0_res,\n",
    "        model_1_res,\n",
    "        model_2_res,\n",
    "        model_3_res,\n",
    "        model_4_res,\n",
    "        model_5_res,\n",
    "        model_6_res,\n",
    "        model_7_res,\n",
    "    ]\n",
    ")\n",
    "fig = px.bar(\n",
    "    res,\n",
    "    y=\"class1\",\n",
    "    x=\"model_name\",\n",
    "    color=\"metric\",\n",
    "    barmode=\"group\",\n",
    "    range_y=(0.6, 1),\n",
    "    text_auto=\".2f\",\n",
    ")\n",
    "fig.update_traces(\n",
    "    textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../_images/comparisation_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining our models (model ensembling/stacking)\n",
    "\n",
    "ref: [__Chapter 6 of the Machine Learning Engineering Book__](http://www.mlebook.com/wiki/doku.php)\n",
    "\n",
    "S·ª≠ d·ª•ng ensemble ƒë·ªÉ k·∫øt h·ª£p nhi·ªÅu model ƒë·ªÉ make prediction v·ªõi ƒëi·ªÅu ki·ªán l√† c√°c models c√≥ t√≠nh ch·∫•t __uncorrelated__ v·ªõi nhau, hay n√≥i c√°ch kh√°c l√† m·ªói model c√≥ 1 c√°ch ti·∫øp c·∫≠n/ki·∫øn tr√∫c m·∫°ng kh√°c nhau, c√°ch t√¨m ra patterns kh√°c nhau.\n",
    "\n",
    "C√°c ph∆∞∆°ng ph√°p combine output:\n",
    "1. average the probabilities\n",
    "2. Majority vote\n",
    "3. Model stacking: s·ª≠ d·ª•ng output c·ªßa model n√†y ƒë·ªÉ l√†m input cho model kh√°c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use H5 format\n",
    "model_6.save(\"models/NLP/H5/model_6.h5\")\n",
    "\n",
    "# Load model with custom Hub Layer (required with HDF5 format)\n",
    "# do model 6 s·ª≠ d·ª•ng c·∫•u tr√∫c t·ª´ ngu·ªìn ngo√†i n√™n ph·∫£i khai b√°o custom_objects\n",
    "loaded_model_6 = tf.keras.models.load_model(\n",
    "    \"models/NLP/H5/model_6.h5\", custom_objects={\"KerasLayer\": hub.KerasLayer}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most wrong examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 3s 107ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>make sure evacuate past fire doors questions y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>foodscare offersgo nestleindia slips loss magg...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>phiddleface theres choking hazard dont die get</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>ruin life</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>blazing elwoods blazingelwoods bother doug son...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  pred  \\\n",
       "3289  make sure evacuate past fire doors questions y...       0   0.0   \n",
       "4221  foodscare offersgo nestleindia slips loss magg...       1   1.0   \n",
       "4186     phiddleface theres choking hazard dont die get       0   0.0   \n",
       "5873                                          ruin life       0   0.0   \n",
       "706   blazing elwoods blazingelwoods bother doug son...       0   0.0   \n",
       "\n",
       "      pred_prob  \n",
       "3289   0.474197  \n",
       "4221   0.526637  \n",
       "4186   0.062413  \n",
       "5873   0.047390  \n",
       "706    0.036600  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dataframe with validation sentences and best performing model predictions\n",
    "y_pred = model_6.predict(x_val)\n",
    "val_df = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": x_val,\n",
    "        \"target\": y_val,\n",
    "        \"pred\": y_pred.round().squeeze(),\n",
    "        \"pred_prob\": y_pred.squeeze(),\n",
    "    }\n",
    ")\n",
    "val_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>general news uae demolition houses waterways b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.921028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>alaska wolves face catastrophe denali wolves p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>madonnamking rspca site multiple story high ri...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3821</th>\n",
       "      <td>juneau empire first responders turn national n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>government concerned population explosion popu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>could die falling sinkhole still blamed</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>fredolsencruise please take faroeislands itine...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.852597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>steveycheese mapmyrun electrocuted way round m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>plan emergency preparedness families children ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>nikostar lakes ohio thought abject desolation ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  pred  \\\n",
       "2345  general news uae demolition houses waterways b...       0   1.0   \n",
       "1491  alaska wolves face catastrophe denali wolves p...       0   1.0   \n",
       "3991  madonnamking rspca site multiple story high ri...       0   1.0   \n",
       "3821  juneau empire first responders turn national n...       0   1.0   \n",
       "3506  government concerned population explosion popu...       0   1.0   \n",
       "6070            could die falling sinkhole still blamed       0   1.0   \n",
       "4832  fredolsencruise please take faroeislands itine...       0   1.0   \n",
       "3111  steveycheese mapmyrun electrocuted way round m...       0   1.0   \n",
       "3193  plan emergency preparedness families children ...       0   1.0   \n",
       "2525  nikostar lakes ohio thought abject desolation ...       0   1.0   \n",
       "\n",
       "      pred_prob  \n",
       "2345   0.921028  \n",
       "1491   0.913840  \n",
       "3991   0.907247  \n",
       "3821   0.894248  \n",
       "3506   0.891692  \n",
       "6070   0.872665  \n",
       "4832   0.852597  \n",
       "3111   0.829169  \n",
       "3193   0.815358  \n",
       "2525   0.805084  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the wrong predictions and sort by prediction probabilities\n",
    "most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\n",
    "    \"pred_prob\", ascending=False\n",
    ")\n",
    "most_wrong[:10]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
