{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Techniques Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN's)\n",
    "\n",
    "Khi đọc và hiểu 1 words thì để hiểu từ đó trong cả câu/ngữ cảnh đó, hay nói cách khác cần phải hiểu bối cảnh của câu hoặc các words trước đó. Ví dụ chúng ta có 2 câu là \"Bố ăn tối chưa ?\" và \"Bố chưa ăn tối\". Mặc dù cả 2 sequence đều có các từ giống nhau nhưng khác nhau về mặt ý nghĩa. Thứ tự các words quyết định ý nghĩa của sentence.\n",
    "\n",
    "Khi RNN looks at 1 sequence of text (dưới dạng numeric), các pattern sẽ được học 1 cách liên tục dựa vào order của sequence đó. RNN's là mạng đặc trưng bởi việc lấy những input(x) + previous inputs để compute Y, helpful when dealing with sequences data, such as natural language text.\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc)\n",
    "\n",
    "Với mỗi timestep t, the activation $a^{<t>}$ và output $y^{t}$ được tính theo công thức\n",
    "> $$a^{<t>}=g_{1}(W_{a a}a^{<t-1>}+W_{a x}x^{<t>}+b_{a})$$\n",
    "\n",
    "> $$y^{<t>}=g_{2}(W_{y a}a^{<t>}+b_{y})$$\n",
    "\n",
    "Trong đó các hệ số `W` và `b` là các weights sẽ được update trong quá trình learn, còn `g1`, `g2` là các hàm activation functions\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn-ltr.png?74e25518f882f8758439bcb3637715e5)\n",
    "\n",
    "__Ưu điểm của RNN__:\n",
    "- Xử lý input có bất kỳ độ dài như thế nào\n",
    "- Model size not increasing with size of input\n",
    "- Computation takes into account history information\n",
    "- Weights được share theo thời gian\n",
    "\n",
    "__Hạn chế của RNN__:\n",
    "- Phải thực hiện tuần tự data, nên ko tuận dụng được sức mạnh tính toán song song (CPU/GPU), tính toán lâu\n",
    "- Khó trong việc accessing những long-history information vào thời điểm hiện tại\n",
    "- Cannot consider any future input for the current state\n",
    "- Vanishing gradient: do các hàm activation trong RNN thường là `tanh` (có output y [-1,1] và đạo hàm [0,1]) và `sigmoid` (có output y [0,1] và đạo hàm [0,0.25]), rất dễ gây ra đạo hàm = 0 với các giá trị activation_input lớn khiến các weights phía xa đều không được update, tức là các node phía xa không còn tác dụng nhiều tới node hiện tại nữa. Có một số cách khắc phục bằng việc:\n",
    "    - Sử dụng activation là __ReLU__ hoặc các biến thể\n",
    "    - Sử dụng 1 số mạng biến thể như __GRU__ hay __LSTM__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Một số các triển khai của RNN's\n",
    "\n",
    "![](https://3863425935-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LIA3amopGH9NC6Rf0mA%2F-LIA3mTJltflw3MVKAEQ%2F-LIA3nSKrqNJpLgASeso%2Fsequence.png?generation=1532415397328022&alt=media)\n",
    "\n",
    "- ___One to one___: one input, one output. Ví dụ: image classification,...\n",
    "- ___One to many___: one input, many output. Ví dụ: image captioning (input 1 image, output ra 1 sequence of text as image caption)\n",
    "- ___Many to one___: many input, one output. Ví dụ: text classification,...\n",
    "- ___Many to many___: many input, many output. Ví dụ: machine translation, speech to text,...\n",
    "\n",
    "Ví dụ __Many to many__:\n",
    "\n",
    "![](https://images.viblo.asia/4a1049be-e04c-482b-b8f4-775a7bd55c15.png)\n",
    "\n",
    "Nếu như mạng NN chỉ là input_layer $x$ đi qua hidden_layer $h$ và cho ra output_layer $y$ với __fully connected__ giữa các layers thì trong RNN, các input $x_t$ sẽ được kết hợp với hidden layer $h_{t-1}$ chạy qua activation function $g_1$ để tính toán ra hidden layer $h_t$. Thường hàm $g_1$ là hàm $\\tanh$ kết hợp với tập hợp các trọng số W (tính là total Loss từ L1, L2,..Lt). Ngoài ra còn có activation function $g_2$ khi tính toán output $y_t$.\n",
    "$$h_t = g_1(h_{t-1}, x_t) = \\tanh (W_{hh}h_{t-1} + W_{xh}x_{t} + b_h)$$\n",
    "Tính output $y_t$:\n",
    "$$y_t = g_2(W_{hy}h_t + b_y)$$\n",
    "\n",
    "Tổng hợp quá trình tính toán được thể hiện:\n",
    "\n",
    "![](https://images.viblo.asia/4b1cc09d-99fa-422a-9bee-14908aace750.png)\n",
    "\n",
    "Trong mạng NN thì chỉ có 1 matrix $W$ duy nhất, nhưng trong mạng RNN thì có 3 matrix trọng số:\n",
    "- $W_{hh}$: là matrix trọng số của \"bộ nhớ trước\" $h_{t-1}$\n",
    "- $W_{xh}$: là matrix trọng số của \"input hiện tại\" $x_t$\n",
    "- $W_{hy}$: là matrix trọng số của \"bộ nhớ hiện tại\" $h_t$ để tạo ra output $y_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of RNNs\n",
    "\n",
    "| Loại network | minh hoạ | ứng dụng  | \n",
    "| --------- | ------ |------ |\n",
    "| One-to-One | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-one-to-one-ltr.png?9c8e3b04d222d178d6bee4506cc3f779) | Traditional Neral network |\n",
    "| One-to-many | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-one-to-many-ltr.png?d246c2f0d1e0f43a21a8bd95f579cb3b) | Music generation |\n",
    "| Many-to-one | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-one-ltr.png?c8a442b3ea9f4cb81f929c089b910c9d) |  Sentiment classification |\n",
    "| Many-to-many | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-same-ltr.png?2790431b32050b34b80011afead1f232) |  Name entity recognition |\n",
    "| Many-to-many | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-different-ltr.png?8ca8bafd1eeac4e8c961d9293858407b) | Machine translation |\n",
    "\n",
    "RNN cho phép ta dự đoán xác suất của một từ mới nhờ vào các từ đã biết liền trước nó. Cơ chế này hoạt động giống với ví dụ bên trên, với các đầu ra của cụm này sẽ là đầu vào của cụm tiếp theo cho đến khi ta được một câu hoàn chỉnh. Các input thường được encode dưới dạng 1 vector one hot encoding. Ví dụ với tập dataset gồm 50000 câu ta lấy ra được một dictionary gồm 4000 từ, từ \"hot\" nằm ở vị trí 128 thì vector one hot của từ \"hot\" sẽ là một vector gồm 4000 phần tử đều bằng 0 chỉ có duy nhất vị trí 128 bằng 1. Mô hình này này chính là mô hình Many to Many với số lượng đầu ra, đầu vào và lớp ẩn bằng nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Loss function của RNNs bằng tổng loss tại mỗi output trong mạng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling long term dependencies\n",
    "\n",
    "1. Activation Function\n",
    "\n",
    "Các hàm activation function phổ biến trong RNNs là : __sigmoid, tanh, ReLU__\n",
    "\n",
    "2. Vanishing/exploding gradient\n",
    "\n",
    "Hiện tượng gradient biến mất hoặc bùng nổ thường xuyên xảy ra trong mạng RNN, nên rất khó trong việc capture những yếu tố dài hạn phía trước ảnh hưởng tới hiện tại.\n",
    "\n",
    "__Gradient clipping__ thường được sử dụng để setting max value of gradient trong TH gặp phải vấn đề gradient exploding\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/gradient-clipping-en.png?6c3de441dc56aad634dc1a91accb48f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Tham số | LSTM | GRU  | \n",
    "| --------- | ------ |------ |\n",
    "| Minh hoạ | ![](https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_768/http://dprogrammer.org/wp-content/uploads/2019/04/LSTM-Core-768x466.png) | ![](https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_768/http://dprogrammer.org/wp-content/uploads/2019/04/GRU-768x502.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Loại gate - state |  Công thức  | Vai trò | minh hoạ |\n",
    "| --------- | ------ |------ |------ |\n",
    "| __Forget gate__ $f_t$ | $$ f_t = \\text{sigmoid}(W_f.[h_{t-1}, x_t] + b_f) $$  |  Forget gate quyết định thông tin nào từ bộ nhớ dài hạn được lưu giữ hoặc loại bỏ | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C_{t}\\,\\longrightarrow\\,f_{t}\\,*\\,C_{t-1}\\,+\\,\\dot{\\iota}_{t}\\,*\\,\\widetilde C_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "[Long short-term memory cells (LSTMs)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) khác RNN ở điểm thay vì 1 tầng mạng neural với hàm `tanh` thì LSTM có 4 tâng (4 cổng gate) tương tác với nhau, nhờ vậy mà có thể bỏ đi hoặc thêm vào các thông tin cần thiết thông qua các gate, một nơi giúp sàng lọc thông tin với activation là 1 hàm sigmoid.\n",
    "> Tầng sigmoid cho output trong khoảng [0,1], mô tả có bao nhiêu thông tin có thể được thông qua. Khi output là 0 có nghĩa là không có thông tin nào, nếu là 1 thì có nghĩa cho tất cả các thông tin đi qua.\n",
    "\n",
    "![](https://sp-ao.shortpixel.ai/client/q_glossy,ret_img,w_768/http://dprogrammer.org/wp-content/uploads/2019/04/LSTM-Core-768x466.png)\n",
    "\n",
    "\n",
    "| Loại gate - state |  Công thức  | Vai trò | minh hoạ |\n",
    "| --------- | ------ |------ |------ |\n",
    "| __Forget gate__ $f_t$ | $$ f_t = \\text{sigmoid}(W_f.[h_{t-1}, x_t] + b_f) $$  |  Forget gate quyết định thông tin nào từ bộ nhớ dài hạn được lưu giữ hoặc loại bỏ | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png) |\n",
    "| __Input gate__ $i_t$ |  $$ i_t = \\text{sigmoid}(W_i.[h_{t-1}, x_t] + b_i) $$| Cổng đầu vào quyết định thông tin nào sẽ được lưu trữ trong bộ nhớ dài hạn. Nó chỉ hoạt động với thông tin từ đầu vào hiện tại và bộ nhớ ngắn hạn từ bước trước. Tại cổng này, nó lọc ra thông tin từ các biến không hữu ích | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png) |\n",
    "| __Output gate__ ($o_t$) |  $$ o_t = \\text{sigmoid}(W_o.[h_{t-1}, x_t] + b_o) $$ | Output gate sử dụng $x_t$, $h_{t-1}$ và long-term memory mới vừa được tính để tạo ra bộ lọc cho short-term memory $h_t$ (để dùng trong next step) và cấu phần ra the cell state $C_t$ (dùng trong step hiện tại) | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png) |\n",
    "| __Hidden state__ ($\\tilde{C}_{t}$) | $$ \\tilde{C}_{t} = \\text{tanh}(W_c.[h_{t-1}, x_t] + b_c) $$  |  Trạng thái ẩn tạm thời cấu phần ra the cell state $C_t$ (dùng trong step hiện tại)  | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png) |\n",
    "| __Cell state__ ($C_{t}$) | $$ C_{t} = f_t*C_{t-1} + i_t*\\tilde{C}_{t} $$ | là bộ nhớ trong của LSTM được tổng hợp của bộ nhớ trước $C_{t-1}$ đã được lọc qua forget gate $f_t$ và trạng thái ẩn $\\tilde{C}_{t}$ đã được lọc qua Input gate $i_t$, từ đó các thông tin quan trọng (__long-term memory__) sẽ được đi xa hơn và sẽ được dùng khi cần | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png) |\n",
    "| __Short-term memory Output__ ($h_{t}$) | $$ h_{t} = o_t*\\text{tanh}({C}_{t}) $$  |  Cell state $C_{t}$ sau khi qua tanh activation sẽ được lọc 1 lần nữa qua Output gate $o_t$ tạo ra output của step. | ![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png) |\n",
    "\n",
    "- Nếu nhìn kỹ một chút, ta có thể thấy RNN truyền thống là dạng đặc biệt của LSTM. Nếu thay giá trị đầu ra của input gate = 1 và đầu ra forget gate = 0 (không nhớ trạng thái trước)\n",
    "- LSTM có long-term memory. Tuy nhiên, LSTM khá giống với RNN truyền thống, tức có short-term memory. Nhìn chung, LSTM giải quyết phần nào vanishing gradient so với RNN, nhưng chỉ một phần.\n",
    "- Với lượng tính toán như trên, RNN đã chậm, LSTM nay còn chậm hơn.\n",
    "\n",
    "__Reference:__\n",
    "- https://dominhhai.github.io/vi/2017/10/what-is-lstm/#3-2-bên-trong-lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "Gated Recurrent Unit (GRU) là 1 TH đặc biệt của LSTM. GRU sử dụng less training parameter nên do đó sử dụng less memory and executes faster than LSTM trong khi đó LSTM is more accurate on a larger dataset. \n",
    "- LSTM if you are dealing with large sequences and accuracy is concerned\n",
    "- GRU is used when you have less memory consumption and want faster results. \n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)\n",
    "\n",
    "GRU chỉ có 2 cổng: cổng thiết lập lại `r` và cổng cập nhập `z`. Cổng thiết lập lại sẽ quyết định cách kết hợp giữa đầu vào hiện tại với bộ nhớ trước, còn cổng cập nhập sẽ chỉ định có bao nhiêu thông tin về bộ nhớ trước nên giữa lại. Như vậy RNN thuần cũng là một dạng đặc biệt của GRU, với đầu ra của cổng thiết lập lại là 1 và cổng cập nhập là 0.\n",
    "\n",
    "__What is the difference between GRU & LSTM?__\n",
    "- The GRU has 2 gates, LSTM has 3 gates\n",
    "- GRU không có bộ nhớ trong và output gate như LSTM\n",
    "- 2 cổng vào và cổng quên được kết hợp lại thành cổng cập nhập z và cổng thiết lập lại r sẽ được áp dụng trực tiếp cho trạng thái ẩn trước.\n",
    "- GRU không sử dụng một hàm phi tuyến tính để tính đầu ra như LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional (BRNN)\n",
    "\n",
    "| BRNN |  áp dụng  |  minh hoạ |\n",
    "| --------- | ------ |------ |\n",
    "| BRNN khác RNN một điểm là thay vì process 1 sequense từ trái ---> phải thì BRNN process 2 chiều (thêm cả từ phải ---> trái) | Việc phân tích câu cả 2 chiều có khả năng cải thiện performance tuy nhiên chi phí training và số lượng các tham số sẽ phải x2  | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/bidirectional-rnn-ltr.png?e3e66fae56ea500924825017917b464a) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep (DRNN)\n",
    "\n",
    "| DRNN |  áp dụng  |  minh hoạ |\n",
    "| --------- | ------ |------ |\n",
    "| DRNN | DRNN | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/deep-rnn-ltr.png?f57da6de44ddd4709ad3b696cac6a912) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "\n",
    "def get_performance(y_val, y_pred, model_name=\"baseline\"):\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        y_val, y_pred\n",
    "    )\n",
    "    class_name = [\"class0\", \"class1\"]\n",
    "    df = pd.DataFrame(\n",
    "        [precision, recall, fscore],\n",
    "        columns=class_name,\n",
    "        index=[\"precision\", \"recall\", \"fscore\"],\n",
    "    )\n",
    "    df.loc[\"accuracy\"] = accuracy_score(y_val, y_pred)\n",
    "    df = df.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "    df[\"model_name\"] = model_name\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model0: baseline model\n",
    "Sử dụng [TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) formula để convert words sang dạng numeric và model chúng bằng [Multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB), model này thường được refering cho các model dữ liệu text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84       435\n",
      "           1       0.83      0.69      0.76       327\n",
      "\n",
      "    accuracy                           0.81       762\n",
      "   macro avg       0.81      0.79      0.80       762\n",
      "weighted avg       0.81      0.81      0.80       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# create model pipeline\n",
    "model_0 = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),  # convert words to numbers using TF-IDF\n",
    "        (\"clf\", MultinomialNB()),  # model the text\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit model\n",
    "model_0.fit(x_train, y_train)\n",
    "\n",
    "# evaluate our model\n",
    "y_pred = model_0.predict(x_val)\n",
    "model_0_res = get_performance(y_val, y_pred, model_name=\"0_baseline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1: simple dense\n",
    "Với dữ liệu đầu vào, ta thực hiện theo các bước\n",
    "```\n",
    "tokenization ---> embedding ---> average pooling ---> fully connected dence with sigmoid activation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_model(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_1_dense\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = build_model(10000, 15)\n",
    "model_1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.keras import TqdmCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.6289 - accuracy: 0.6509 - val_loss: 0.5621 - val_accuracy: 0.7493\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.4534 - accuracy: 0.8206 - val_loss: 0.4598 - val_accuracy: 0.7927\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.3400 - accuracy: 0.8673 - val_loss: 0.4364 - val_accuracy: 0.8045\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2745 - accuracy: 0.8956 - val_loss: 0.4397 - val_accuracy: 0.7913\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2277 - accuracy: 0.9177 - val_loss: 0.4600 - val_accuracy: 0.7953\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       435\n",
      "           1       0.80      0.69      0.74       327\n",
      "\n",
      "    accuracy                           0.80       762\n",
      "   macro avg       0.80      0.78      0.79       762\n",
      "weighted avg       0.80      0.80      0.79       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_1.predict(x_val).round().squeeze()\n",
    "model_1_res = get_performance(y_val, y_pred, model_name=\"1_simple_dence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding vector and metadata\n",
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\n",
    "    \"Datasets/nlp_getting_started/embedding_vectors.tsv\", \"w\", encoding=\"utf-8\"\n",
    ")\n",
    "out_m = io.open(\n",
    "    \"Datasets/nlp_getting_started/embedding_metadata.tsv\",\n",
    "    \"w\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "# get vocab and embedding_weight from model\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "words_in_vocab = model_1.get_layer(\"text_vectorization_1\").get_vocabulary()\n",
    "\n",
    "\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "    if num == 0:\n",
    "        continue  # skip padding token\n",
    "    vec = embed_weights[num]\n",
    "    out_m.write(word + \"\\n\")  # write words to file\n",
    "    out_v.write(\n",
    "        \"\\t\".join([str(x) for x in vec]) + \"\\n\"\n",
    "    )  # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2: LSTM model\n",
    "\n",
    "[`tensorflow.keras.layers.LSTM()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_lstm(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = keras.layers.LSTM(64)(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_2 = build_lstm(10000, 15)\n",
    "model_2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 10s 44ms/step - loss: 0.5158 - accuracy: 0.7396 - val_loss: 0.4424 - val_accuracy: 0.8071\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.2985 - accuracy: 0.8824 - val_loss: 0.4780 - val_accuracy: 0.7835\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 16ms/step - loss: 0.1931 - accuracy: 0.9295 - val_loss: 0.5891 - val_accuracy: 0.7559\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1324 - accuracy: 0.9486 - val_loss: 0.7432 - val_accuracy: 0.7835\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0908 - accuracy: 0.9631 - val_loss: 0.7791 - val_accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "model_2_history = model_2.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       435\n",
      "           1       0.74      0.68      0.71       327\n",
      "\n",
      "    accuracy                           0.76       762\n",
      "   macro avg       0.76      0.75      0.75       762\n",
      "weighted avg       0.76      0.76      0.76       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_2.predict(x_val).round().squeeze()\n",
    "model_2_res = get_performance(y_val, y_pred, model_name=\"2_lstm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model3: GRU model\n",
    "\n",
    "[`tensorflow.keras.layers.GRU()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers_GRU -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_gru(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = keras.layers.GRU(64)(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_3_GRU\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_3 = build_gru(10000, 15)\n",
    "model_3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 24ms/step - loss: 0.6505 - accuracy: 0.6179 - val_loss: 0.5060 - val_accuracy: 0.7690\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3814 - accuracy: 0.8348 - val_loss: 0.4461 - val_accuracy: 0.7979\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.2317 - accuracy: 0.9066 - val_loss: 0.5470 - val_accuracy: 0.7848\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1470 - accuracy: 0.9448 - val_loss: 0.7074 - val_accuracy: 0.7520\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1061 - accuracy: 0.9594 - val_loss: 0.7763 - val_accuracy: 0.7559\n"
     ]
    }
   ],
   "source": [
    "model_3_history = model_3.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.79       435\n",
      "           1       0.74      0.66      0.70       327\n",
      "\n",
      "    accuracy                           0.76       762\n",
      "   macro avg       0.75      0.74      0.75       762\n",
      "weighted avg       0.75      0.76      0.75       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_3.predict(x_val).round().squeeze()\n",
    "model_3_res = get_performance(y_val, y_pred, model_name=\"3_GRU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model4: Bidirectonal RNN\n",
    "\n",
    "[`tensorflow.keras.layers.Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) là một cách tiếp cận training 2 chiều thay vì một chiều, do đó có thể wrap với bất kỳ layers nào nếu muốn training 2 chiều"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 25ms/step - loss: 0.6467 - accuracy: 0.6189 - val_loss: 0.4780 - val_accuracy: 0.8005\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3829 - accuracy: 0.8339 - val_loss: 0.4440 - val_accuracy: 0.8031\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.2445 - accuracy: 0.9053 - val_loss: 0.5274 - val_accuracy: 0.7992\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1554 - accuracy: 0.9441 - val_loss: 0.6703 - val_accuracy: 0.7546\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1085 - accuracy: 0.9609 - val_loss: 0.7498 - val_accuracy: 0.7690\n",
      "24/24 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80       435\n",
      "           1       0.75      0.69      0.72       327\n",
      "\n",
      "    accuracy                           0.77       762\n",
      "   macro avg       0.77      0.76      0.76       762\n",
      "weighted avg       0.77      0.77      0.77       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_gru_bidir(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = layers.Bidirectional(keras.layers.GRU(64))(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_4_GRU_bidir\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_4 = build_gru(10000, 15)\n",
    "# model_4.summary()\n",
    "model_4_history = model_4.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n",
    "# prediction and evaluation\n",
    "y_pred = model_4.predict(x_val).round().squeeze()\n",
    "model_4_res = get_performance(y_val, y_pred, model_name=\"model_4_GRU_bidir\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model5: Convolutional NN for text\n",
    "\n",
    "Khi sử dụng [__Convolutional layers__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) cho dữ liệu text (sequences) thì điểm khác biệt chính là số chiều sẽ là 1 (thay vì D = 2 khi sử lý dữ liệu dạng image)\n",
    "\n",
    "Các bước chính trong viêc sử dụng CNN for text data: ([chi tiết tại Understanding Convolutional Neural Networks for Text Classification](https://www.aclweb.org/anthology/W18-5408.pdf))\n",
    "1. Sử dụng `Conv1D()` để filter bằng __ngram detectors__, mỗi 1 filter cụ thể sẽ tạo đặc điểm gần nhất với 1 họ ngrams\n",
    "> an ngram là 1 collection of n-words\n",
    "2. Sử dụng Maxpooling trong suốt quá trình extracts những ngrams có liên quan để phục vụ việc ra quyét định\n",
    "3. Model sẽ phân loại dựa trên các thông tin sau lớp maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,300,545\n",
      "Trainable params: 1,300,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_cnn_text(max_vocab_length, max_sequence_length):\n",
    "    # setup TextVectorization\n",
    "    tvect = layers.TextVectorization(\n",
    "        max_tokens=max_vocab_length,\n",
    "        output_sequence_length=max_sequence_length,\n",
    "        output_mode=\"int\",\n",
    "        name=\"text_vectorization_1\",\n",
    "    )\n",
    "    tvect.adapt(x_train)\n",
    "\n",
    "    # setup embedding layer\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=max_vocab_length,  # set input shape\n",
    "        output_dim=128,  # set size of embedding vector\n",
    "        embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "        input_length=max_sequence_length,  # how long is each input\n",
    "        name=\"embedding_1\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = tvect(inputs)\n",
    "    x = embedding(x)\n",
    "    x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(\n",
    "        x\n",
    "    )  # sử dụng ngram với n = 5 words\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_5 = build_cnn_text(10000, 15)\n",
    "model_5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.5586 - accuracy: 0.7254 - val_loss: 0.4585 - val_accuracy: 0.7913\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.3250 - accuracy: 0.8678 - val_loss: 0.4647 - val_accuracy: 0.7940\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.1881 - accuracy: 0.9355 - val_loss: 0.5576 - val_accuracy: 0.7940\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.1173 - accuracy: 0.9581 - val_loss: 0.6164 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.0837 - accuracy: 0.9699 - val_loss: 0.6956 - val_accuracy: 0.7703\n"
     ]
    }
   ],
   "source": [
    "model_5_history = model_5.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       435\n",
      "           1       0.74      0.72      0.73       327\n",
      "\n",
      "    accuracy                           0.77       762\n",
      "   macro avg       0.77      0.76      0.76       762\n",
      "weighted avg       0.77      0.77      0.77       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "y_pred = model_5.predict(x_val).round().squeeze()\n",
    "model_5_res = get_performance(y_val, y_pred, model_name=\"5_Conv1D\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model6: TensorFlow Hub Pretrained Sentence Encoder\n",
    "\n",
    "the __Universal Sentence Encoder__ embedding a whole sentence-level (thay vì word-level như layer Embedding phía trên), với mỗi sentence được encode thành vector có 512 dimentional\n",
    "\n",
    "> 🔑 Note: An __encoder__ is the name for a model which converts raw data such as text into a numerical representation (feature vector), a __decoder__ converts the numerical representation to a desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 20s 90ms/step - loss: 0.5124 - accuracy: 0.7719 - val_loss: 0.4251 - val_accuracy: 0.8058\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 12s 56ms/step - loss: 0.4283 - accuracy: 0.8067 - val_loss: 0.4156 - val_accuracy: 0.8071\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 11s 50ms/step - loss: 0.4179 - accuracy: 0.8129 - val_loss: 0.4206 - val_accuracy: 0.8097\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 10s 48ms/step - loss: 0.4103 - accuracy: 0.8161 - val_loss: 0.4189 - val_accuracy: 0.8123\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 10s 47ms/step - loss: 0.4049 - accuracy: 0.8196 - val_loss: 0.4201 - val_accuracy: 0.8045\n",
      "24/24 [==============================] - 4s 155ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       435\n",
      "           1       0.77      0.78      0.77       327\n",
      "\n",
      "    accuracy                           0.80       762\n",
      "   macro avg       0.80      0.80      0.80       762\n",
      "weighted avg       0.80      0.80      0.80       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_USE():\n",
    "    # We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "    sentence_encoder_layer = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "        input_shape=[],  # shape of inputs coming to our model\n",
    "        dtype=tf.string,  # data type of inputs coming to the USE layer\n",
    "        trainable=False,  # keep the pretrained weights (we'll create a feature extractor)\n",
    "        name=\"USE\",\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = tf.keras.layers.Input(shape=[], dtype=\"string\")\n",
    "    x = sentence_encoder_layer(inputs)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_6_USE\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_6 = build_USE()\n",
    "model_6_history = model_6.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n",
    "# prediction and evaluation\n",
    "y_pred = model_6.predict(x_val).round().squeeze()\n",
    "model_6_res = get_performance(y_val, y_pred, model_name=\"model_6_USE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model7: Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7_Transformers\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " keras_layer_3 (KerasLayer)  (None, 50)                48190600  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                3264      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,193,929\n",
      "Trainable params: 48,193,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 0.5330 - accuracy: 0.7453 - val_loss: 0.4240 - val_accuracy: 0.8228\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 12s 58ms/step - loss: 0.3394 - accuracy: 0.8568 - val_loss: 0.4291 - val_accuracy: 0.8228\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 12s 58ms/step - loss: 0.2187 - accuracy: 0.9164 - val_loss: 0.5073 - val_accuracy: 0.7874\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 12s 58ms/step - loss: 0.1378 - accuracy: 0.9512 - val_loss: 0.6148 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 13s 58ms/step - loss: 0.0953 - accuracy: 0.9667 - val_loss: 0.6857 - val_accuracy: 0.7808\n",
      "24/24 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_performance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# prediction and evaluation\u001b[39;00m\n",
      "\u001b[1;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_7\u001b[38;5;241m.\u001b[39mpredict(x_val)\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;32m---> 24\u001b[0m model_7_res \u001b[38;5;241m=\u001b[39m \u001b[43mget_performance\u001b[49m(y_val, y_pred, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_7_Transformers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_performance' is not defined"
     ]
    }
   ],
   "source": [
    "def build_tfm():\n",
    "    embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "    hub_layer = hub.KerasLayer(\n",
    "        embedding, input_shape=[], dtype=tf.string, trainable=True\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    inputs = tf.keras.layers.Input(shape=[], dtype=\"string\")\n",
    "    x = hub_layer(inputs)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"model_7_Transformers\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.legacy.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "model_7 = build_tfm()\n",
    "model_7_history = model_7.fit(\n",
    "    x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=1\n",
    ")\n",
    "# prediction and evaluation\n",
    "y_pred = model_7.predict(x_val).round().squeeze()\n",
    "model_7_res = get_performance(y_val, y_pred, model_name=\"model_7_Transformers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "res = pd.concat(\n",
    "    [\n",
    "        model_0_res,\n",
    "        model_1_res,\n",
    "        model_2_res,\n",
    "        model_3_res,\n",
    "        model_4_res,\n",
    "        model_5_res,\n",
    "        model_6_res,\n",
    "        model_7_res,\n",
    "    ]\n",
    ")\n",
    "fig = px.bar(\n",
    "    res,\n",
    "    y=\"class1\",\n",
    "    x=\"model_name\",\n",
    "    color=\"metric\",\n",
    "    barmode=\"group\",\n",
    "    range_y=(0.6, 1),\n",
    "    text_auto=\".2f\",\n",
    ")\n",
    "fig.update_traces(\n",
    "    textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../_images/comparisation_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining our models (model ensembling/stacking)\n",
    "\n",
    "ref: [__Chapter 6 of the Machine Learning Engineering Book__](http://www.mlebook.com/wiki/doku.php)\n",
    "\n",
    "Sử dụng ensemble để kết hợp nhiều model để make prediction với điều kiện là các models có tính chất __uncorrelated__ với nhau, hay nói cách khác là mỗi model có 1 cách tiếp cận/kiến trúc mạng khác nhau, cách tìm ra patterns khác nhau.\n",
    "\n",
    "Các phương pháp combine output:\n",
    "1. average the probabilities\n",
    "2. Majority vote\n",
    "3. Model stacking: sử dụng output của model này để làm input cho model khác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use H5 format\n",
    "model_6.save(\"models/NLP/H5/model_6.h5\")\n",
    "\n",
    "# Load model with custom Hub Layer (required with HDF5 format)\n",
    "# do model 6 sử dụng cấu trúc từ nguồn ngoài nên phải khai báo custom_objects\n",
    "loaded_model_6 = tf.keras.models.load_model(\n",
    "    \"models/NLP/H5/model_6.h5\", custom_objects={\"KerasLayer\": hub.KerasLayer}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most wrong examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 3s 107ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>make sure evacuate past fire doors questions y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>foodscare offersgo nestleindia slips loss magg...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>phiddleface theres choking hazard dont die get</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>ruin life</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>blazing elwoods blazingelwoods bother doug son...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  pred  \\\n",
       "3289  make sure evacuate past fire doors questions y...       0   0.0   \n",
       "4221  foodscare offersgo nestleindia slips loss magg...       1   1.0   \n",
       "4186     phiddleface theres choking hazard dont die get       0   0.0   \n",
       "5873                                          ruin life       0   0.0   \n",
       "706   blazing elwoods blazingelwoods bother doug son...       0   0.0   \n",
       "\n",
       "      pred_prob  \n",
       "3289   0.474197  \n",
       "4221   0.526637  \n",
       "4186   0.062413  \n",
       "5873   0.047390  \n",
       "706    0.036600  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dataframe with validation sentences and best performing model predictions\n",
    "y_pred = model_6.predict(x_val)\n",
    "val_df = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": x_val,\n",
    "        \"target\": y_val,\n",
    "        \"pred\": y_pred.round().squeeze(),\n",
    "        \"pred_prob\": y_pred.squeeze(),\n",
    "    }\n",
    ")\n",
    "val_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>general news uae demolition houses waterways b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.921028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>alaska wolves face catastrophe denali wolves p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>madonnamking rspca site multiple story high ri...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3821</th>\n",
       "      <td>juneau empire first responders turn national n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>government concerned population explosion popu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>could die falling sinkhole still blamed</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>fredolsencruise please take faroeislands itine...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.852597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>steveycheese mapmyrun electrocuted way round m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>plan emergency preparedness families children ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>nikostar lakes ohio thought abject desolation ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  pred  \\\n",
       "2345  general news uae demolition houses waterways b...       0   1.0   \n",
       "1491  alaska wolves face catastrophe denali wolves p...       0   1.0   \n",
       "3991  madonnamking rspca site multiple story high ri...       0   1.0   \n",
       "3821  juneau empire first responders turn national n...       0   1.0   \n",
       "3506  government concerned population explosion popu...       0   1.0   \n",
       "6070            could die falling sinkhole still blamed       0   1.0   \n",
       "4832  fredolsencruise please take faroeislands itine...       0   1.0   \n",
       "3111  steveycheese mapmyrun electrocuted way round m...       0   1.0   \n",
       "3193  plan emergency preparedness families children ...       0   1.0   \n",
       "2525  nikostar lakes ohio thought abject desolation ...       0   1.0   \n",
       "\n",
       "      pred_prob  \n",
       "2345   0.921028  \n",
       "1491   0.913840  \n",
       "3991   0.907247  \n",
       "3821   0.894248  \n",
       "3506   0.891692  \n",
       "6070   0.872665  \n",
       "4832   0.852597  \n",
       "3111   0.829169  \n",
       "3193   0.815358  \n",
       "2525   0.805084  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the wrong predictions and sort by prediction probabilities\n",
    "most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\n",
    "    \"pred_prob\", ascending=False\n",
    ")\n",
    "most_wrong[:10]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
