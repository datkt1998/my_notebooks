{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing in NLP\n",
    "\n",
    "Mục tiêu của toàn bộ quá trình này là giúp mô hình học máy (Machine Learning) hoặc mô hình học sâu (Deep Learning) nhận được đầu vào “sạch” và “chuẩn hoá”, từ đó đạt hiệu quả cao hơn trong phân tích và dự đoán.\n",
    "\n",
    "NLP Python packages:\n",
    "\n",
    "\n",
    "\n",
    "|NLP Library|\tDescription|\n",
    "|---|---|\n",
    "|NLTK\t|This is one of the most usable and mother of all NLP libraries.|\n",
    "|spaCy\t|This is a completely optimized and highly accurate library widely used in deep learning|\n",
    "|Stanford CoreNLP| Python\tFor client-server-based architecture, this is a good library in NLTK. This is written in JAVA, but it provides modularity to use it in Python.|\n",
    "|TextBlob\t|This is an NLP library which works in Pyhton2 and python3. This is used for processing textual data and provide mainly all type of operation in the form of API.|\n",
    "|Gensim\t|Genism is a robust open source NLP library support in Python. This library is highly efficient and scalable.|\n",
    "|Pattern\t|It is a light-weighted NLP module. This is generally used in Web-mining, crawling or such type of spidering task|\n",
    "|Polyglot\t|For massive multilingual applications, Polyglot is best suitable NLP library. Feature extraction in the way on Identity and Entity.|\n",
    "|PyNLPl\t|PyNLPI also was known as ‘Pineapple’ and supports Python. It provides a parser for many data formats like FoLiA/Giza/Moses/ARPA/Timbl/CQL.|\n",
    "|Vocabulary\t|This library is best to get Semantic type information from the given text.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Cleaning (Làm sạch dữ liệu thô ban đầu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Chuyển đổi chữ hoa/thường (Case Normalization)**:\n",
    "    - Thường chuyển tất cả về chữ thường (lowercase) để giảm độ phức tạp khi so sánh từ.\n",
    "    - However, do remember that **lowercasing can change the meaning of some text** e.g \"US\" vs \"us\".\n",
    "\n",
    "- **Loại bỏ ký tự hoặc biểu tượng không mong muốn**:\n",
    "    - Ví dụ: ký tự đặc biệt, emoji, đường dẫn (URL), email, ký tự HTML, thẻ HTML, v.v.\n",
    "    - Mục đích: giảm bớt những thành phần không có giá trị ngữ nghĩa hoặc gây nhiễu.\n",
    "\n",
    "- **Loại bỏ khoảng trắng, xuống dòng thừa**:\n",
    "    - Giúp dữ liệu gọn gàng, nhất quán.\n",
    "\n",
    "- **Sửa lỗi chính tả (nếu cần)**: Trong một số bài toán phân tích ngôn ngữ, việc chính tả chính xác có ý nghĩa quan trọng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise from the Dataset (Loại bỏ nhiễu)\n",
    "\n",
    "- **Loại bỏ hoặc thay thế token vô nghĩa (Stopwords, từ vô nghĩa trong ngữ cảnh)**:\n",
    "    - Stopwords (như \"và\", \"hoặc\", \"của\" trong tiếng Việt; \"the\", \"is\", \"at\" trong tiếng Anh, v.v.) thường ít mang thông tin ngữ nghĩa và có thể gây nhiễu cho mô hình.\n",
    "    - Tùy bài toán mà quyết định giữ hay bỏ, vì đôi khi stopwords cũng quan trọng trong một số ngữ cảnh.\n",
    "- **Xử lý các từ viết tắt, từ lóng**:\n",
    "    - Ví dụ: “ko” -> “không”, “k” -> “không” (trong tiếng Việt), hoặc “u” -> “you” (tiếng Anh).\n",
    "    - Việc nhất quán hoá các biến thể từ vựng giúp mô hình hiểu rõ hơn.\n",
    "- **Loại bỏ những phần tử không liên quan**:\n",
    "    - Ví dụ: trong các đoạn văn bản có chèn các code snippet, bảng biểu, metadata… không cần thiết cho phân tích.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convert to right-format for the ML Algorithm\n",
    "\n",
    "(Chuẩn hoá dữ liệu để đưa vào mô hình)\n",
    "\n",
    "- **Tokenization (Tách từ)**:\n",
    "    - Tách câu thành các đơn vị từ hoặc subword.\n",
    "    - Trong tiếng Anh thường dễ dàng hơn (tách theo dấu cách và ký tự đặc biệt), còn tiếng Việt cần sử dụng mô hình hoặc thư viện tách từ chuyên dụng (như VnCoreNLP, PyVi, v.v.).\n",
    "\n",
    "- **Stemming / Lemmatization (Giảm biến thể từ vựng)**:\n",
    "    - **Stemming**: cắt bỏ phần “đuôi” của từ để đưa về “gốc” (có thể không phải là từ đúng trong từ điển).\n",
    "        ```text\n",
    "        connecting  -->  connect\n",
    "        connected  -->  connect\n",
    "        connectivity  -->  connect\n",
    "        connect  -->  connect\n",
    "        connects  -->  connect\n",
    "        ```\n",
    "    - **Lemmatization**: đưa từ về dạng “gốc từ điển” (chính tắc) dựa vào từ loại, ngữ cảnh. **Lemmatization** về cơ bản là giống với **stemming** khi nó loại bỏ phần đuôi của từ để thu được gốc từ, tuy nhiên các gốc từ ở đây đều thực sự tốn tại chứ không như **stemming** (như ví dụ trên thì từ `moved` sau khi lemmatize sẽ thu được `move`). Trong thư viện NLTK sẽ sử dụng từ điển **Wordnet** để map theo các quy tắc (theo tính chất của từ, từ là danh từ, động từ, trạng từ hay tính từ). Sử dụng part-of-speech tagging (nltk.pos_tag) để thu được các tính chất của từ.\n",
    "  \n",
    "    -> Hai kỹ thuật này giúp giảm sự trùng lặp khi cùng một từ xuất hiện ở các dạng biến thể khác nhau.\n",
    "\n",
    "- **Chuyển dữ liệu sang định dạng mô hình yêu cầu**:\n",
    "    - Bag-of-Words (đếm tần suất từ)\n",
    "    - TF-IDF (xem xét tần suất và mức độ phân biệt của từ)\n",
    "    - Word Embeddings (word2vec, GloVe)\n",
    "    - Mã hoá subword (BPE, SentencePiece)\n",
    "\n",
    "    ->    Tuỳ thuật toán và mô hình mà chọn cách biểu diễn phù hợp.\n",
    "\n",
    "- **Xử lý nhãn (nếu là bài toán giám sát)**:\n",
    "    - Kiểm tra và chuẩn hoá dữ liệu nhãn (label).\n",
    "Ví dụ: chuyển từ “positive” / “negative” / “neutral” sang 0 / 1 / 2 hoặc tương tự."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "An n-gram is simply a sequence of neighbouring n words (or tokens), where n can be any number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/datkhong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/datkhong/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/datkhong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "data = pd.read_csv(\n",
    "    r\"contents/theory/aiml_algorithms/dl_nlp/data/90 - tripadvisor-hotel-reviews.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pike, place, market)         8\n",
       "(stay, crown, plaza)          5\n",
       "(stay, hotel, monaco)         5\n",
       "(hotel, great, locat)         5\n",
       "(view, space, needl)          5\n",
       "                             ..\n",
       "(agenc, unfortun, warwick)    1\n",
       "(travel, agenc, unfortun)     1\n",
       "(stay, travel, agenc)         1\n",
       "(arrang, stay, travel)        1\n",
       "(hotel, right, street)        1\n",
       "Name: count, Length: 9265, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# lowercase\n",
    "data[\"review_preprocessed\"] = data[\"Review\"].str.lower()\n",
    "\n",
    "# replace * by star\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].str.replace(\n",
    "    r\"[*]\", \"star\"\n",
    ")\n",
    "\n",
    "# remove punctuation\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].str.replace(\n",
    "    r\"[^\\w\\s]\", \"\"\n",
    ")\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    ")\n",
    "\n",
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "en_stopwords.remove(\"not\")\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: \" \".join(\n",
    "        [word for word in x.split() if word not in en_stopwords]\n",
    "    )\n",
    ")\n",
    "\n",
    "# tokenize\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: nltk.word_tokenize(x)\n",
    ")\n",
    "\n",
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lmtz = WordNetLemmatizer()\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: [lmtz.lemmatize(word) for word in x]\n",
    ")\n",
    "\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: [ps.stem(word) for word in x]\n",
    ")\n",
    "\n",
    "# 3-gram\n",
    "from nltk.util import ngrams\n",
    "\n",
    "tokens_clean = sum(data[\"review_preprocessed\"], [])\n",
    "ngrams_3 = pd.Series(ngrams(tokens_clean, 3)).value_counts()\n",
    "ngrams_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n",
    "\n",
    "**Tokenization**: Tokenization (quy trình tách từ, chia nhỏ văn bản) là bước tiền xử lý (preprocessing) vô cùng quan trọng trong xử lý văn bản. Mục tiêu của tokenization là chuyển văn bản gốc (chuỗi ký tự) thành danh sách các token (những đơn vị có ý nghĩa).\n",
    "\n",
    "Ánh xạ character/word/subword sang giá trị số numberical value. Có 3 level của tokenization\n",
    "- _Word-level tokenization_: Mỗi từ sẽ đại diện bởi 1 numerical value. Tách theo khoảng trắng hoặc dựa trên thư viện tokenizer (tiếng Việt: VnCoreNLP, PyVi, RDRSegmenter, …). Ví dụ: \"I love yout\" ---> [0,1,2]\n",
    "- _Character-level tokenization_: Mỗi character (chữ cái, dấu câu) sẽ đại diện cho 1 token. Hữu ích trong một số bài toán (đặc biệt với các mô hình ngôn ngữ có ý định xử lý đánh vần, hoặc khi dữ liệu có nhiều từ mới).\n",
    "- _Sub-word tokenization_: break từng từ thành các phần và tokenization nó, khi đó mỗi word có thể thành nhiều tokens. Kết hợp ưu điểm giữa word-level và character-level, được sử dụng trong BERT, GPT, RoBERTa, PhoBERT, v.v.\n",
    "\n",
    "> Tuỳ thuộc vào problem mà nên chọn level tokenization cho phù hợp, hoặc có thể thử các level và kiểm tra performance, hoặc có thể sử dụng `tf.keras.layers.concatenate` để combine/stacking chúng lại với nhau.\n",
    "---\n",
    "**Tại sao tokenization quan trọng?**\n",
    "- Các mô hình NLP cổ điển (Bag-of-Words, TF-IDF, v.v.) hay hiện đại (Deep Learning) đều làm việc trên các đơn vị rời rạc (token).\n",
    "- Tokenization quyết định cách mô hình nhận thức văn bản: sai sót hoặc thiếu hợp lý trong tokenization ảnh hưởng đáng kể đến chất lượng mô hình.\n",
    "- Với các mô hình ngôn ngữ hiện đại (**Transformer** như `BERT`, `GPT`, `RoBERTa`, v.v.), vẫn cần **tokenization**, thường là subword tokenization (ví dụ `BPE`, `SentencePiece`). Lý do: mô hình cần chia văn bản thành các “mã” (code) được học sẵn trong từ vựng (vocabulary) để ánh xạ mỗi token sang vector **embedding** phù hợp.\n",
    "\n",
    "---\n",
    "**Bản chất hành động của Tokenization và Embedding:**\n",
    "\n",
    "- **Tokenization**: Chuyển đổi từ văn bản thành danh sách các token (dạng chữ) (word, subword, character). Sau đó sẽ chuyển sang dạng số (index) thông qua từ điển (vocabulary).\n",
    "- **Embedding**: Với mỗi 1 token (dạng số - index) thì token chuyển đổi từ dạng số (index) sang **vector** (nhiều chiều) số thực (embedding). Mỗi token sẽ được biểu diễn bởi một vector số thực có số chiều là `d` xác định trước (ví dụ: 100, 200, 300 chiều). Thể hiện sự tương quan giữa các token trong không gian vector.\n",
    "- Khi mô hình nhận input (list các token ID), nó sẽ tra cứu (lookup) từng token ID trong hàng tương ứng của ma trận embedding `𝑊` để lấy được vector (d chiều) tương ứng với token đó.\n",
    "- Kết quả: Thay vì list số ID, mô hình có một chuỗi vector (một cho mỗi token), phản ánh thông tin ngữ nghĩa và ngữ cảnh (với các mô hình hiện đại) của những token đó. Sau đó, mô hình sẽ sử dụng vector này để thực hiện các phép toán (tích vô hướng, pooling, attention, v.v.) để học cấu trúc ngữ nghĩa của văn bản.\n",
    "\n",
    "---\n",
    "\n",
    "**Tokenization Parameters**\n",
    "\n",
    "Sử dụng [`tf.keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) với một số params như sau:\n",
    "- `max_tokens` - Số lượng word tối đa trong vocabulary (e.g. 20000 or the number of unique words in your text), bao gồm 1 slot cho OOV (out of vocabulary) tokens.\n",
    "- `standardize` - Phương thức để standardizing text. Default is \"lower_and_strip_punctuation\" nghĩa là lowers text and removes all punctuation marks.\n",
    "- `split` - How to split text, default is \"whitespace\" which splits on spaces.\n",
    "- `ngrams` - How many words to contain per token split (create groups of n-words?), for example, ngrams=2 splits tokens into continuous sequences of 2.\n",
    "- `output_mode` - How to output tokens:\n",
    "    - \"int\" (integer mapping): map theo index của từ trong vocab\n",
    "    - \"multi_hot\" : mapping theo kiểu one-hot nếu từ đó có xuất hiện trong text\n",
    "    - \"count\": mapping theo số lần từ đó xuất hiện trong text\n",
    "    - [\"tf-idf\"](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting):  **Term Frequency - Inverse Document Frequency** trích xuất và đánh trọng số cho từ/ngữ (term) trong các tài liệu văn bản.\n",
    "        - `TF` Là tần suất xuất hiện của một từ (term) trong tài liệu. cho biết một từ có “nổi bật” trong tài liệu cụ thể hay không.\n",
    "        - `IDF`  Là nghịch đảo tần suất xuất hiện của một từ trong tập các tài liệu. cho biết từ đó có phổ biến/hay hiếm trong toàn bộ kho tài liệu.\n",
    "        - **TF cao**: Từ thường xuất hiện nhiều trong tài liệu => có vai trò quan trọng trong tài liệu đó.\n",
    "        - **IDF cao**: Từ hiếm (ít xuất hiện trong toàn bộ tập tài liệu) => độ “phân biệt” cao.\n",
    "        - **TF-IDF cao**: Từ rất quan trọng (phân biệt) cho tài liệu đó (xuất hiện nhiều trong tài liệu và hiếm gặp trong tài liệu khác).\n",
    "- `output_sequence_length` - Sử dụng trong `output_mode=int`, quy định độ dài của mỗi sequence output gồm bao nhiêu tokens, nếu sequence có độ dài hơn `output_sequence_length` thì sẽ được truncated, nếu ít hơn thì được padded để đảm bảo độ dài chính xác của output tensor là `shape = (batch_size, output_sequence_length)`\n",
    "- `pad_to_max_tokens` - Defaults to False, if True, the output feature axis sẽ được padded/mở rộng tới độ dài bằng max_tokens ngay cả khi số lượng unique tokens in the vocabulary nhỏ hơn max_tokens. Chỉ có tác dụng trong `output_mode` là `multi_hot`, `count`, `tf_id`\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practice cho Tokenization**\n",
    "\n",
    "***1. Chọn phương pháp tokenization phù hợp:***\n",
    "\n",
    "- Với mô hình Transformer hiện đại, thường dùng subword tokenization (BPE, SentencePiece) vì khả năng xử lý tốt từ mới, từ sai chính tả, từ hiếm, v.v.\n",
    "- Nếu làm truyền thống (Bag-of-Words, TF-IDF) với tiếng Việt, hãy sử dụng thư viện tách từ chuyên dụng (VD: VnCoreNLP).\n",
    "\n",
    "***2. Giữ nguyên (hoặc xử lý phù hợp) dấu câu, biểu tượng cảm xúc (emoji) nếu chúng mang ý nghĩa trong bài toán.***\n",
    "\n",
    "- Nhiều bài toán phân tích cảm xúc ở MXH cần emoji để hiểu sắc thái.\n",
    "\n",
    "***3. Kiểm tra chất lượng tokenization:***\n",
    "\n",
    "- Đặc biệt với tiếng Việt, tokenization chưa chuẩn có thể gây “vỡ nghĩa”.\n",
    "- Thử soi một số văn bản sau khi tokenization để chắc chắn phù hợp, tránh tách sai từ ghép (VD: “điện thoại”, “cầm tay” thành “điện”, “thoại”, “cầm”, “tay”).\n",
    "\n",
    "***4. Xử lý từ đặc biệt, hashtag, mention:***\n",
    "\n",
    "- Trong bài toán MXH, token hóa hashtag (#myhashtag), mention (@username), link URL, v.v., tùy xem bạn có muốn giữ hay loại bỏ.\n",
    "\n",
    "***5. Chuẩn hoá (normalization):***\n",
    "\n",
    "- Thông thường, chuyển văn bản về dạng chữ thường (lowercasing).\n",
    "- Với tiếng Việt, cần xem có biến đổi dấu không, hoặc chuẩn hoá ký tự unicode tổ hợp.\n",
    "\n",
    "**Note**: Mô hình BERT/PhoBERT gốc có thể không lowercasing để giữ nguyên case. Nên kiểm tra mô hình pre-trained yêu cầu gì.\n",
    "\n",
    "---\n",
    "\n",
    "**Có cần tuning lại tokenization?**\n",
    "\n",
    "Với các Large Language Model (LLM) hiện đại như GPT, BERT-based, T5, RoBERTa, … thường dùng một vocabulary và cơ chế tokenization đã huấn luyện sẵn. Khi bạn tải mô hình này về, đi kèm luôn có tokenizer (và embedding matrix tương ứng) đã đồng bộ với mô hình.\n",
    "\n",
    "> Thực tế, đa số người dùng không phải tự retrain hay tune lại tokenizer.\n",
    "> - Vì mô hình đã được pre-trained trên một khối lượng dữ liệu khổng lồ, tokenizer ban đầu (subword / BPE / SentencePiece) đã tương đối tối ưu.\n",
    "> - Nếu tự ý thay đổi tokenizer (thêm token, bớt token, thay đổi cách tách subword), bạn sẽ phải huấn luyện lại (hoặc điều chỉnh lớn) embedding, làm mất tính tương thích với các trọng số đã được pre-trained.\n",
    "\n",
    "\n",
    "***Ngoại lệ: Trong một số trường hợp đặc biệt, bạn có thể re-train / fine-tune tokenizer.***\n",
    "\n",
    "> Ví dụ: khi bạn có một miền dữ liệu rất đặc thù (như y học, hóa học, tài chính) với nhiều thuật ngữ, từ viết tắt, ký hiệu… hoàn toàn không (hoặc rất ít) xuất hiện trong bộ pre-training.\n",
    "> - Khi đó, tokenizer cũ có thể tạo ra nhiều token “[UNK]” (token lạ) hoặc token rất dài do không tìm thấy subword phù hợp.\n",
    "> - Bạn muốn nâng cao khả năng biểu diễn, có thể huấn luyện lại tokenizer trên data chuyên ngành, rồi fine-tune (hoặc huấn luyện lại từ đầu) embedding. Tuy nhiên, việc này đòi hỏi rất nhiều tài nguyên và kinh nghiệm, vì bạn cần đảm bảo mô hình (và embedding matrix) tương thích với tokenizer mới.\n",
    "\n",
    "\n",
    "Với hầu hết dự án NLP phổ biến, tuning lại tokenizer là không cần thiết (thậm chí bất lợi nếu bạn không có đủ dữ liệu để huấn luyện lại phần embedding). Thông thường, bạn:\n",
    "- Dùng tokenizer gốc kèm mô hình.\n",
    "- Fine-tune mô hình trên dữ liệu bạn quan tâm. Mọi thứ từ tokenization đến embedding được kế thừa sẵn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Tải tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "**Embedding** là cách ánh xạ mỗi **token/từ/câu từ** không gian rời rạc sang không gian vector liên tục (thường là **vector thực chiều d**, ví dụ 100, 300, 768, 1024). Các vector này được thiết kế/huấn luyện sao cho mang thông tin về ngữ nghĩa và ngữ cảnh của từ, bổ sung phần relationships giữa các tokens và có thể learning trong lúc train thay vì chỉ là những con số static\n",
    "\n",
    "Các từ đồng nghĩa/sát nghĩa thì sau khi chạy qua Embeddings layers/model (ví dụ __word2vec__) sẽ trả ra các vector gần bằng nhau, thể hiện nếu biểu diễn trong không gian thì các từ này sẽ gần nhau.\n",
    "\n",
    "- ***Word Embedding (ví dụ: Word2Vec, GloVe, FastText):***\n",
    "    - Mỗi từ tĩnh (static embedding) có một vector cố định.\n",
    "    - Không phân biệt nghĩa khác nhau của từ trong các ngữ cảnh khác nhau (cùng một từ “bank” – “ngân hàng” hoặc “bờ sông” – thường có 1 vector duy nhất).\n",
    "\n",
    "\n",
    "- ***Contextual Embedding (ví dụ: BERT, GPT, ELMo):***\n",
    "    - Vector biểu diễn từ phụ thuộc vào ngữ cảnh xung quanh.\n",
    "    - Từ “bank” trong câu “I put money in the bank” và “he sat on the bank of the river” sẽ có 2 vector khác nhau, nhờ mô hình Transformer/RNN nắm được ngữ cảnh.\n",
    "\n",
    "- ***Sentence Embedding / Document Embedding:***\n",
    "    - Thay vì embedding cho từng từ, mô hình tạo vector cho câu hoặc đoạn văn. Ví dụ: Sentence-BERT, Universal Sentence Encoder, InferSent, v.v.\n",
    "\n",
    "> Có thể upload các `vector output` kèm `labels` (các words) lên [__TensorFlow Embedding Projector__](http://projector.tensorflow.org/?_gl=1*1jn4yxx*_ga*NTEzMDY1NzcwLjE2ODIxNTU3Mjg.*_ga_W0YLR4190T*MTY4Mzc4MzIwNy41Mi4xLjE2ODM3ODMzMjQuMC4wLjA.) để visualize tính relationships giữa các words\n",
    "\n",
    "---\n",
    "\n",
    "**Có 2 hướng tiếp cận Embeddings:**\n",
    "- ___Create your own embedding___: Để sử dụng được embedding, text cần phải được turn into numbers, sau đó sử dụng `embedding layer` (such as `tf.keras.layers.Embedding`) để learn trong quá trình training\n",
    "- ___Reuse a pre-learned embedding___(**prefer**): Sử dụng các tham số embedding trong các pre-train model để fine-tune lại own model. Các pre-train này học từ một số lượng lớn text ( Ví dụ như all of Wikipedia) nên có khả năng đại diện chính xác mỗi quan hệ giữa các từ \n",
    "    > Sử dụng cách search tương tự như computer vision pretrain model trên [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) để lựa chọn ra một số pre-train model hiệu quả như  [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/),.. \n",
    "\n",
    "---\n",
    "\n",
    "**Có 2 cách thức embedding:**\n",
    "\n",
    "- ___Continuous Bag-of-Words (CBOW)___: Phương pháp này lấy inputs đầu vào là một/nhiều từ context word và cố gắng dự đoán output là target word thông qua một tầng neural đơn giản. Nhờ việc đánh giá output error với target word ở dạng one-hot, mô hình có thể điều chỉnh weight, học được vector biểu diễn cho target word. \n",
    "> Ví dụ ta có một câu tiếng anh như sau : \"I love you\". Ta có Input context word là \"love\" và Output target word là \"you\". Ta biến đổi input context đầu vào dưới dạng one-hot đi qua một tầng hidden layer và thực hiện softmax phân loại để dự đoán ra từ tiếp theo là gì.\n",
    "\n",
    "![](https://images.viblo.asia/1df2b1bc-c823-4b36-be92-46755788506a.png)\n",
    "\n",
    "- ___Skip-gram___: Nếu như __CBOW__ sử dụng input là __context word__ và cố gắng dự đoán từ đầu ra (__target word__) thì ngược lại, mô hình __Skip-gram__ sử dụng input là __target word__ và cố gắng dự đoán ra các từ hàng xóm của nó. Chúng định nghĩa các từ là hàng xóm (__neightbor word__) của nó thông qua tham số __window size__. \n",
    "> Ví dụ nếu bạn có một câu như sau: \"Tôi thích ăn cua hoàng đế\". Và input target word ban đầu là từ cua. Với kích thước window size = 2, ta sẽ có các neighbor word (thích, ăn, hoàng, đế ). Và chúng ta sẽ có 4 cặp input-output như sau: (cua, thích ), (cua, hoàng ), (cua, đế ), (cua, ăn ). Các __neightbor word__ được coi như nhau trong quá trình training.\n",
    "\n",
    "![](https://images.viblo.asia/d6dd1927-085e-45a7-89d0-0bd3ea152827.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Ứng dụng**\n",
    "- Phân loại văn bản (sentiment, topic): Dùng vector embedding (của từ hoặc câu) làm đầu vào mô hình.\n",
    "- Dịch máy, tóm tắt, chatbot: Hầu như mọi mô hình sequence-to-sequence (seq2seq) đều yêu cầu embedding tốt.\n",
    "- Truy xuất thông tin, tìm kiếm ngữ nghĩa: Tính tương đồng giữa câu truy vấn và câu trong văn bản thông qua cosine similarity, v.v.\n",
    "- Clustering, topic modeling: Nếu có embedding tốt, ta có thể gom nhóm các văn bản (hoặc từ) về chủ đề.\n",
    "- Recommendation, Phân tích đồ thị, …: Embedding được mở rộng không chỉ cho từ mà còn cho người dùng, item, node trong đồ thị (Graph Embedding).\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practice cho Embedding**\n",
    "\n",
    "***Sử dụng mô hình pre-trained (fine-tuned khi cần):***\n",
    "- Mô hình Transformer hiện đại (BERT, RoBERTa, GPT, XLM-R, PhoBERT, v.v.) thường cung cấp embedding có chất lượng rất tốt.\n",
    "- Nếu dữ liệu bạn chuyên biệt (y tế, tài chính) hoặc tiếng Việt, nên dùng hoặc fine-tune mô hình pre-trained phù hợp (chẳng hạn PhoBERT cho tiếng Việt).\n",
    "\n",
    "***Chọn loại embedding:***\n",
    "- Tĩnh (Word2Vec, GloVe): Đơn giản, dễ dùng, ít tài nguyên; nhưng không xử lý được từ đồng âm đa nghĩa theo ngữ cảnh.\n",
    "- Ngữ cảnh (Contextual, ví dụ BERT): Chính xác hơn, nắm bắt được meaning tùy ngữ cảnh, nhưng nặng về tính toán, phức tạp.\n",
    "\n",
    "***Khai thác “last hidden state” hay “pooler output” của BERT:***\n",
    "Khi muốn lấy embedding từ BERT, ta có thể lấy vector ẩn tại layer cuối, hoặc Mean Pooling hay [CLS] token (tuỳ mô hình). Chẳng hạn:\n",
    "- BERT gốc dùng vector tại [CLS] như “sentence embedding”, nhưng hiệu quả có thể kém hơn so với Mean Pooling.\n",
    "- Sentence-BERT đã tinh chỉnh cách lấy embedding câu.\n",
    "> Khuyến nghị: Thử Mean Pooling toàn bộ token (hoặc fine-tune mô hình “sentence embedding” chuyên dụng) để có câu embedding tốt hơn.\n",
    "\n",
    "***Fine-tune nếu có đủ dữ liệu:***\n",
    "- Nếu bài toán của bạn có sẵn dữ liệu labeled và quan trọng về chất lượng, hãy fine-tune mô hình pre-trained.\n",
    "- Fine-tune có thể cải thiện đáng kể độ chính xác so với chỉ dùng embedding tĩnh hoặc dùng mô hình pre-trained “thô”.\n",
    "\n",
    "***Quản lý OOV (Out-of-Vocabulary):***\n",
    "- Với embedding tĩnh (Word2Vec, GloVe), từ mới không có trong từ điển => mô hình không có vector. Cần kỹ thuật fallback (như sử dụng vector trung bình).\n",
    "- Subword embedding (BERT) giảm thiểu vấn đề OOV, vì từ mới được chia nhỏ thành subword.\n",
    "\n",
    "***Kiểm tra trực quan (phân tích PCA, t-SNE)***\n",
    "- Để đảm bảo embedding phản ánh tốt, bạn có thể trực quan hoá vector trong không gian 2D/3D.\n",
    "> Ví dụ, các từ/câu cùng chủ đề sẽ gần nhau, các từ/câu khác nghĩa sẽ xa nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
