{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing in NLP\n",
    "\n",
    "M·ª•c ti√™u c·ªßa to√†n b·ªô qu√° tr√¨nh n√†y l√† gi√∫p m√¥ h√¨nh h·ªçc m√°y (Machine Learning) ho·∫∑c m√¥ h√¨nh h·ªçc s√¢u (Deep Learning) nh·∫≠n ƒë∆∞·ª£c ƒë·∫ßu v√†o ‚Äús·∫°ch‚Äù v√† ‚Äúchu·∫©n ho√°‚Äù, t·ª´ ƒë√≥ ƒë·∫°t hi·ªáu qu·∫£ cao h∆°n trong ph√¢n t√≠ch v√† d·ª± ƒëo√°n.\n",
    "\n",
    "NLP Python packages:\n",
    "\n",
    "\n",
    "\n",
    "|NLP Library|\tDescription|\n",
    "|---|---|\n",
    "|NLTK\t|This is one of the most usable and mother of all NLP libraries.|\n",
    "|spaCy\t|This is a completely optimized and highly accurate library widely used in deep learning|\n",
    "|Stanford CoreNLP| Python\tFor client-server-based architecture, this is a good library in NLTK. This is written in JAVA, but it provides modularity to use it in Python.|\n",
    "|TextBlob\t|This is an NLP library which works in Pyhton2 and python3. This is used for processing textual data and provide mainly all type of operation in the form of API.|\n",
    "|Gensim\t|Genism is a robust open source NLP library support in Python. This library is highly efficient and scalable.|\n",
    "|Pattern\t|It is a light-weighted NLP module. This is generally used in Web-mining, crawling or such type of spidering task|\n",
    "|Polyglot\t|For massive multilingual applications, Polyglot is best suitable NLP library. Feature extraction in the way on Identity and Entity.|\n",
    "|PyNLPl\t|PyNLPI also was known as ‚ÄòPineapple‚Äô and supports Python. It provides a parser for many data formats like FoLiA/Giza/Moses/ARPA/Timbl/CQL.|\n",
    "|Vocabulary\t|This library is best to get Semantic type information from the given text.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Cleaning (L√†m s·∫°ch d·ªØ li·ªáu th√¥ ban ƒë·∫ßu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Chuy·ªÉn ƒë·ªïi ch·ªØ hoa/th∆∞·ªùng (Case Normalization)**:\n",
    "    - Th∆∞·ªùng chuy·ªÉn t·∫•t c·∫£ v·ªÅ ch·ªØ th∆∞·ªùng (lowercase) ƒë·ªÉ gi·∫£m ƒë·ªô ph·ª©c t·∫°p khi so s√°nh t·ª´.\n",
    "    - However, do remember that **lowercasing can change the meaning of some text** e.g \"US\" vs \"us\".\n",
    "\n",
    "- **Lo·∫°i b·ªè k√Ω t·ª± ho·∫∑c bi·ªÉu t∆∞·ª£ng kh√¥ng mong mu·ªën**:\n",
    "    - V√≠ d·ª•: k√Ω t·ª± ƒë·∫∑c bi·ªát, emoji, ƒë∆∞·ªùng d·∫´n (URL), email, k√Ω t·ª± HTML, th·∫ª HTML, v.v.\n",
    "    - M·ª•c ƒë√≠ch: gi·∫£m b·ªõt nh·ªØng th√†nh ph·∫ßn kh√¥ng c√≥ gi√° tr·ªã ng·ªØ nghƒ©a ho·∫∑c g√¢y nhi·ªÖu.\n",
    "\n",
    "- **Lo·∫°i b·ªè kho·∫£ng tr·∫Øng, xu·ªëng d√≤ng th·ª´a**:\n",
    "    - Gi√∫p d·ªØ li·ªáu g·ªçn g√†ng, nh·∫•t qu√°n.\n",
    "\n",
    "- **S·ª≠a l·ªói ch√≠nh t·∫£ (n·∫øu c·∫ßn)**: Trong m·ªôt s·ªë b√†i to√°n ph√¢n t√≠ch ng√¥n ng·ªØ, vi·ªác ch√≠nh t·∫£ ch√≠nh x√°c c√≥ √Ω nghƒ©a quan tr·ªçng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise from the Dataset (Lo·∫°i b·ªè nhi·ªÖu)\n",
    "\n",
    "- **Lo·∫°i b·ªè ho·∫∑c thay th·∫ø token v√¥ nghƒ©a (Stopwords, t·ª´ v√¥ nghƒ©a trong ng·ªØ c·∫£nh)**:\n",
    "    - Stopwords (nh∆∞ \"v√†\", \"ho·∫∑c\", \"c·ªßa\" trong ti·∫øng Vi·ªát; \"the\", \"is\", \"at\" trong ti·∫øng Anh, v.v.) th∆∞·ªùng √≠t mang th√¥ng tin ng·ªØ nghƒ©a v√† c√≥ th·ªÉ g√¢y nhi·ªÖu cho m√¥ h√¨nh.\n",
    "    - T√πy b√†i to√°n m√† quy·∫øt ƒë·ªãnh gi·ªØ hay b·ªè, v√¨ ƒë√¥i khi stopwords c≈©ng quan tr·ªçng trong m·ªôt s·ªë ng·ªØ c·∫£nh.\n",
    "- **X·ª≠ l√Ω c√°c t·ª´ vi·∫øt t·∫Øt, t·ª´ l√≥ng**:\n",
    "    - V√≠ d·ª•: ‚Äúko‚Äù -> ‚Äúkh√¥ng‚Äù, ‚Äúk‚Äù -> ‚Äúkh√¥ng‚Äù (trong ti·∫øng Vi·ªát), ho·∫∑c ‚Äúu‚Äù -> ‚Äúyou‚Äù (ti·∫øng Anh).\n",
    "    - Vi·ªác nh·∫•t qu√°n ho√° c√°c bi·∫øn th·ªÉ t·ª´ v·ª±ng gi√∫p m√¥ h√¨nh hi·ªÉu r√µ h∆°n.\n",
    "- **Lo·∫°i b·ªè nh·ªØng ph·∫ßn t·ª≠ kh√¥ng li√™n quan**:\n",
    "    - V√≠ d·ª•: trong c√°c ƒëo·∫°n vƒÉn b·∫£n c√≥ ch√®n c√°c code snippet, b·∫£ng bi·ªÉu, metadata‚Ä¶ kh√¥ng c·∫ßn thi·∫øt cho ph√¢n t√≠ch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can read more about AI at \n"
     ]
    }
   ],
   "source": [
    "url_example = \"You can read more about AI at https://viblo.asia/\"\n",
    "url_pattern = re.compile(r\"http\\S+\")\n",
    "print(url_pattern.sub(r\"\", url_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Without you by my side,   I am not complete. You have given me the best of love,  and I want to be by your side forever. Thank you for giving my life that direction it needed.  Thank you for loving me unconditional. \n"
     ]
    }
   ],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    \"[\"\n",
    "    \"\\U0001f600-\\U0001f64f\"  # emoticons\n",
    "    \"\\U0001f300-\\U0001f5ff\"  # symbols & pictographs\n",
    "    \"\\U0001f680-\\U0001f6ff\"  # transport & map symbols\n",
    "    \"\\U0001f1e0-\\U0001f1ff\"  # flags (iOS)\n",
    "    \"\\U00002500-\\U00002bef\"  # chinese char\n",
    "    \"\\U00002702-\\U000027b0\"\n",
    "    \"\\U00002702-\\U000027b0\"\n",
    "    \"\\U000024c2-\\U0001f251\"\n",
    "    \"\\U0001f926-\\U0001f937\"\n",
    "    \"\\U00010000-\\U0010ffff\"\n",
    "    \"\\u2640-\\u2642\"\n",
    "    \"\\u2600-\\u2b55\"\n",
    "    \"\\u200d\"\n",
    "    \"\\u23cf\"\n",
    "    \"\\u23e9\"\n",
    "    \"\\u231a\"\n",
    "    \"\\ufe0f\"  # dingbats\n",
    "    \"\\u3030\"\n",
    "    \"]+\",\n",
    "    flags=re.UNICODE,\n",
    ")\n",
    "emoji_example = \"üòÖ üë† üòÜ Without you by my side, üíì üòâ I am not complete. You have given me the best of love, üéà and I want to be by your side forever. Thank you for giving my life that direction it needed. üíã‚Äç Thank you for loving me unconditional. üíè\"\n",
    "print(emoji_pattern.sub(r\"\", emoji_example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuations: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Her cats name is Luna Her dogs name is max'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuation removal\n",
    "\n",
    "import string\n",
    "\n",
    "print(\"Punctuations: \" + string.punctuation + \"\\n\")\n",
    "\n",
    "sentences = \"Her cat's name is #Luna. Her dog's name is max\"\n",
    "\n",
    "sentences.translate(str.maketrans(\"\", \"\", string.punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "We remove these words because it removes a lot of complexity from the data. These words don't add much meaning to text so by removing them we are left with a smaller, cleaner dataset. Smaller, cleaner datasets often lead to increased accuracy in machine learning and will also speed up processing times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\datkt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "# assign our stop words to a variable\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "print(en_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions\n",
    "\n",
    "Check more detail in package `re` and `regex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.search`: check if a certain pattern is in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(22, 29), match='pattern'>\n"
     ]
    }
   ],
   "source": [
    "result_search = re.search(\"pattern\", r\"string containing the pattern\")\n",
    "print(result_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pattern'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_search.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.sub`: find certain text and replace it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW_SARAH was able to help me\n"
     ]
    }
   ],
   "source": [
    "new_string = re.sub(r\"sara\", r\"NEW_SARAH\", r\"sara was able to help me\")\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convert to right-format for the ML Algorithm\n",
    "\n",
    "(Chu·∫©n ho√° d·ªØ li·ªáu ƒë·ªÉ ƒë∆∞a v√†o m√¥ h√¨nh)\n",
    "\n",
    "- **Tokenization (T√°ch t·ª´)**:\n",
    "    - T√°ch c√¢u th√†nh c√°c ƒë∆°n v·ªã t·ª´ ho·∫∑c subword.\n",
    "    - Trong ti·∫øng Anh th∆∞·ªùng d·ªÖ d√†ng h∆°n (t√°ch theo d·∫•u c√°ch v√† k√Ω t·ª± ƒë·∫∑c bi·ªát), c√≤n ti·∫øng Vi·ªát c·∫ßn s·ª≠ d·ª•ng m√¥ h√¨nh ho·∫∑c th∆∞ vi·ªán t√°ch t·ª´ chuy√™n d·ª•ng (nh∆∞ VnCoreNLP, PyVi, v.v.).\n",
    "\n",
    "- **Stemming / Lemmatization (Gi·∫£m bi·∫øn th·ªÉ t·ª´ v·ª±ng)**:\n",
    "    - **Stemming**: c·∫Øt b·ªè ph·∫ßn ‚Äúƒëu√¥i‚Äù c·ªßa t·ª´ ƒë·ªÉ ƒë∆∞a v·ªÅ ‚Äúg·ªëc‚Äù (c√≥ th·ªÉ kh√¥ng ph·∫£i l√† t·ª´ ƒë√∫ng trong t·ª´ ƒëi·ªÉn).\n",
    "    - **Lemmatization**: ƒë∆∞a t·ª´ v·ªÅ d·∫°ng ‚Äúg·ªëc t·ª´ ƒëi·ªÉn‚Äù (ch√≠nh t·∫Øc) d·ª±a v√†o t·ª´ lo·∫°i, ng·ªØ c·∫£nh. **Lemmatization** v·ªÅ c∆° b·∫£n l√† gi·ªëng v·ªõi **stemming** khi n√≥ lo·∫°i b·ªè ph·∫ßn ƒëu√¥i c·ªßa t·ª´ ƒë·ªÉ thu ƒë∆∞·ª£c g·ªëc t·ª´, tuy nhi√™n c√°c g·ªëc t·ª´ ·ªü ƒë√¢y ƒë·ªÅu th·ª±c s·ª± t·ªën t·∫°i ch·ª© kh√¥ng nh∆∞ **stemming** (nh∆∞ v√≠ d·ª• tr√™n th√¨ t·ª´ `moved` sau khi lemmatize s·∫Ω thu ƒë∆∞·ª£c `move`). Trong th∆∞ vi·ªán NLTK s·∫Ω s·ª≠ d·ª•ng t·ª´ ƒëi·ªÉn **Wordnet** ƒë·ªÉ map theo c√°c quy t·∫Øc (theo t√≠nh ch·∫•t c·ªßa t·ª´, t·ª´ l√† danh t·ª´, ƒë·ªông t·ª´, tr·∫°ng t·ª´ hay t√≠nh t·ª´). S·ª≠ d·ª•ng part-of-speech tagging (nltk.pos_tag) ƒë·ªÉ thu ƒë∆∞·ª£c c√°c t√≠nh ch·∫•t c·ªßa t·ª´.\n",
    "  \n",
    "    -> Hai k·ªπ thu·∫≠t n√†y gi√∫p gi·∫£m s·ª± tr√πng l·∫∑p khi c√πng m·ªôt t·ª´ xu·∫•t hi·ªán ·ªü c√°c d·∫°ng bi·∫øn th·ªÉ kh√°c nhau.\n",
    "\n",
    "- **Chuy·ªÉn d·ªØ li·ªáu sang ƒë·ªãnh d·∫°ng m√¥ h√¨nh y√™u c·∫ßu**:\n",
    "    - Bag-of-Words (ƒë·∫øm t·∫ßn su·∫•t t·ª´)\n",
    "    - TF-IDF (xem x√©t t·∫ßn su·∫•t v√† m·ª©c ƒë·ªô ph√¢n bi·ªát c·ªßa t·ª´)\n",
    "    - Word Embeddings (word2vec, GloVe)\n",
    "    - M√£ ho√° subword (BPE, SentencePiece)\n",
    "\n",
    "    ->    Tu·ª≥ thu·∫≠t to√°n v√† m√¥ h√¨nh m√† ch·ªçn c√°ch bi·ªÉu di·ªÖn ph√π h·ª£p.\n",
    "\n",
    "- **X·ª≠ l√Ω nh√£n (n·∫øu l√† b√†i to√°n gi√°m s√°t)**:\n",
    "    - Ki·ªÉm tra v√† chu·∫©n ho√° d·ªØ li·ªáu nh√£n (label).\n",
    "V√≠ d·ª•: chuy·ªÉn t·ª´ ‚Äúpositive‚Äù / ‚Äúnegative‚Äù / ‚Äúneutral‚Äù sang 0 / 1 / 2 ho·∫∑c t∆∞∆°ng t·ª±."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\datkt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Her cat's name is Luna.\", \"Her dog's name is max\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"Her cat's name is Luna. Her dog's name is max\"\n",
    "sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Her',\n",
       " 'cat',\n",
       " \"'s\",\n",
       " 'name',\n",
       " 'is',\n",
       " 'Luna',\n",
       " '.',\n",
       " 'Her',\n",
       " 'dog',\n",
       " \"'s\",\n",
       " 'name',\n",
       " 'is',\n",
       " 'max']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# create stemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting  -->  connect\n",
      "connected  -->  connect\n",
      "connectivity  -->  connect\n",
      "connect  -->  connect\n",
      "connects  -->  connect\n"
     ]
    }
   ],
   "source": [
    "connect_tokens = [\n",
    "    \"connecting\",\n",
    "    \"connected\",\n",
    "    \"connectivity\",\n",
    "    \"connect\",\n",
    "    \"connects\",\n",
    "]\n",
    "\n",
    "for t in connect_tokens:\n",
    "    print(t, \" --> \", ps.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes  -->  like\n",
      "better  -->  better\n",
      "worse  -->  wors\n",
      "worst  -->  worst\n"
     ]
    }
   ],
   "source": [
    "likes_tokens = [\"likes\", \"better\", \"worse\", \"worst\"]\n",
    "\n",
    "for t in likes_tokens:\n",
    "    print(t, \" --> \", ps.stem(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "ƒë∆∞a t·ª´ v·ªÅ d·∫°ng ‚Äúg·ªëc t·ª´ ƒëi·ªÉn‚Äù (ch√≠nh t·∫Øc) d·ª±a v√†o t·ª´ lo·∫°i, ng·ªØ c·∫£nh.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\datkt\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# create lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting  :  connecting\n",
      "connected  :  connected\n",
      "connectivity  :  connectivity\n",
      "connect  :  connect\n",
      "connects  :  connects\n"
     ]
    }
   ],
   "source": [
    "for t in connect_tokens:\n",
    "    print(t, \" : \", lemmatizer.lemmatize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes  :  like\n",
      "better  :  better\n",
      "worse  :  worse\n",
      "worst  :  worst\n"
     ]
    }
   ],
   "source": [
    "for t in likes_tokens:\n",
    "    print(t, \" : \", lemmatizer.lemmatize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "An n-gram is simply a sequence of neighbouring n words (or tokens), where n can be any number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(to,)       7\n",
      "(of,)       6\n",
      "(the,)      4\n",
      "(in,)       4\n",
      "(and,)      3\n",
      "           ..\n",
      "(only,)     1\n",
      "(set,)      1\n",
      "(grow,)     1\n",
      "(years,)    1\n",
      "(come,)     1\n",
      "Name: count, Length: 79, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tokens = [\n",
    "    \"the\",\n",
    "    \"rise\",\n",
    "    \"of\",\n",
    "    \"artificial\",\n",
    "    \"intelligence\",\n",
    "    \"has\",\n",
    "    \"led\",\n",
    "    \"to\",\n",
    "    \"significant\",\n",
    "    \"advancements\",\n",
    "    \"in\",\n",
    "    \"natural\",\n",
    "    \"language\",\n",
    "    \"processing\",\n",
    "    \"computer\",\n",
    "    \"vision\",\n",
    "    \"and\",\n",
    "    \"other\",\n",
    "    \"fields\",\n",
    "    \"machine\",\n",
    "    \"learning\",\n",
    "    \"algorithms\",\n",
    "    \"are\",\n",
    "    \"becoming\",\n",
    "    \"more\",\n",
    "    \"sophisticated\",\n",
    "    \"enabling\",\n",
    "    \"computers\",\n",
    "    \"to\",\n",
    "    \"perform\",\n",
    "    \"complex\",\n",
    "    \"tasks\",\n",
    "    \"that\",\n",
    "    \"were\",\n",
    "    \"once\",\n",
    "    \"thought\",\n",
    "    \"to\",\n",
    "    \"be\",\n",
    "    \"the\",\n",
    "    \"exclusive\",\n",
    "    \"domain\",\n",
    "    \"of\",\n",
    "    \"humans\",\n",
    "    \"with\",\n",
    "    \"the\",\n",
    "    \"advent\",\n",
    "    \"of\",\n",
    "    \"deep\",\n",
    "    \"learning\",\n",
    "    \"neural\",\n",
    "    \"networks\",\n",
    "    \"have\",\n",
    "    \"become\",\n",
    "    \"even\",\n",
    "    \"more\",\n",
    "    \"powerful\",\n",
    "    \"capable\",\n",
    "    \"of\",\n",
    "    \"processing\",\n",
    "    \"vast\",\n",
    "    \"amounts\",\n",
    "    \"of\",\n",
    "    \"data\",\n",
    "    \"and\",\n",
    "    \"learning\",\n",
    "    \"from\",\n",
    "    \"it\",\n",
    "    \"in\",\n",
    "    \"ways\",\n",
    "    \"that\",\n",
    "    \"were\",\n",
    "    \"not\",\n",
    "    \"possible\",\n",
    "    \"before\",\n",
    "    \"as\",\n",
    "    \"a\",\n",
    "    \"result\",\n",
    "    \"ai\",\n",
    "    \"is\",\n",
    "    \"increasingly\",\n",
    "    \"being\",\n",
    "    \"used\",\n",
    "    \"in\",\n",
    "    \"a\",\n",
    "    \"wide\",\n",
    "    \"range\",\n",
    "    \"of\",\n",
    "    \"industries\",\n",
    "    \"from\",\n",
    "    \"healthcare\",\n",
    "    \"to\",\n",
    "    \"finance\",\n",
    "    \"to\",\n",
    "    \"transportation\",\n",
    "    \"and\",\n",
    "    \"its\",\n",
    "    \"impact\",\n",
    "    \"is\",\n",
    "    \"only\",\n",
    "    \"set\",\n",
    "    \"to\",\n",
    "    \"grow\",\n",
    "    \"in\",\n",
    "    \"the\",\n",
    "    \"years\",\n",
    "    \"to\",\n",
    "    \"come\",\n",
    "]\n",
    "\n",
    "# unigrams: n=1\n",
    "unigrams = pd.Series(nltk.ngrams(tokens, 1)).value_counts()\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(that, were)                  2\n",
      "(rise, of)                    1\n",
      "(of, artificial)              1\n",
      "(artificial, intelligence)    1\n",
      "(intelligence, has)           1\n",
      "                             ..\n",
      "(grow, in)                    1\n",
      "(in, the)                     1\n",
      "(the, years)                  1\n",
      "(years, to)                   1\n",
      "(to, come)                    1\n",
      "Name: count, Length: 105, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# unigrams: n=1\n",
    "unigrams = pd.Series(nltk.ngrams(tokens, 2)).value_counts()\n",
    "print(unigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
