{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing in NLP\n",
    "\n",
    "Mục tiêu của toàn bộ quá trình này là giúp mô hình học máy (Machine Learning) hoặc mô hình học sâu (Deep Learning) nhận được đầu vào “sạch” và “chuẩn hoá”, từ đó đạt hiệu quả cao hơn trong phân tích và dự đoán.\n",
    "\n",
    "NLP Python packages:\n",
    "\n",
    "\n",
    "\n",
    "|NLP Library|\tDescription|\n",
    "|---|---|\n",
    "|NLTK\t|This is one of the most usable and mother of all NLP libraries.|\n",
    "|spaCy\t|This is a completely optimized and highly accurate library widely used in deep learning|\n",
    "|Stanford CoreNLP| Python\tFor client-server-based architecture, this is a good library in NLTK. This is written in JAVA, but it provides modularity to use it in Python.|\n",
    "|TextBlob\t|This is an NLP library which works in Pyhton2 and python3. This is used for processing textual data and provide mainly all type of operation in the form of API.|\n",
    "|Gensim\t|Genism is a robust open source NLP library support in Python. This library is highly efficient and scalable.|\n",
    "|Pattern\t|It is a light-weighted NLP module. This is generally used in Web-mining, crawling or such type of spidering task|\n",
    "|Polyglot\t|For massive multilingual applications, Polyglot is best suitable NLP library. Feature extraction in the way on Identity and Entity.|\n",
    "|PyNLPl\t|PyNLPI also was known as ‘Pineapple’ and supports Python. It provides a parser for many data formats like FoLiA/Giza/Moses/ARPA/Timbl/CQL.|\n",
    "|Vocabulary\t|This library is best to get Semantic type information from the given text.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Cleaning (Làm sạch dữ liệu thô ban đầu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Chuyển đổi chữ hoa/thường (Case Normalization)**:\n",
    "    - Thường chuyển tất cả về chữ thường (lowercase) để giảm độ phức tạp khi so sánh từ.\n",
    "    - However, do remember that **lowercasing can change the meaning of some text** e.g \"US\" vs \"us\".\n",
    "\n",
    "- **Loại bỏ ký tự hoặc biểu tượng không mong muốn**:\n",
    "    - Ví dụ: ký tự đặc biệt, emoji, đường dẫn (URL), email, ký tự HTML, thẻ HTML, v.v.\n",
    "    - Mục đích: giảm bớt những thành phần không có giá trị ngữ nghĩa hoặc gây nhiễu.\n",
    "\n",
    "- **Loại bỏ khoảng trắng, xuống dòng thừa**:\n",
    "    - Giúp dữ liệu gọn gàng, nhất quán.\n",
    "\n",
    "- **Sửa lỗi chính tả (nếu cần)**: Trong một số bài toán phân tích ngôn ngữ, việc chính tả chính xác có ý nghĩa quan trọng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise from the Dataset (Loại bỏ nhiễu)\n",
    "\n",
    "- **Loại bỏ hoặc thay thế token vô nghĩa (Stopwords, từ vô nghĩa trong ngữ cảnh)**:\n",
    "    - Stopwords (như \"và\", \"hoặc\", \"của\" trong tiếng Việt; \"the\", \"is\", \"at\" trong tiếng Anh, v.v.) thường ít mang thông tin ngữ nghĩa và có thể gây nhiễu cho mô hình.\n",
    "    - Tùy bài toán mà quyết định giữ hay bỏ, vì đôi khi stopwords cũng quan trọng trong một số ngữ cảnh.\n",
    "- **Xử lý các từ viết tắt, từ lóng**:\n",
    "    - Ví dụ: “ko” -> “không”, “k” -> “không” (trong tiếng Việt), hoặc “u” -> “you” (tiếng Anh).\n",
    "    - Việc nhất quán hoá các biến thể từ vựng giúp mô hình hiểu rõ hơn.\n",
    "- **Loại bỏ những phần tử không liên quan**:\n",
    "    - Ví dụ: trong các đoạn văn bản có chèn các code snippet, bảng biểu, metadata… không cần thiết cho phân tích.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convert to right-format for the ML Algorithm\n",
    "\n",
    "(Chuẩn hoá dữ liệu để đưa vào mô hình)\n",
    "\n",
    "- **Tokenization (Tách từ)**:\n",
    "    - Tách câu thành các đơn vị từ hoặc subword.\n",
    "    - Trong tiếng Anh thường dễ dàng hơn (tách theo dấu cách và ký tự đặc biệt), còn tiếng Việt cần sử dụng mô hình hoặc thư viện tách từ chuyên dụng (như VnCoreNLP, PyVi, v.v.).\n",
    "\n",
    "- **Stemming / Lemmatization (Giảm biến thể từ vựng)**:\n",
    "    - **Stemming**: cắt bỏ phần “đuôi” của từ để đưa về “gốc” (có thể không phải là từ đúng trong từ điển).\n",
    "        ```text\n",
    "        connecting  -->  connect\n",
    "        connected  -->  connect\n",
    "        connectivity  -->  connect\n",
    "        connect  -->  connect\n",
    "        connects  -->  connect\n",
    "        ```\n",
    "    - **Lemmatization**: đưa từ về dạng “gốc từ điển” (chính tắc) dựa vào từ loại, ngữ cảnh. **Lemmatization** về cơ bản là giống với **stemming** khi nó loại bỏ phần đuôi của từ để thu được gốc từ, tuy nhiên các gốc từ ở đây đều thực sự tốn tại chứ không như **stemming** (như ví dụ trên thì từ `moved` sau khi lemmatize sẽ thu được `move`). Trong thư viện NLTK sẽ sử dụng từ điển **Wordnet** để map theo các quy tắc (theo tính chất của từ, từ là danh từ, động từ, trạng từ hay tính từ). Sử dụng part-of-speech tagging (nltk.pos_tag) để thu được các tính chất của từ.\n",
    "  \n",
    "    -> Hai kỹ thuật này giúp giảm sự trùng lặp khi cùng một từ xuất hiện ở các dạng biến thể khác nhau.\n",
    "\n",
    "- **Chuyển dữ liệu sang định dạng mô hình yêu cầu**:\n",
    "    - Bag-of-Words (đếm tần suất từ)\n",
    "    - TF-IDF (xem xét tần suất và mức độ phân biệt của từ)\n",
    "    - Word Embeddings (word2vec, GloVe)\n",
    "    - Mã hoá subword (BPE, SentencePiece)\n",
    "\n",
    "    ->    Tuỳ thuật toán và mô hình mà chọn cách biểu diễn phù hợp.\n",
    "\n",
    "- **Xử lý nhãn (nếu là bài toán giám sát)**:\n",
    "    - Kiểm tra và chuẩn hoá dữ liệu nhãn (label).\n",
    "Ví dụ: chuyển từ “positive” / “negative” / “neutral” sang 0 / 1 / 2 hoặc tương tự."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "An n-gram is simply a sequence of neighbouring n words (or tokens), where n can be any number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/datkhong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/datkhong/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/datkhong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "data = pd.read_csv(\n",
    "    r\"contents/theory/aiml_algorithms/dl_nlp/data/90 - tripadvisor-hotel-reviews.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pike, place, market)         8\n",
       "(stay, crown, plaza)          5\n",
       "(stay, hotel, monaco)         5\n",
       "(hotel, great, locat)         5\n",
       "(view, space, needl)          5\n",
       "                             ..\n",
       "(agenc, unfortun, warwick)    1\n",
       "(travel, agenc, unfortun)     1\n",
       "(stay, travel, agenc)         1\n",
       "(arrang, stay, travel)        1\n",
       "(hotel, right, street)        1\n",
       "Name: count, Length: 9265, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# lowercase\n",
    "data[\"review_preprocessed\"] = data[\"Review\"].str.lower()\n",
    "\n",
    "# replace * by star\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].str.replace(\n",
    "    r\"[*]\", \"star\"\n",
    ")\n",
    "\n",
    "# remove punctuation\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].str.replace(\n",
    "    r\"[^\\w\\s]\", \"\"\n",
    ")\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    ")\n",
    "\n",
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "en_stopwords.remove(\"not\")\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: \" \".join(\n",
    "        [word for word in x.split() if word not in en_stopwords]\n",
    "    )\n",
    ")\n",
    "\n",
    "# tokenize\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: nltk.word_tokenize(x)\n",
    ")\n",
    "\n",
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lmtz = WordNetLemmatizer()\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: [lmtz.lemmatize(word) for word in x]\n",
    ")\n",
    "\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "data[\"review_preprocessed\"] = data[\"review_preprocessed\"].apply(\n",
    "    lambda x: [ps.stem(word) for word in x]\n",
    ")\n",
    "\n",
    "# 3-gram\n",
    "from nltk.util import ngrams\n",
    "\n",
    "tokens_clean = sum(data[\"review_preprocessed\"], [])\n",
    "ngrams_3 = pd.Series(ngrams(tokens_clean, 3)).value_counts()\n",
    "ngrams_3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
