{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8245dbc-98b2-4906-abd4-3efc30dac997",
   "metadata": {},
   "source": [
    "# CF - ALS (by Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee7eb1-9ebc-495b-8962-fb4b6f5403fe",
   "metadata": {},
   "source": [
    "## Algorithm Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3189c4-f144-4626-83c7-14fd0229cb13",
   "metadata": {},
   "source": [
    "### Matrix factorization for collaborative filtering problem\n",
    "\n",
    "Matrix factorization tries to find latent factors that represent intrinsic user and item attributes in a lower dimension. \n",
    "\n",
    "1. **Prediction Formula**\n",
    "\n",
    "$$\\hat r_{u,i} = q_{i}^{T}p_{u}$$\n",
    "\n",
    "where:\n",
    "- $\\hat r_{u,i}$ is the predicted ratings for user $u$ and item $i$\n",
    "- $q_{i}$ and $p_{u}$ are latent factors for item and user, respectively.\n",
    "\n",
    "---> The goal is to find these latent factors $q_{i}^{T}$ and $p_{u}$ that best approximate the observed ratings.\n",
    "\n",
    "2. **Optimization Problem**\n",
    "\n",
    "The challenge to the matrix factorization problem is to find $q_{i}^{T}$ and $p_{u}$. This is achieved by methods such as **matrix decomposition**. A learning approach is therefore developed to converge the decomposition results close to the observed ratings as much as possible. Furthermore, to avoid overfitting issue, the learning process is **regularized**. \n",
    "\n",
    "***The Goal is minimize the prediction error***:\n",
    "\n",
    "$$\\min\\sum(r_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n",
    "\n",
    "where $\\lambda$ is a the regularization parameter that controls the trade-off between fitting the data well and keeping the model simple. \n",
    "\n",
    "**Trường hợp phải tính toán rating dựa trên interaction của KH với items - Implicit Feedback**\n",
    "\n",
    "In case **explict ratings** are not available, **implicit ratings** which are usually derived from users' historical interactions with the items (e.g., *clicks*, *views*, *purchases*, etc.). To account for such implicit ratings, the original matrix factorization algorithm can be formulated as \n",
    "\n",
    "$$\\min\\sum c_{u,i}(p_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n",
    "\n",
    "where:\n",
    "- $c_{u,i}=1+\\alpha r_{u,i}$: là trọng số cho từng tương tác. Nó phụ thuộc vào giá trị $r_{u,i}$, là một biểu thị của sự yêu thích của người dùng $u$ cho item $i$, giúp mô hình nhận biết rằng không phải tất cả các tương tác đều có giá trị như nhau. Một tương tác với số lần nhấp nhiều hơn sẽ được coi là quan trọng hơn so với một tương tác có ít lần nhấp.\n",
    "- $p_{u,i}=1$ if $r_{u,i}>0$ else $p_{u,i}=0$ (if $r_{u,i}=0$): Điều này có nghĩa là chỉ khi có tương tác, chúng ta mới tính đến phản hồi đó\n",
    "  > $r_{u,i}$ is a numerical representation of users' preferences (e.g., number of clicks, etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da33bc-1c9a-49b6-8b4c-57a25efb0a5e",
   "metadata": {},
   "source": [
    "### Alternating Least Square (ALS)\n",
    "\n",
    "Owing to the term of $q_{i}^{T}p_{u}$ the loss function is non-convex. Gradient descent method can be applied but this will incur expensive computations. An Alternating Least Square (ALS) algorithm was therefore developed to overcome this issue. \n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTGkKzxWWEfGMc2qp2b4wQXdgkMNxt6ijs9tg&s\">\n",
    "\n",
    "The basic idea of ALS is to learn one of $q$ and $p$ at a time for optimization while keeping the other as constant. This makes the objective at each iteration convex and solvable. The alternating between $q$ and $p$ stops when there is convergence to the optimal. It is worth noting that this iterative computation can be parallelised and/or distributed, which makes the algorithm desirable for use cases where the dataset is large and thus the user-item rating matrix is super sparse (as is typical in recommendation scenarios). A comprehensive discussion of ALS and its distributed computation can be found [here](http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35e4f5-a942-4e5f-8d8a-ae4099f207a9",
   "metadata": {},
   "source": [
    "### Spark Mllib implementation\n",
    "\n",
    "The matrix factorization algorithm is available as `ALS` module in [Spark `ml`](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) for DataFrame or [Spark `mllib`](https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) for RDD. \n",
    "\n",
    "* The uniqueness of ALS implementation is that it distributes the matrix factorization model training by using \"Alternating Least Square\" method. \n",
    "* In the training method, there are parameters that can be selected to control the model performance.\n",
    "* Both explicit and implicit ratings are supported by Spark ALS model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969dcac-c858-40eb-aae6-510bbddf7a81",
   "metadata": {},
   "source": [
    "## Setup enviroment and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e530b85-16d0-43bc-ab78-71150fbe04f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]\n",
      "Pandas version: 2.2.2\n",
      "PySpark version: 3.5.2\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.functions import col\n",
    "# from pyspark.ml.tuning import CrossValidator\n",
    "# from pyspark.sql.types import StructType, StructField\n",
    "# from pyspark.sql.types import FloatType, IntegerType, LongType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# from recommenders.datasets import movielens\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.evaluation.spark_evaluation import (  # noqa: E402\n",
    "    # SparkRankingEvaluation,\n",
    "    SparkRatingEvaluation,\n",
    ")\n",
    "# from recommenders.tuning.parameter_sweep import generate_param_grid\n",
    "# from recommenders.datasets.spark_splitters import spark_random_split\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6d418-7ca8-4be0-9731-8e82ff190c74",
   "metadata": {},
   "source": [
    "**model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2acad2-fc6b-4b43-9627-6f1e6e2ddaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper-parameters\n",
    "# rank of the factorization\n",
    "RANK = 10\n",
    "# Số lượng vòng lặp tối đa\n",
    "MAX_ITER = 15\n",
    "# Hệ số kiểm soát regularization\n",
    "REG_PARAM = 0.05\n",
    "\n",
    "# Number of recommended items\n",
    "K = 15\n",
    "\n",
    "# rating columns\n",
    "USER_COL = \"acnt_no\"\n",
    "ITEM_COL = \"symbol\"\n",
    "RATING_COL = \"final_score\"\n",
    "\n",
    "# Path to the CSV file\n",
    "train_datapath = r\"data/label/data_process_UserItemRating_min5items_train.csv\"\n",
    "test_datapath = r\"data/label/data_process_UserItemRating_min5items_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da9d29-3c50-4fe3-b787-7e64aa4ae9ee",
   "metadata": {},
   "source": [
    "**Spark session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37dd5a48-6355-4ed3-ac61-1ef7c138d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97735ecd-da97-42e0-9c6d-0651f295b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP SESSION\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d9e7ba-d284-47aa-9987-b48acfc01b56",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8494c9-e2f9-408d-a50e-ba2046fbaeef",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d7e46f-f30a-4fbb-8470-a49ffb72a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "| acnt_no|symbol|final_score|\n",
      "+--------+------+-----------+\n",
      "|00018956|   BID|       0.02|\n",
      "|00024968|   BID|       0.03|\n",
      "|00025170|   VIX|      3.924|\n",
      "+--------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(USER_COL, StringType(), True),\n",
    "        StructField(ITEM_COL, StringType(), True),\n",
    "        StructField(RATING_COL, FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read the CSV file into a DataFrame with the specified schema\n",
    "train = spark.read.csv(train_datapath, sep=\"|\", header=True, schema=schema)\n",
    "test = spark.read.csv(test_datapath, sep=\"|\", header=True, schema=schema)\n",
    "\n",
    "# Show the DataFrame to verify\n",
    "test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fca6a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+-------------------+\n",
      "|summary|          acnt_no|symbol|        final_score|\n",
      "+-------+-----------------+------+-------------------+\n",
      "|  count|           532012|532012|             532012|\n",
      "|   mean|59160.27250137215|  NULL|0.22697969638126742|\n",
      "| stddev|52375.70211050739|  NULL| 0.8522782496254795|\n",
      "|    min|         00000002|   AAA|              0.002|\n",
      "|    max|         00186799|   YTC|             12.869|\n",
      "+-------+-----------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1ec00-eb3a-4aac-8496-c3cc6c37ba66",
   "metadata": {},
   "source": [
    "**Preprocessing pipeline**\n",
    "\n",
    "Do mô hình ALS spark yêu câu dữ liệu **user** và **item** ở dạng `numeric`, nên cần xây dựng pipeline chuyển đổi dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5245b6c-cbae-4249-bfff-f66bdb6cd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi cột user\n",
    "user_indexer = StringIndexer(inputCol=USER_COL, outputCol=\"userIndex\")\n",
    "\n",
    "# Chuyển đổi cột item\n",
    "item_indexer = StringIndexer(inputCol=ITEM_COL, outputCol=\"itemIndex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3e1eb-8917-4e12-a143-025fa746e41c",
   "metadata": {},
   "source": [
    "**Setup model**\n",
    "\n",
    "It is worth noting that Spark ALS model allows dropping cold users to favor a robust evaluation with the testing data. In case there are cold users, Spark ALS implementation allows users to drop cold users in order to make sure evaluations on the prediction results are sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822f2594-d4cf-456e-a193-0a83829eb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    maxIter=MAX_ITER,\n",
    "    rank=RANK,\n",
    "    regParam=REG_PARAM,\n",
    "    userCol=\"userIndex\",\n",
    "    itemCol=\"itemIndex\",\n",
    "    ratingCol=RATING_COL,\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8625b8-95c6-41fc-8c61-add091080eaa",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8270c32-a74e-400a-b866-77b3f1b0902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 07:57:46 WARN DAGScheduler: Broadcasting large task binary with size 1108.0 KiB\n",
      "24/08/21 07:57:46 WARN DAGScheduler: Broadcasting large task binary with size 1110.3 KiB\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n",
      "24/08/21 07:57:47 WARN DAGScheduler: Broadcasting large task binary with size 1111.8 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=24798Kb max_used=24812Kb free=106273Kb\n",
      " bounds [0x00000001069e8000, 0x0000000108248000, 0x000000010e9e8000]\n",
      " total_blobs=9720 nmethods=8731 adapters=900\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 07:57:47 WARN DAGScheduler: Broadcasting large task binary with size 1113.1 KiB\n",
      "24/08/21 07:57:52 WARN DAGScheduler: Broadcasting large task binary with size 1112.1 KiB\n",
      "24/08/21 07:57:52 WARN DAGScheduler: Broadcasting large task binary with size 1113.4 KiB\n",
      "24/08/21 07:57:53 WARN DAGScheduler: Broadcasting large task binary with size 1114.2 KiB\n",
      "24/08/21 07:57:53 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/21 07:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1117.3 KiB\n",
      "24/08/21 07:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1118.7 KiB\n",
      "24/08/21 07:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1120.1 KiB\n",
      "24/08/21 07:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1121.5 KiB\n",
      "24/08/21 07:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1122.9 KiB\n",
      "24/08/21 07:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1124.3 KiB\n",
      "24/08/21 07:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1125.6 KiB\n",
      "24/08/21 07:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1127.0 KiB\n",
      "24/08/21 07:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1128.4 KiB\n",
      "24/08/21 07:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1129.8 KiB\n",
      "24/08/21 07:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1131.2 KiB\n",
      "24/08/21 07:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1132.6 KiB\n",
      "24/08/21 07:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1134.0 KiB\n",
      "24/08/21 07:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1135.4 KiB\n",
      "24/08/21 07:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1136.7 KiB\n",
      "24/08/21 07:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1138.1 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1139.5 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1140.9 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1142.3 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1143.7 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1145.1 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1146.4 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1147.8 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1149.2 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1150.6 KiB\n",
      "24/08/21 07:57:56 WARN DAGScheduler: Broadcasting large task binary with size 1152.0 KiB\n",
      "24/08/21 07:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1153.4 KiB\n",
      "24/08/21 07:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1154.8 KiB\n",
      "24/08/21 07:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1156.2 KiB\n",
      "24/08/21 07:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1158.1 KiB\n",
      "24/08/21 07:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1156.7 KiB\n"
     ]
    }
   ],
   "source": [
    "# make pipeline\n",
    "model_pipeline = Pipeline(stages=[user_indexer, item_indexer, als])\n",
    "\n",
    "# fit transform train data\n",
    "model = model_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c38885-7164-484b-8bb5-308eb7016e94",
   "metadata": {},
   "source": [
    "**Model Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a50a0e6-893d-4c21-a871-b4942bac4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9d6f7-a950-43e5-955f-f2ec509152ad",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04bd47f8-0186-41f7-92e2-6e13b6fbe469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 07:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1088.5 KiB\n",
      "24/08/21 07:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1166.1 KiB\n",
      "24/08/21 07:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1164.8 KiB\n",
      "24/08/21 07:57:58 WARN DAGScheduler: Broadcasting large task binary with size 1195.1 KiB\n",
      "24/08/21 07:58:00 WARN DAGScheduler: Broadcasting large task binary with size 1223.4 KiB\n",
      "24/08/21 07:58:01 WARN DAGScheduler: Broadcasting large task binary with size 1088.5 KiB\n",
      "24/08/21 07:58:01 WARN DAGScheduler: Broadcasting large task binary with size 1166.1 KiB\n",
      "24/08/21 07:58:01 WARN DAGScheduler: Broadcasting large task binary with size 1164.8 KiB\n",
      "24/08/21 07:58:01 WARN DAGScheduler: Broadcasting large task binary with size 1195.1 KiB\n",
      "24/08/21 07:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1223.4 KiB\n",
      "24/08/21 07:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1089.0 KiB\n",
      "24/08/21 07:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1166.1 KiB\n",
      "24/08/21 07:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1164.8 KiB\n",
      "24/08/21 07:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1088.8 KiB\n",
      "24/08/21 07:58:03 WARN DAGScheduler: Broadcasting large task binary with size 1196.1 KiB\n",
      "24/08/21 07:58:03 WARN DAGScheduler: Broadcasting large task binary with size 1195.4 KiB\n",
      "24/08/21 07:58:04 WARN DAGScheduler: Broadcasting large task binary with size 1222.5 KiB\n",
      "24/08/21 07:58:04 WARN DAGScheduler: Broadcasting large task binary with size 1219.7 KiB\n",
      "24/08/21 07:58:05 WARN DAGScheduler: Broadcasting large task binary with size 1249.6 KiB\n",
      "24/08/21 07:58:07 WARN DAGScheduler: Broadcasting large task binary with size 1264.3 KiB\n",
      "24/08/21 07:58:08 WARN DAGScheduler: Broadcasting large task binary with size 1265.2 KiB\n",
      "24/08/21 07:58:08 WARN DAGScheduler: Broadcasting large task binary with size 1089.0 KiB\n",
      "24/08/21 07:58:08 WARN DAGScheduler: Broadcasting large task binary with size 1166.1 KiB\n",
      "24/08/21 07:58:08 WARN DAGScheduler: Broadcasting large task binary with size 1164.8 KiB\n",
      "24/08/21 07:58:08 WARN DAGScheduler: Broadcasting large task binary with size 1088.8 KiB\n",
      "24/08/21 07:58:09 WARN DAGScheduler: Broadcasting large task binary with size 1196.5 KiB\n",
      "24/08/21 07:58:09 WARN DAGScheduler: Broadcasting large task binary with size 1195.8 KiB\n",
      "24/08/21 07:58:10 WARN DAGScheduler: Broadcasting large task binary with size 1222.9 KiB\n",
      "24/08/21 07:58:10 WARN DAGScheduler: Broadcasting large task binary with size 1220.1 KiB\n",
      "24/08/21 07:58:11 WARN DAGScheduler: Broadcasting large task binary with size 1258.9 KiB\n",
      "24/08/21 07:58:12 WARN DAGScheduler: Broadcasting large task binary with size 1089.0 KiB\n",
      "24/08/21 07:58:12 WARN DAGScheduler: Broadcasting large task binary with size 1166.1 KiB\n",
      "24/08/21 07:58:12 WARN DAGScheduler: Broadcasting large task binary with size 1164.8 KiB\n",
      "24/08/21 07:58:12 WARN DAGScheduler: Broadcasting large task binary with size 1088.8 KiB\n",
      "24/08/21 07:58:13 WARN DAGScheduler: Broadcasting large task binary with size 1196.5 KiB\n",
      "24/08/21 07:58:13 WARN DAGScheduler: Broadcasting large task binary with size 1195.8 KiB\n",
      "24/08/21 07:58:14 WARN DAGScheduler: Broadcasting large task binary with size 1220.5 KiB\n",
      "24/08/21 07:58:14 WARN DAGScheduler: Broadcasting large task binary with size 1220.1 KiB\n",
      "24/08/21 07:58:15 WARN DAGScheduler: Broadcasting large task binary with size 1257.0 KiB\n",
      "[Stage 836:===========>                                             (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score = 0.5255889928752164\n",
      "MAE score = 0.146146664142994\n",
      "R2 score = 0.6196963192818451\n",
      "Explained variance score = 0.6204599566890703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluations = SparkRatingEvaluation(\n",
    "    test_predictions,\n",
    "    test_predictions,\n",
    "    col_user=\"userIndex\",\n",
    "    col_item=\"itemIndex\",\n",
    "    col_rating=RATING_COL,\n",
    "    col_prediction=\"prediction\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"RMSE score = {}\".format(evaluations.rmse()),\n",
    "    \"MAE score = {}\".format(evaluations.mae()),\n",
    "    \"R2 score = {}\".format(evaluations.rsquared()),\n",
    "    \"Explained variance score = {}\".format(evaluations.exp_var()),\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f143f",
   "metadata": {},
   "source": [
    "Các ranking metrics should be apply the differenct from those that have been rated by the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a598a04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 08:24:35 WARN DAGScheduler: Broadcasting large task binary with size 1166.1 KiB\n",
      "24/08/21 08:24:35 WARN DAGScheduler: Broadcasting large task binary with size 1164.8 KiB\n",
      "24/08/21 08:24:36 WARN DAGScheduler: Broadcasting large task binary with size 1126.8 KiB\n",
      "24/08/21 08:25:14 WARN DAGScheduler: Broadcasting large task binary with size 1209.4 KiB\n",
      "24/08/21 08:26:35 WARN DAGScheduler: Broadcasting large task binary with size 1234.7 KiB\n",
      "24/08/21 08:28:37 WARN DAGScheduler: Broadcasting large task binary with size 1243.1 KiB\n",
      "[Stage 1040:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+---------+-----------+-------+------+-----------+\n",
      "| acnt_no|symbol|userIndex|itemIndex| prediction|acnt_no|symbol|final_score|\n",
      "+--------+------+---------+---------+-----------+-------+------+-----------+\n",
      "|00000002|   ACV|  17914.0|    167.0| 0.55084217|   NULL|  NULL|       NULL|\n",
      "|00000002|   AFC|  17914.0|   1559.0| 0.03381136|   NULL|  NULL|       NULL|\n",
      "|00000002|   AMD|  17914.0|    136.0|  1.4452658|   NULL|  NULL|       NULL|\n",
      "|00000002|   APT|  17914.0|   1115.0| 0.07506817|   NULL|  NULL|       NULL|\n",
      "|00000002|   AVF|  17914.0|    515.0| 0.31911272|   NULL|  NULL|       NULL|\n",
      "|00000002|   BAL|  17914.0|    951.0| 0.26299208|   NULL|  NULL|       NULL|\n",
      "|00000002|   BBT|  17914.0|    952.0|  0.0714774|   NULL|  NULL|       NULL|\n",
      "|00000002|   BCF|  17914.0|    924.0| 0.31961817|   NULL|  NULL|       NULL|\n",
      "|00000002|   BDG|  17914.0|    697.0| 0.13765448|   NULL|  NULL|       NULL|\n",
      "|00000002|   BDT|  17914.0|    791.0| 0.62766224|   NULL|  NULL|       NULL|\n",
      "|00000002|   BKH|  17914.0|   1408.0|0.075907566|   NULL|  NULL|       NULL|\n",
      "|00000002|   BPW|  17914.0|   1337.0| 0.13132277|   NULL|  NULL|       NULL|\n",
      "|00000002|   BST|  17914.0|   1146.0| 0.16768205|   NULL|  NULL|       NULL|\n",
      "|00000002|   BTS|  17914.0|    412.0| 0.30302882|   NULL|  NULL|       NULL|\n",
      "|00000002|   BVH|  17914.0|     23.0| 0.31429577|   NULL|  NULL|       NULL|\n",
      "|00000002|   CDG|  17914.0|   1264.0|  0.1334994|   NULL|  NULL|       NULL|\n",
      "|00000002|   CGV|  17914.0|    834.0|   1.254009|   NULL|  NULL|       NULL|\n",
      "|00000002|   CHC|  17914.0|   1195.0| 0.08920823|   NULL|  NULL|       NULL|\n",
      "|00000002|   CIA|  17914.0|    635.0|  0.5169226|   NULL|  NULL|       NULL|\n",
      "|00000002|   CKV|  17914.0|   1065.0| 0.08676781|   NULL|  NULL|       NULL|\n",
      "+--------+------+---------+---------+-----------+-------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfs_pred_exclude_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c958db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 07:58:16 WARN Column: Constructing trivially true equals predicate, 'acnt_no#0 = acnt_no#0'. Perhaps you need to use aliases.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `train`.`Rating` cannot be resolved. Did you mean one of the following? [`acnt_no`, `symbol`, `userIndex`, `itemIndex`, `prediction`, `acnt_no`, `symbol`, `final_score`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Remove seen items.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dfs_pred_exclude_train \u001b[38;5;241m=\u001b[39m dfs_pred\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      9\u001b[0m     train\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m     (dfs_pred[USER_COL] \u001b[38;5;241m==\u001b[39m train[USER_COL])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m&\u001b[39m (dfs_pred[ITEM_COL] \u001b[38;5;241m==\u001b[39m train[ITEM_COL]),\n\u001b[1;32m     12\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m dfs_pred_final \u001b[38;5;241m=\u001b[39m dfs_pred_exclude_train\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mdfs_pred_exclude_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.Rating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misNull()\n\u001b[1;32m     17\u001b[0m )\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m USER_COL, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m ITEM_COL, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m dfs_pred_final\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:3080\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001b[39;00m\n\u001b[1;32m   3009\u001b[0m \n\u001b[1;32m   3010\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3077\u001b[0m \u001b[38;5;124;03m+---+----+\u001b[39;00m\n\u001b[1;32m   3078\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 3080\u001b[0m     jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n\u001b[1;32m   3082\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Column):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `train`.`Rating` cannot be resolved. Did you mean one of the following? [`acnt_no`, `symbol`, `userIndex`, `itemIndex`, `prediction`, `acnt_no`, `symbol`, `final_score`]."
     ]
    }
   ],
   "source": [
    "# Get the cross join of all user-item pairs and score them.\n",
    "users = train.select(USER_COL).distinct()\n",
    "items = train.select(ITEM_COL).distinct()\n",
    "user_item = users.crossJoin(items)\n",
    "dfs_pred = model.transform(user_item)\n",
    "\n",
    "# Remove seen items.\n",
    "dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "    train.alias(\"train\"),\n",
    "    (dfs_pred[USER_COL] == train[USER_COL])\n",
    "    & (dfs_pred[ITEM_COL] == train[ITEM_COL]),\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "dfs_pred_final = dfs_pred_exclude_train.filter(\n",
    "    dfs_pred_exclude_train[\"train.Rating\"].isNull()\n",
    ").select(\"pred.\" + USER_COL, \"pred.\" + ITEM_COL, \"pred.\" + \"prediction\")\n",
    "\n",
    "dfs_pred_final.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0dc7e",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0d0a5-ae08-4e65-8ed7-06f889341945",
   "metadata": {},
   "source": [
    "### Transform label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84240247-e308-4e35-8a1b-babc97098a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\n",
    "    r\"data/label/data_process_UserItemRating_min5items_train.csv\",\n",
    "    sep=\"|\",\n",
    "    dtype={\"acnt_no\": str},\n",
    ")\n",
    "test = pd.read_csv(\n",
    "    r\"data/label/data_process_UserItemRating_min5items_test.csv\",\n",
    "    sep=\"|\",\n",
    "    dtype={\"acnt_no\": str},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
