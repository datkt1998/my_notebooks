{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8245dbc-98b2-4906-abd4-3efc30dac997",
   "metadata": {},
   "source": [
    "# CF - ALS (by Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee7eb1-9ebc-495b-8962-fb4b6f5403fe",
   "metadata": {},
   "source": [
    "## Algorithm Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3189c4-f144-4626-83c7-14fd0229cb13",
   "metadata": {},
   "source": [
    "### Matrix factorization for collaborative filtering problem\n",
    "\n",
    "Matrix factorization tries to find latent factors that represent intrinsic user and item attributes in a lower dimension. \n",
    "\n",
    "1. **Prediction Formula**\n",
    "\n",
    "$$\\hat r_{u,i} = q_{i}^{T}p_{u}$$\n",
    "\n",
    "where:\n",
    "- $\\hat r_{u,i}$ is the predicted ratings for user $u$ and item $i$\n",
    "- $q_{i}$ and $p_{u}$ are latent factors for item and user, respectively.\n",
    "\n",
    "---> The goal is to find these latent factors $q_{i}^{T}$ and $p_{u}$ that best approximate the observed ratings.\n",
    "\n",
    "2. **Optimization Problem**\n",
    "\n",
    "The challenge to the matrix factorization problem is to find $q_{i}^{T}$ and $p_{u}$. This is achieved by methods such as **matrix decomposition**. A learning approach is therefore developed to converge the decomposition results close to the observed ratings as much as possible. Furthermore, to avoid overfitting issue, the learning process is **regularized**. \n",
    "\n",
    "***The Goal is minimize the prediction error***:\n",
    "\n",
    "$$\\min\\sum(r_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n",
    "\n",
    "where $\\lambda$ is a the regularization parameter that controls the trade-off between fitting the data well and keeping the model simple. \n",
    "\n",
    "**Trường hợp phải tính toán rating dựa trên interaction của KH với items - Implicit Feedback**\n",
    "\n",
    "In case **explict ratings** are not available, **implicit ratings** which are usually derived from users' historical interactions with the items (e.g., *clicks*, *views*, *purchases*, etc.). To account for such implicit ratings, the original matrix factorization algorithm can be formulated as \n",
    "\n",
    "$$\\min\\sum c_{u,i}(p_{u,i} - q_{i}^{T}p_{u})^2 + \\lambda(||q_{i}||^2 + ||p_{u}||^2)$$\n",
    "\n",
    "where:\n",
    "- $c_{u,i}=1+\\alpha r_{u,i}$: là trọng số cho từng tương tác. Nó phụ thuộc vào giá trị $r_{u,i}$, là một biểu thị của sự yêu thích của người dùng $u$ cho item $i$, giúp mô hình nhận biết rằng không phải tất cả các tương tác đều có giá trị như nhau. Một tương tác với số lần nhấp nhiều hơn sẽ được coi là quan trọng hơn so với một tương tác có ít lần nhấp.\n",
    "- $p_{u,i}=1$ if $r_{u,i}>0$ else $p_{u,i}=0$ (if $r_{u,i}=0$): Điều này có nghĩa là chỉ khi có tương tác, chúng ta mới tính đến phản hồi đó\n",
    "  > $r_{u,i}$ is a numerical representation of users' preferences (e.g., number of clicks, etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da33bc-1c9a-49b6-8b4c-57a25efb0a5e",
   "metadata": {},
   "source": [
    "### Alternating Least Square (ALS)\n",
    "\n",
    "Owing to the term of $q_{i}^{T}p_{u}$ the loss function is non-convex. Gradient descent method can be applied but this will incur expensive computations. An Alternating Least Square (ALS) algorithm was therefore developed to overcome this issue. \n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTGkKzxWWEfGMc2qp2b4wQXdgkMNxt6ijs9tg&s\">\n",
    "\n",
    "The basic idea of ALS is to learn one of $q$ and $p$ at a time for optimization while keeping the other as constant. This makes the objective at each iteration convex and solvable. The alternating between $q$ and $p$ stops when there is convergence to the optimal. It is worth noting that this iterative computation can be parallelised and/or distributed, which makes the algorithm desirable for use cases where the dataset is large and thus the user-item rating matrix is super sparse (as is typical in recommendation scenarios). A comprehensive discussion of ALS and its distributed computation can be found [here](http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35e4f5-a942-4e5f-8d8a-ae4099f207a9",
   "metadata": {},
   "source": [
    "### Spark Mllib implementation\n",
    "\n",
    "The matrix factorization algorithm is available as `ALS` module in [Spark `ml`](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) for DataFrame or [Spark `mllib`](https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) for RDD. \n",
    "\n",
    "* The uniqueness of ALS implementation is that it distributes the matrix factorization model training by using \"Alternating Least Square\" method. \n",
    "* In the training method, there are parameters that can be selected to control the model performance.\n",
    "* Both explicit and implicit ratings are supported by Spark ALS model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969dcac-c858-40eb-aae6-510bbddf7a81",
   "metadata": {},
   "source": [
    "## Setup enviroment and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e530b85-16d0-43bc-ab78-71150fbe04f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]\n",
      "Pandas version: 2.2.2\n",
      "PySpark version: 3.5.2\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.functions import col\n",
    "# from pyspark.ml.tuning import CrossValidator\n",
    "# from pyspark.sql.types import StructType, StructField\n",
    "# from pyspark.sql.types import FloatType, IntegerType, LongType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# from recommenders.datasets import movielens\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.evaluation.spark_evaluation import (\n",
    "    # SparkRankingEvaluation,\n",
    "    SparkRatingEvaluation,\n",
    ")\n",
    "# from recommenders.tuning.parameter_sweep import generate_param_grid\n",
    "# from recommenders.datasets.spark_splitters import spark_random_split\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6d418-7ca8-4be0-9731-8e82ff190c74",
   "metadata": {},
   "source": [
    "**model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2acad2-fc6b-4b43-9627-6f1e6e2ddaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper-parameters\n",
    "# rank of the factorization\n",
    "RANK = 10\n",
    "# Số lượng vòng lặp tối đa\n",
    "MAX_ITER = 15\n",
    "# Hệ số kiểm soát regularization\n",
    "REG_PARAM = 0.05\n",
    "\n",
    "# Number of recommended items\n",
    "K = 15\n",
    "\n",
    "# rating columns\n",
    "USER_COL = \"acnt_no\"\n",
    "ITEM_COL = \"symbol\"\n",
    "RATING_COL = \"final_score\"\n",
    "\n",
    "# Path to the CSV file\n",
    "train_datapath = r\"data/label/data_process_UserItemRating_min5items_train.csv\"\n",
    "test_datapath = r\"data/label/data_process_UserItemRating_min5items_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da9d29-3c50-4fe3-b787-7e64aa4ae9ee",
   "metadata": {},
   "source": [
    "**Spark session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37dd5a48-6355-4ed3-ac61-1ef7c138d48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 19:40:03 WARN Utils: Your hostname, ats-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.1.3 instead (on interface en0)\n",
      "24/08/20 19:40:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/20 19:40:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = start_or_get_spark(\"ALS\", memory=\"16g\")\n",
    "# spark = SparkSession.builder.appName(\"ALS recommendation spark session\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97735ecd-da97-42e0-9c6d-0651f295b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP SESSION\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d9e7ba-d284-47aa-9987-b48acfc01b56",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8494c9-e2f9-408d-a50e-ba2046fbaeef",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d7e46f-f30a-4fbb-8470-a49ffb72a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "| acnt_no|symbol|final_score|\n",
      "+--------+------+-----------+\n",
      "|00018956|   BID|       0.02|\n",
      "|00024968|   BID|       0.03|\n",
      "|00025170|   VIX|      3.924|\n",
      "+--------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(USER_COL, StringType(), True),\n",
    "        StructField(ITEM_COL, StringType(), True),\n",
    "        StructField(RATING_COL, FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read the CSV file into a DataFrame with the specified schema\n",
    "train = spark.read.csv(train_datapath, sep=\"|\", header=True, schema=schema)\n",
    "test = spark.read.csv(test_datapath, sep=\"|\", header=True, schema=schema)\n",
    "\n",
    "# Show the DataFrame to verify\n",
    "test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fca6a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+-------------------+\n",
      "|summary|          acnt_no|symbol|        final_score|\n",
      "+-------+-----------------+------+-------------------+\n",
      "|  count|           532012|532012|             532012|\n",
      "|   mean|59160.27250137215|  NULL|0.22697969638126742|\n",
      "| stddev|52375.70211050739|  NULL| 0.8522782496254795|\n",
      "|    min|         00000002|   AAA|              0.002|\n",
      "|    max|         00186799|   YTC|             12.869|\n",
      "+-------+-----------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1ec00-eb3a-4aac-8496-c3cc6c37ba66",
   "metadata": {},
   "source": [
    "**Preprocessing pipeline**\n",
    "\n",
    "Do mô hình ALS spark yêu câu dữ liệu **user** và **item** ở dạng `numeric`, nên cần xây dựng pipeline chuyển đổi dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5245b6c-cbae-4249-bfff-f66bdb6cd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Chuyển đổi cột user\n",
    "user_indexer = StringIndexer(inputCol=USER_COL, outputCol=\"userIndex\")\n",
    "\n",
    "# Chuyển đổi cột item\n",
    "item_indexer = StringIndexer(inputCol=ITEM_COL, outputCol=\"itemIndex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3e1eb-8917-4e12-a143-025fa746e41c",
   "metadata": {},
   "source": [
    "**Setup model**\n",
    "\n",
    "It is worth noting that Spark ALS model allows dropping cold users to favor a robust evaluation with the testing data. In case there are cold users, Spark ALS implementation allows users to drop cold users in order to make sure evaluations on the prediction results are sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822f2594-d4cf-456e-a193-0a83829eb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    maxIter=MAX_ITER,\n",
    "    rank=RANK,\n",
    "    regParam=REG_PARAM,\n",
    "    userCol=\"userIndex\",\n",
    "    itemCol=\"itemIndex\",\n",
    "    ratingCol=RATING_COL,\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8625b8-95c6-41fc-8c61-add091080eaa",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8270c32-a74e-400a-b866-77b3f1b0902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 19:40:33 WARN DAGScheduler: Broadcasting large task binary with size 1106.3 KiB\n",
      "24/08/20 19:40:33 WARN DAGScheduler: Broadcasting large task binary with size 1108.6 KiB\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n",
      "24/08/20 19:40:33 WARN DAGScheduler: Broadcasting large task binary with size 1110.1 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=24376Kb max_used=24383Kb free=106696Kb\n",
      " bounds [0x00000001071e8000, 0x00000001089d8000, 0x000000010f1e8000]\n",
      " total_blobs=9483 nmethods=8496 adapters=898\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 19:40:34 WARN DAGScheduler: Broadcasting large task binary with size 1111.4 KiB\n",
      "24/08/20 19:40:34 WARN DAGScheduler: Broadcasting large task binary with size 1110.4 KiB\n",
      "24/08/20 19:40:35 WARN DAGScheduler: Broadcasting large task binary with size 1111.7 KiB\n",
      "24/08/20 19:40:35 WARN DAGScheduler: Broadcasting large task binary with size 1112.4 KiB\n",
      "24/08/20 19:40:35 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/20 19:40:36 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "24/08/20 19:40:36 WARN DAGScheduler: Broadcasting large task binary with size 1117.0 KiB\n",
      "24/08/20 19:40:36 WARN DAGScheduler: Broadcasting large task binary with size 1118.4 KiB\n",
      "24/08/20 19:40:36 WARN DAGScheduler: Broadcasting large task binary with size 1119.8 KiB\n",
      "24/08/20 19:40:36 WARN DAGScheduler: Broadcasting large task binary with size 1121.1 KiB\n",
      "24/08/20 19:40:36 WARN DAGScheduler: Broadcasting large task binary with size 1122.5 KiB\n",
      "24/08/20 19:40:36 WARN DAGScheduler: Broadcasting large task binary with size 1123.9 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1125.3 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1126.7 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1128.1 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1129.5 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1130.9 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1132.2 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1133.6 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1135.0 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1136.4 KiB\n",
      "24/08/20 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1137.8 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1139.2 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1140.6 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1141.9 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1143.3 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1144.7 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1146.1 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1147.5 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1148.9 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1150.3 KiB\n",
      "24/08/20 19:40:38 WARN DAGScheduler: Broadcasting large task binary with size 1151.7 KiB\n",
      "24/08/20 19:40:39 WARN DAGScheduler: Broadcasting large task binary with size 1153.0 KiB\n",
      "24/08/20 19:40:39 WARN DAGScheduler: Broadcasting large task binary with size 1154.4 KiB\n",
      "24/08/20 19:40:39 WARN DAGScheduler: Broadcasting large task binary with size 1156.4 KiB\n",
      "24/08/20 19:40:39 WARN DAGScheduler: Broadcasting large task binary with size 1155.0 KiB\n"
     ]
    }
   ],
   "source": [
    "# make pipeline\n",
    "model_pipeline = Pipeline(stages=[user_indexer, item_indexer, als])\n",
    "\n",
    "# fit transform train data\n",
    "model = model_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f95b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trans = model.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c38885-7164-484b-8bb5-308eb7016e94",
   "metadata": {},
   "source": [
    "**Model Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a50a0e6-893d-4c21-a871-b4942bac4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9d6f7-a950-43e5-955f-f2ec509152ad",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5165e36-f3f1-47d9-8070-647558c91213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 19:43:05 WARN DAGScheduler: Broadcasting large task binary with size 1089.5 KiB\n",
      "24/08/20 19:43:05 WARN DAGScheduler: Broadcasting large task binary with size 1164.4 KiB\n",
      "24/08/20 19:43:05 WARN DAGScheduler: Broadcasting large task binary with size 1163.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+---------+---------+----------+\n",
      "| acnt_no|symbol|final_score|userIndex|itemIndex|prediction|\n",
      "+--------+------+-----------+---------+---------+----------+\n",
      "|00102155|   VNM|      0.068|   6654.0|     14.0|0.04956831|\n",
      "|00002006|   NTP|       0.03|   7754.0|    322.0|0.51148427|\n",
      "|00032726|   MBB|      0.026|   1580.0|      0.0|0.17433679|\n",
      "+--------+------+-----------+---------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04bd47f8-0186-41f7-92e2-6e13b6fbe469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 19:43:32 WARN DAGScheduler: Broadcasting large task binary with size 1088.5 KiB\n",
      "24/08/20 19:43:32 WARN DAGScheduler: Broadcasting large task binary with size 1164.4 KiB\n",
      "24/08/20 19:43:32 WARN DAGScheduler: Broadcasting large task binary with size 1163.0 KiB\n",
      "24/08/20 19:43:34 WARN DAGScheduler: Broadcasting large task binary with size 1088.5 KiB\n",
      "24/08/20 19:43:34 WARN DAGScheduler: Broadcasting large task binary with size 1164.4 KiB\n",
      "24/08/20 19:43:34 WARN DAGScheduler: Broadcasting large task binary with size 1163.0 KiB\n",
      "24/08/20 19:43:35 WARN DAGScheduler: Broadcasting large task binary with size 1089.0 KiB\n",
      "24/08/20 19:43:35 WARN DAGScheduler: Broadcasting large task binary with size 1164.4 KiB\n",
      "24/08/20 19:43:35 WARN DAGScheduler: Broadcasting large task binary with size 1163.0 KiB\n",
      "24/08/20 19:43:35 WARN DAGScheduler: Broadcasting large task binary with size 1088.8 KiB\n",
      "24/08/20 19:43:38 WARN DAGScheduler: Broadcasting large task binary with size 1236.0 KiB\n",
      "24/08/20 19:43:40 WARN DAGScheduler: Broadcasting large task binary with size 1250.8 KiB\n",
      "24/08/20 19:43:43 WARN DAGScheduler: Broadcasting large task binary with size 1251.7 KiB\n",
      "24/08/20 19:43:44 WARN DAGScheduler: Broadcasting large task binary with size 1089.0 KiB\n",
      "24/08/20 19:43:44 WARN DAGScheduler: Broadcasting large task binary with size 1164.4 KiB\n",
      "24/08/20 19:43:44 WARN DAGScheduler: Broadcasting large task binary with size 1163.0 KiB\n",
      "24/08/20 19:43:44 WARN DAGScheduler: Broadcasting large task binary with size 1088.8 KiB\n",
      "24/08/20 19:43:46 WARN DAGScheduler: Broadcasting large task binary with size 1245.4 KiB\n",
      "24/08/20 19:43:47 WARN DAGScheduler: Broadcasting large task binary with size 1089.0 KiB\n",
      "24/08/20 19:43:47 WARN DAGScheduler: Broadcasting large task binary with size 1164.4 KiB\n",
      "24/08/20 19:43:47 WARN DAGScheduler: Broadcasting large task binary with size 1163.0 KiB\n",
      "24/08/20 19:43:48 WARN DAGScheduler: Broadcasting large task binary with size 1088.8 KiB\n",
      "24/08/20 19:43:49 WARN DAGScheduler: Broadcasting large task binary with size 1243.3 KiB\n",
      "[Stage 412:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score = 0.5246021666414594\n",
      "MAE score = 0.14616747823206927\n",
      "R2 score = 0.6211230665514835\n",
      "Explained variance score = 0.6218804488068812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluations = SparkRatingEvaluation(\n",
    "    predictions,\n",
    "    predictions,\n",
    "    col_user=\"userIndex\",\n",
    "    col_item=\"itemIndex\",\n",
    "    col_rating=RATING_COL,\n",
    "    col_prediction=\"prediction\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"RMSE score = {}\".format(evaluations.rmse()),\n",
    "    \"MAE score = {}\".format(evaluations.mae()),\n",
    "    \"R2 score = {}\".format(evaluations.rsquared()),\n",
    "    \"Explained variance score = {}\".format(evaluations.exp_var()),\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bcfce3-88e7-4f9c-b056-2aa5f41fecc1",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0d0a5-ae08-4e65-8ed7-06f889341945",
   "metadata": {},
   "source": [
    "### Transform label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84240247-e308-4e35-8a1b-babc97098a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\n",
    "    r\"data/label/data_process_UserItemRating_min5items_train.csv\",\n",
    "    sep=\"|\",\n",
    "    dtype={\"acnt_no\": str},\n",
    ")\n",
    "test = pd.read_csv(\n",
    "    r\"data/label/data_process_UserItemRating_min5items_test.csv\",\n",
    "    sep=\"|\",\n",
    "    dtype={\"acnt_no\": str},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
