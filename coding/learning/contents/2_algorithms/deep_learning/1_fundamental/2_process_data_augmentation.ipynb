{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474c149d-af42-4bb8-9a89-4dbbde44d9c3",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "Các bộ dữ liệu deep learning thường có kích thước rất lớn. Trong quá trình huấn luyện các model deep learning chúng ta không thể truyền toàn bộ dữ liệu vào mô hình cùng một lúc bởi dữ liệu thường có kích thước lớn hơn RAM máy tính. Xuất phát từ lý do này, các framework deep learning đều hỗ trợ các hàm huấn luyện mô hình theo generator. Dữ liệu sẽ không được khởi tạo ngay toàn bộ từ đầu mà sẽ huấn luyện đến đâu sẽ được khởi tạo đến đó theo từng phần nhỏ gọi là batch.\n",
    "\n",
    "Tùy theo định dạng dữ liệu là text, image, data frame, numpy array,… mà chúng ta sẽ sử dụng những module tạo dữ liệu huấn luyện khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60970fc9-289e-4369-b0eb-02f50f9dc238",
   "metadata": {},
   "source": [
    "__Generator__\n",
    "\n",
    "Generator sẽ không trả về kết quả ngay mà chỉ tạo sẵn các ô nhớ lưu hàm generator mô tả cách tính. Do đó chúng ta sẽ không tốn chi phí thời gian để thực hiện các phép tính. Thực tế là chúng ta đang nợ máy tính kết quả trả về. Chỉ khi nào được gọi tên bằng cách kích hoạt trong hàm next() thì mới tính kết quả.\n",
    "\n",
    "Chúng ta có thể thấy generator có lợi thế là:\n",
    "\n",
    "- Không sinh toàn bộ dữ liệu cùng một lúc, do đó sẽ nâng cao hiệu suất vì sử dụng ít bộ nhớ hơn.\n",
    "- Không phải chờ toàn bộ các vòng lặp được xử lý xong thì mới xử lý tiếp nên tiết kiệm thời gian tính toán.\n",
    "\n",
    "Đó chính là lý do generator chính là giải pháp được lựa chọn cho huấn luyện mô hình deep learning với dữ liệu lớn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f220c9a3-7fa3-4256-b57b-70c25a363dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scales of origin balance:  [<generator object _gen_interest_rate at 0x106acd5b0>, <generator object _gen_interest_rate at 0x106acd1c0>, <generator object _gen_interest_rate at 0x106acd3f0>, <generator object _gen_interest_rate at 0x106acd7e0>, <generator object _gen_interest_rate at 0x106acd850>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.010000000000000009,\n",
       " 0.030301000000000133,\n",
       " 0.061520150601000134,\n",
       " 0.09368527268436089,\n",
       " 0.12682503013196977]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _gen_interest_rate(month):\n",
    "    yield (1+0.01)**month - 1\n",
    "\n",
    "\n",
    "periods = [1, 3, 6, 9, 12]\n",
    "scales = [_gen_interest_rate(month) for month in periods]\n",
    "print('scales of origin balance: ', scales)\n",
    "\n",
    "[next(_gen_interest_rate(n)) for n in periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb0e62-d860-4f09-87d3-30aa727dbd17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## In memory Dataset\n",
    "Khởi tạo các dataset ngay từ đầu và dữ liệu được lưu trữ trên memory. Phương pháp In memory Dataset sẽ phù hợp với các bộ dữ liệu kích thước nhỏ mà RAM có thể load được. Quá trình huấn luyện theo cách này thì nhanh hơn so với phương pháp Generator Dataset vì dữ liệu đã được chuẩn bị sẵn mà không tốn thời gian chờ khởi tạo batch. Tuy nhiên dễ xảy ra out of memory trong quá trình huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef8753b-7c20-4da8-a1ee-6dd4ef0e0442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d7554-fb35-4421-b964-1fa843383253",
   "metadata": {},
   "source": [
    "Như vậy các dữ liệu __train__ và __test__ của bộ dữ liệu __mnist__ đã được load vào bộ nhớ. Tiếp theo chúng ta sẽ khởi tạo Dataset cho những dữ liệu in memory này bằng hàm `tf.data.Dataset.from_tensor_slices()`. Hàm này sẽ khai báo dữ liệu đầu vào cho mô hình huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03223b32-f53b-4fb9-9aec-b4c7eede6844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45464fee-14d6-4e74-a42b-0f55fccfef77",
   "metadata": {},
   "source": [
    "Khi đó chúng ta đã có thể fit vào mô hình huấn luyện các dữ liệu được truyền vào tf.Dataset là (X_train, y_train).\n",
    "\n",
    "Chúng ta cũng có thể áp dụng các phép biến đổi bằng các hàm như `Dataset.map()` hoặc `Dataset.batch()` để biến đổi dữ liệu trước khi fit vào model. Các bạn xem thêm tại `tf.Dataset`. Chẳng hạn trước khi truyền batch vào huấn luyện tôi sẽ thực hiện chuẩn hóa batch theo phân phối chuẩn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35935aa2-0994-4b74-bf17-9733535c92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.backend import std, mean\n",
    "from tensorflow.math import reduce_std, reduce_mean\n",
    "\n",
    "def _normalize(X_batch, y_batch):\n",
    "    '''\n",
    "    X_batch: matrix digit images, shape batch_size x 28 x 28\n",
    "    y_batch: labels of digit.\n",
    "    '''\n",
    "    X_batch = tf.cast(X_batch, dtype = tf.float32)\n",
    "    # Padding về 2 chiều các giá trị 0 để được shape là 32 x 32\n",
    "    pad = tf.constant([[0, 0], [2, 2], [2, 2]])\n",
    "    X_batch = tf.pad(X_batch, paddings=pad, mode='CONSTANT', constant_values=0)\n",
    "    X_batch = tf.expand_dims(X_batch, axis=-1)\n",
    "    mean = reduce_mean(X_batch)\n",
    "    std = reduce_std(X_batch)\n",
    "    X_norm = (X_batch-mean)/std\n",
    "    return X_norm, y_batch\n",
    "\n",
    "# batch(32): Trích xuất ra từ list (X_train, y_train) các batch_size có kích thước là 32.\n",
    "# map(_normalize): Mapping đầu vào là các batch (X_batch, y_batch) kích thước 32 vào hàm số _normalize()\n",
    "# Kết quả trả về là giá trị đã chuẩn hóa theo batch của X_batch và y_batch\n",
    "train_dataset = train_dataset.batch(32).map(_normalize)\n",
    "valid_dataset = valid_dataset.batch(32).map(_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f2752-7a2a-48e9-a984-fee7eefac6ed",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f42e6af9-f2ba-45aa-bb41-a8a133632732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_1.00_32 (Function  (None, 1, 1, 1024)       3228288   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,238,538\n",
      "Trainable params: 3,216,650\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "base_extractor = MobileNet(input_shape = (32, 32, 1), include_top = False, weights = None)\n",
    "flat = Flatten()\n",
    "den = Dense(10, activation='softmax')\n",
    "model = Sequential([base_extractor, \n",
    "                   flat,\n",
    "                   den])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f3f1a-aefd-41be-9ae0-6f68f6df6fa6",
   "metadata": {},
   "source": [
    "fit data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67471087-6d09-4b45-b799-20108045d52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 11:12:06.157131: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 68s 33ms/step - loss: 0.5088 - accuracy: 0.8390 - val_loss: 0.2128 - val_accuracy: 0.9398\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 63s 33ms/step - loss: 0.1546 - accuracy: 0.9567 - val_loss: 0.1376 - val_accuracy: 0.9616\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 62s 33ms/step - loss: 0.1276 - accuracy: 0.9672 - val_loss: 0.1276 - val_accuracy: 0.9689\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 63s 33ms/step - loss: 0.1001 - accuracy: 0.9742 - val_loss: 0.1002 - val_accuracy: 0.9732\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 63s 33ms/step - loss: 0.0882 - accuracy: 0.9783 - val_loss: 0.0625 - val_accuracy: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177635e70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(), loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(train_dataset, validation_data = valid_dataset, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f9a91-1ddb-4ff0-94d4-14c8f452d1ba",
   "metadata": {},
   "source": [
    "## Generator Dataset\n",
    "Theo cách Generator Dataset chúng ta sẽ qui định cách mà dữ liệu được tạo ra như thế nào thông qua một hàm generator. Quá trình huấn luyện đến đâu sẽ tạo batch đến đó. Do đó các bộ dữ liệu big data có thể được load theo từng batch sao cho kích thước vừa được dung lượng RAM. Theo cách huấn luyện này chúng ta có thể huấn luyện được các bộ dữ liệu có kích thước lớn hơn nhiều so với RAM bằng cách chia nhỏ chúng theo batch. Đồng thời có thể áp dụng thêm các step preprocessing data trước khi dữ liệu được đưa vào huấn luyện. Do đó đây thường là phương pháp được ưa chuộng khi huấn luyện các model deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7cd1d-def9-42d7-ba31-db236c2effd5",
   "metadata": {},
   "source": [
    "### Ví dụ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89560a27-b043-4f89-916f-c9ade90f1d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hochiminh</td>\n",
       "      <td>hủ tiếu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hochiminh</td>\n",
       "      <td>banh phở sài gòn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hanoi</td>\n",
       "      <td>chả cá lã vọng hà nội</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hochiminh</td>\n",
       "      <td>hủ tiếu nam vang sài gòn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hanoi</td>\n",
       "      <td>chả cá</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city                      food\n",
       "12  hochiminh                   hủ tiếu\n",
       "10  hochiminh          banh phở sài gòn\n",
       "1       hanoi     chả cá lã vọng hà nội\n",
       "8   hochiminh  hủ tiếu nam vang sài gòn\n",
       "5       hanoi                    chả cá"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hanoi = ['bún chả hà nội', 'chả cá lã vọng hà nội', 'cháo lòng hà nội', 'ô mai sấu hà nội', 'ô mai', 'chả cá', 'cháo lòng']\n",
    "hochiminh = ['bánh canh sài gòn', 'hủ tiếu nam vang sài gòn', 'hủ tiếu bò sài gòn', 'banh phở sài gòn', 'bánh phở', 'hủ tiếu']\n",
    "city = ['hanoi'] * len(hanoi) + ['hochiminh'] * len(hochiminh)\n",
    "corpus = hanoi+hochiminh\n",
    "\n",
    "data = pd.DataFrame({'city': city, 'food': corpus})\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97cca116-21a6-4edb-9e72-aac77151734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk': 0,\n",
       " 'bún': 1,\n",
       " 'chả': 2,\n",
       " 'hà': 3,\n",
       " 'nội': 4,\n",
       " 'cá': 5,\n",
       " 'lã': 6,\n",
       " 'vọng': 7,\n",
       " 'cháo': 8,\n",
       " 'lòng': 9,\n",
       " 'ô': 10,\n",
       " 'mai': 11,\n",
       " 'sấu': 12,\n",
       " 'bánh': 13,\n",
       " 'canh': 14,\n",
       " 'sài': 15,\n",
       " 'gòn': 16,\n",
       " 'hủ': 17,\n",
       " 'tiếu': 18,\n",
       " 'nam': 19,\n",
       " 'vang': 20,\n",
       " 'bò': 21,\n",
       " 'banh': 22,\n",
       " 'phở': 23}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Voc(object):\n",
    "    \"\"\"Class Voc có tác dụng khởi tạo index từ điển cho toàn bộ corpus (bộ văn bản)\"\"\"\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus                     # list toàn bộ tên các món ăn\n",
    "        self.dictionary = {'unk': 0}\n",
    "        self._initialize_dict(corpus)\n",
    "\n",
    "    def _add_dict_sentence(self, sentence):\n",
    "        words = sentence.split(' ')\n",
    "        for word in words:\n",
    "            if word not in self.dictionary.keys():\n",
    "                max_indice = max(self.dictionary.values())\n",
    "                self.dictionary[word] = (max_indice + 1)\n",
    "\n",
    "    def _initialize_dict(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            self._add_dict_sentence(sentence)\n",
    "\n",
    "    def _tokenize(self, sentence):\n",
    "        words = sentence.split(' ')\n",
    "        token_seq = [self.dictionary[word] for word in words]\n",
    "        return np.array(token_seq)\n",
    "\n",
    "voc = Voc(corpus = corpus)\n",
    "voc.dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cce029-7f31-4dca-bee8-b615c07e5d30",
   "metadata": {},
   "source": [
    "Tiếp theo chúng ta sẽ khởi tạo một `random_generator` có tác dụng lựa chọn ngẫu nhiên một tên món ăn trong `corpus` và `tokenize` chúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6b176d-c266-4512-9d12-17c5c12eee5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_FlatMapDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.float16, name=None), TensorSpec(shape=(), dtype=tf.float16, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "cat_indices = {\n",
    "    'hanoi': 0,\n",
    "    'hochiminh': 1\n",
    "}\n",
    "\n",
    "def generators():\n",
    "    i = 0\n",
    "    while True:\n",
    "        i = np.random.choice(data.shape[0])\n",
    "        sentence = data.iloc[i, 1]\n",
    "        x_indice = voc._tokenize(sentence)\n",
    "        label = data.iloc[i, 0]\n",
    "        y_indice = cat_indices[label]\n",
    "        yield x_indice, y_indice\n",
    "        i += 1\n",
    "\n",
    "random_generator = tf.data.Dataset.from_generator(\n",
    "    generators,\n",
    "    output_types = (tf.float16, tf.float16),\n",
    "    output_shapes = ((None,), ())\n",
    ")\n",
    "\n",
    "random_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3abdbf69-61e4-43ca-89c3-8f066d5ce0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.  2.  3.  4.  0.  0.]\n",
      " [ 8.  9.  3.  4.  0.  0.]\n",
      " [10. 11.  0.  0.  0.  0.]\n",
      " [13. 23.  0.  0.  0.  0.]\n",
      " [ 1.  2.  3.  4.  0.  0.]\n",
      " [10. 11.  0.  0.  0.  0.]\n",
      " [ 8.  9.  3.  4.  0.  0.]\n",
      " [10. 11.  0.  0.  0.  0.]\n",
      " [13. 23.  0.  0.  0.  0.]\n",
      " [17. 18.  0.  0.  0.  0.]\n",
      " [ 1.  2.  3.  4.  0.  0.]\n",
      " [10. 11. 12.  3.  4.  0.]\n",
      " [17. 18.  0.  0.  0.  0.]\n",
      " [ 2.  5.  0.  0.  0.  0.]\n",
      " [ 2.  5.  0.  0.  0.  0.]\n",
      " [10. 11.  0.  0.  0.  0.]\n",
      " [ 1.  2.  3.  4.  0.  0.]\n",
      " [17. 18. 21. 15. 16.  0.]\n",
      " [10. 11.  0.  0.  0.  0.]\n",
      " [ 2.  5.  6.  7.  3.  4.]], shape=(20, 6), dtype=float16)\n",
      "tf.Tensor([0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.], shape=(20,), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# hàm shuffle(20) có tác dụng trộn lẫn ngẫu nhiên dữ liệu\n",
    "# Sau đó dữ liệu được chia thành những batch có kích thước là 20 \n",
    "# padding giá trị 0 sao cho bằng với độ dài của câu dài nhất bằng hàm padded_batch()\n",
    "random_generator_batch = random_generator.shuffle(20).padded_batch(20, padded_shapes=([None], []))\n",
    "sequence_batch, label = next(iter(random_generator_batch))\n",
    "\n",
    "print(sequence_batch)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d47d1-71ce-4e96-b5fd-1dd001c9e2fd",
   "metadata": {},
   "source": [
    "### ImageGenerator\n",
    "ImageGenerator cũng là một dạng data generator được xây dựng trên framework keras và dành riêng cho dữ liệu ảnh.\n",
    "\n",
    "Đây là một high level function nên cú pháp đơn giản, rất dễ sử dụng nhưng khả năng tùy biến và can thiệp sâu vào dữ liệu kém.\n",
    "\n",
    "Khi khởi tạo `ImageGenerator` chúng ta sẽ khai báo các thủ tục preprocessing image trước khi đưa vào huấn luyện. Mình sẽ không quá đi sâu vào các kĩ thuật preprocessing data này. Bạn đọc quan tâm có thể xem thêm tại `ImageDataGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd0c50e9-3823-4357-a07c-4f5609f7d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1399 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import glob2\n",
    "root_folder = 'Datasets/Dog-Cat-Classifier/Data/Train_Data/'\n",
    "\n",
    "image_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255, \n",
    "    rotation_range = 20,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "\n",
    "images, labels = next(image_gen.flow_from_directory(root_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6049db3-4ad5-4e26-ba86-913a14e60ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 256, 256, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e7ccf-ebbb-446a-92e2-2276aab36cf9",
   "metadata": {},
   "source": [
    "Hàm `flow_from_directory()` sẽ có tác dụng đọc các ảnh từ __root_folder__ và lấy ra những thông tin bao gồm ma trận ảnh sau biến đổi và nhãn tương ứng. Cấu trúc cây thư mục của __root_folder__ có dạng như sau:\n",
    "```\n",
    "| root-folder\n",
    "----| sub-folder-class-1\n",
    "----| sub-folder-class-2\n",
    "----| ...\n",
    "----| sub-folder-class-C\n",
    "```\n",
    "Trong đó bên trong các __sub-folder-class-i__ là list toàn bộ các ảnh thuộc về một class. Hàm `flow_from_directory()` sẽ tự động xác định các file dữ liệu nào là ảnh để load vào quá trình huấn luyện mô hình. Ở đây trong root_folder chúng ta có 2 sub-folders tương ứng với 2 classes là dog, cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706ae23-2feb-485d-826d-1a5a79c87485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiếp theo ta sẽ khởi tạo một tf.Dataset từ generator thông qua hàm from_generator().\n",
    "# Khai báo bắt buộc định dạng dữ liệu input và output thông qua tham số output_types và output shape thông qua tham số output_shapes\n",
    "# Như vậy kết quả trả ra sẽ là những batch có kích thước 32 và ảnh có kích thước 256 x 256 và nhãn tương ứng của ảnh.\n",
    "image_gen_dataset = tf.data.Dataset.from_generator(\n",
    "    image_gen.flow_from_directory, \n",
    "    args = ([root_folder]),\n",
    "    output_types=(tf.float32, tf.float32), \n",
    "    output_shapes=([32,256,256,3], [32, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30204f3-b942-4c49-8fb8-36a07e62c1d4",
   "metadata": {},
   "source": [
    "### Customize ImageGenerator\n",
    "\n",
    "Giả sử bạn có một bộ dữ liệu ảnh mà kích thước các ảnh là khác biệt nhau. Đồng thời bạn cũng muốn can thiệp sâu hơn vào bức ảnh trước khi đưa vào huấn luyện như giảm nhiễu bằng bộ lọc __Gausianblur, rotate ảnh, crop, zoom ảnh__, …. Nếu sử dụng các hàm mặc định của image preprocessing trong `ImageGenerator` thì sẽ gặp hạn chế đó là bị giới hạn bởi một số phép biến đổi mà hàm này hỗ trợ. Sử dụng high level framework tiện thì rất tiện nhưng khi muốn can thiệp sâu thì rất khó. Muốn can thiệp được sâu vào bên trong các biến đổi chúng ta phải customize lại một chút `ImageGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6dbbaa4-f13e-4b0a-99cc-e9cd8b2d425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.7.0.72-cp37-abi3-macosx_11_0_arm64.whl (32.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.6/32.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /Users/datkhong/miniconda3/lib/python3.10/site-packages (from opencv-python) (1.24.2)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.7.0.72\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08d27d1a-ae08-4a1e-9003-573bc7074d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "import cv2\n",
    "import glob2\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,\n",
    "                 all_filenames, \n",
    "                 labels, \n",
    "                 batch_size, \n",
    "                 index2class,\n",
    "                 input_dim,\n",
    "                 n_channels,\n",
    "                 n_classes=2, \n",
    "                 shuffle=True):\n",
    "        '''\n",
    "        all_filenames: list toàn bộ các filename\n",
    "        labels: nhãn của toàn bộ các file\n",
    "        batch_size: kích thước của 1 batch\n",
    "        index2class: index của các class\n",
    "        input_dim: (width, height) đầu vào của ảnh\n",
    "        n_channels: số lượng channels của ảnh\n",
    "        n_classes: số lượng các class \n",
    "        shuffle: có shuffle dữ liệu sau mỗi epoch hay không?\n",
    "        '''\n",
    "        self.all_filenames = all_filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.index2class = index2class\n",
    "        self.input_dim = input_dim\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Số lượng step trong một epoch.\n",
    "        \n",
    "        return:\n",
    "          Trả về số lượng batch/1 epoch\n",
    "        '''\n",
    "        return int(np.floor(len(self.all_filenames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Trong quá trình huấn luyện chúng ta cần phải access vào từng batch trong bộ dữ liệu. \n",
    "        Hàm __getitem__() sẽ khởi tạo batch theo thứ tự của batch được truyền vào hàm.\n",
    "        \n",
    "        params:\n",
    "          index: index của batch\n",
    "        return:\n",
    "          X, y cho batch thứ index\n",
    "        '''\n",
    "        # Lấy ra indexes của batch thứ index\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # List all_filenames trong một batch\n",
    "        all_filenames_temp = [self.all_filenames[k] for k in indexes]\n",
    "\n",
    "        # Khởi tạo data\n",
    "        X, y = self.__data_generation(all_filenames_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        Đây là hàm được tự động run mỗi khi một epoch huấn luyện bắt đầu và kết thúc. \n",
    "        Tại hàm này chúng ta sẽ xác định các hành động khi bắt đầu hoặc kết thúc một epoch như: \n",
    "        - Có shuffle dữ liệu hay không?\n",
    "        - Điều chỉnh lại tỷ lệ các class tước khi fit vào model,….\n",
    "        \n",
    "        Shuffle dữ liệu khi epochs end hoặc start.\n",
    "        '''\n",
    "        self.indexes = np.arange(len(self.all_filenames))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, all_filenames_temp):\n",
    "        '''\n",
    "        Hàm này sẽ được gọi trong __getitem__(). __data_generation() sẽ trực tiếp \n",
    "        biến đổi dữ liệu và quyết định các kết quả dữ liệu trả về cho người dùng. \n",
    "        Tại hàm này ta có thể thực hiện các phép preprocessing image.\n",
    "        \n",
    "        params:\n",
    "          all_filenames_temp: list các filenames trong 1 batch\n",
    "        return:\n",
    "          Trả về giá trị cho một batch.\n",
    "        '''\n",
    "        X = np.empty((self.batch_size, *self.input_dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Khởi tạo dữ liệu\n",
    "        for i, fn in enumerate(all_filenames_temp):\n",
    "            # Đọc file từ folder name\n",
    "            img = cv2.imread(fn)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, self.input_dim)\n",
    "            label = os.path.basename(fn)\n",
    "            label = self.index2class[label[:3]]\n",
    "    \n",
    "            X[i,] = img\n",
    "\n",
    "            # Lưu class\n",
    "            y[i] = label\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db5932a3-8f95-4685-a3f7-8d1c2637b77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1399\n",
      "(32, 224, 224, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dict_labels = {\n",
    "    'dog': 0,\n",
    "    'cat': 1\n",
    "}\n",
    "root = r\"/Users/datkhong/Library/CloudStorage/GoogleDrive-k55.1613310017@ftu.edu.vn/My Drive/GitCode/My_learning/1. DA - DS/3. Learning/2_Notebooks/5_Deep_learning/\"\n",
    "root_folder = root + r'Datasets/Dog-Cat-Classifier/Data/Train_Data/*/*'\n",
    "fns = glob2.glob(root_folder)\n",
    "print(len(fns))\n",
    "\n",
    "image_generator = DataGenerator(\n",
    "    all_filenames = fns,\n",
    "    labels = None,\n",
    "    batch_size = 32,\n",
    "    index2class = dict_labels,\n",
    "    input_dim = (224, 224),\n",
    "    n_channels = 3,\n",
    "    n_classes = 2,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "X, y = image_generator.__getitem__(1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d6af0-ad97-4b48-a1b3-4e49dc946789",
   "metadata": {},
   "source": [
    "Như vậy ta có thể thấy, tại mỗi lượt huấn luyện model lấy ra một `batch` có kích thước là 32. \n",
    "Mặc dù ảnh của chúng ta có kích thước khác nhau nhưng đã được resize về chung một kích thước là `width` x `height` = 224 x 224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ccefbc8c-cb5a-4a6e-8868-4aeb947b9fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 50176)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 50177     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,279,041\n",
      "Trainable params: 3,257,153\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "43/43 [==============================] - 11s 167ms/step - loss: 0.8531 - accuracy: 0.9041\n",
      "Epoch 2/5\n",
      "43/43 [==============================] - 7s 164ms/step - loss: 0.3894 - accuracy: 0.9448\n",
      "Epoch 3/5\n",
      "43/43 [==============================] - 7s 165ms/step - loss: 0.1809 - accuracy: 0.9688\n",
      "Epoch 4/5\n",
      "43/43 [==============================] - 7s 165ms/step - loss: 0.1497 - accuracy: 0.9709\n",
      "Epoch 5/5\n",
      "43/43 [==============================] - 7s 163ms/step - loss: 0.1265 - accuracy: 0.9724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x306e78f10>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "base_extractor = MobileNet(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')\n",
    "flat = Flatten()\n",
    "den = Dense(1, activation='sigmoid')\n",
    "model = Sequential([base_extractor, flat, den])\n",
    "model.summary()\n",
    "\n",
    "# chúng ta chỉ cần thay generator vào vị trí của train data trong hàm fit()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(image_generator, epochs = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
