# Airflow Overview

## Overview

Apache Airflow is a powerful workflow orchestration tool that helps you schedule, monitor, and manage data pipelines by **right way, right order and right time**. Since youâ€™re working with feature engineering and GCP, Airflow can be a great addition to automate your ML pipelines.

NguyÃªn táº¯c cá»§a Airflow:

- TÃ­nh nÄƒng Ä‘á»™ng ( Dynamic ) : Airflow pipeline Ä‘Æ°á»£c config báº±ng code Python, cho phÃ©p báº¡n thay Ä‘á»•i code dá»… dÃ ng Ä‘á»ƒ tÃ¹y biáº¿n luá»“ng lÃ m viá»‡c cá»§a báº¡n.
- TÃ­nh tÄƒng trÆ°á»Ÿng ( Scalable ) : VÃ­ dá»¥ Ä‘Æ¡n giáº£n lÃ  báº¡n cÃ³ thá»ƒ má»Ÿ rá»™ng cÃ¡c task vá» xá»­ lÃ½ dá»¯ liá»‡u Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian
- TÃ­nh gá»n gÃ ng ( Elegant ) : code gá»n gÃ ng, ngÄƒn náº¯p, rÃµ rÃ ng giÃºp báº¡n Ä‘á»c hiá»ƒu code nhanh chÃ³ng.
- TÃ­nh má»Ÿ rá»™ng ( Extensible ) : Báº¡n cÃ³ thá»ƒ thÃªm tháº¯t thÆ° viá»‡n, modules, packages, ... phÃ¹ há»£p vá»›i mÃ´i trÆ°á»ng cá»§a báº¡n

P/S : Airflow khÃ´ng pháº£i má»™t giáº£i phÃ¡p vá» stream dá»¯ liá»‡u nhÆ° Spark Streaming, Apache Storm

**1. Core components:**
- *DAGs (Directed Acyclic Graphs)* â€“ Define workflows. (Ä‘á»c bá»Ÿi scheduler vÃ  executor ( vá»›i má»i worker mÃ  executor cÃ³ ))
- *Tasks & Operators* â€“ Atomic units of execution: nhá»¯ng functions sá»­ dá»¥ng trong Task or DAGs
	- Action Operators: lÃ  function thá»±c hiá»‡n 1 nhiá»‡m vá»¥ execute xá»­ lÃ½ nÃ o Ä‘Ã³
	- Transfer Operator: Transfer data from source to destination
	- Sensor Operator: wait to somethings happen
- *Schedulers* â€“ Manage task execution (cháº¡y workflow , gá»­i cÃ¡c tasks tá»›i executor)
- *Executors* â€“ Define how tasks run (Local, Celery, Kubernetes, etc.) (quáº£n lÃ½ cÃ¡c workers, xá»­ lÃ½ cÃ¡c tÃ¡c vá»¥ Ä‘ang cháº¡y): difined how to run your task should be executed
- *Worker*: process/sub-processs executing your task
- *Metadata Database* â€“ Stores execution history. (nÆ¡i lÆ°u tráº¡ng thÃ¡i cá»§a scheduler, executor, webserver)
- *Web UI* â€“ Monitor DAGs visually. (giao diá»‡n web cho phÃ©p kiá»ƒm tra, kÃ­ch hoáº¡t, sá»­a lá»—i cÃ¡c tasks vÃ  DAGs)

![](https://images.viblo.asia/cf6457f8-82c2-4d05-9ac5-f33fadbe5d15.png)

**2. Advanced Features**
- *Sensors* (waiting for events like files in GCS).
- *Hooks* (connectors to external services like BigQuery, Pub/Sub).
- *XComs* (passing data between tasks).
- *Branching & Conditional Execution* (BranchPythonOperator).
- *TaskRetries & SLAs* (handling failures).

**3. Writing DAGs**

- How to define DAGs in Python.
- Using **Operators** (BashOperator, PythonOperator, DummyOperator).
- Task dependencies (set_upstream, set_downstream).
- Using Jinja templating for dynamic DAGs.

**4. Airflow with GCP**

- Using **Google Cloud Operators**:
	- **BigQueryOperator** â€“ Querying BigQuery.
	- **DataflowTemplateOperator** â€“ Running Dataflow jobs.
	- **PubSubPublishMessageOperator** â€“ Publishing messages.
	- **KubernetesPodOperator** â€“ Running tasks in GKE.
- Integrating Airflow with **Vertex AI Pipelines**.

**5. Airflow in Production**

- Airflow deployment strategies (Kubernetes, Cloud Composer).
- Monitoring & logging best practices.
- Handling failures & retries.
- Airflow security & authentication.

**6. Real-World Use Cases**

- Automating ML feature engineering pipelines with Airflow.
- ETL/ELT pipelines with Airflow & BigQuery.
- Orchestrating ML training & inference workflows.



## [Airflow Components](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/overview.html)


![](https://airflow.apache.org/docs/apache-airflow/stable/_images/edge_label_example.png)

### DAG

**Directed Acyclic Graph**Â lÃ Â **má»™t Ä‘á»“ thá»‹ cÃ³ hÆ°á»›ng khÃ´ng chu trÃ¬nh**, mÃ´ táº£ táº¥t cáº£ cÃ¡c bÆ°á»›c xá»­ lÃ½ dá»¯ liá»‡u trong má»™t quy trÃ¬nh, tá»« Ä‘Ã³ xÃ¡c Ä‘á»‹nh quy trÃ¬nh thá»±c hiá»‡n task cÃ´ng viá»‡c

- Má»—i DAG Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trong 1 file DAG, nÃ³ Ä‘á»‹nh nghÄ©a má»™t quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u, Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t Ä‘á»“ thá»‹ cÃ³ hÆ°á»›ng khÃ´ng chu trÃ¬nh, trong Ä‘Ã³Â **cÃ¡c nÃºt lÃ  cÃ¡c tÃ¡c vá»¥ (tasks) vÃ  cÃ¡c cáº¡nh lÃ  cÃ¡c phá»¥ thuá»™c giá»¯a cÃ¡c tÃ¡c vá»¥**.
- CÃ¡c tÃ¡c vá»¥ trong DAG thÆ°á»ng Ä‘Æ°á»£cÂ **xá»­ lÃ½ tuáº§n tá»± hoáº·c song song**
- Khi má»™t DAG Ä‘Æ°á»£c thá»±c thi, nÃ³ Ä‘Æ°á»£c gá»i lÃ  má»™t láº§n cháº¡y DAG

#### DAG Status

![](https://images.viblo.asia/f0b289e3-60f9-4725-9962-dfb27a37052e.png)

VÃ²ng Ä‘á»i cá»§a 1 tráº¡ng thÃ¡i nhiá»‡m vá»¥ gá»“m cÃ³ cÃ¡c tráº¡ng thÃ¡i sau
    - **No status**: tÃ¡c vá»¥ chÆ°a Ä‘Æ°á»£c xáº¿p hÃ ng Ä‘á»ƒ thá»±c hiá»‡n
    - **Scheduled**: Bá»™ láº­p lá»‹ch Ä‘Ã£ xÃ¡c Ä‘á»‹nh ráº±ng cÃ¡c phá»¥ thuá»™c cá»§a nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘Ã¡p á»©ng vÃ  Ä‘Ã£ lÃªn lá»‹ch cho nÃ³ cháº¡y
    - **Removed**: VÃ¬ má»™t lÃ½ do nÃ o Ä‘Ã³, tÃ¡c vá»¥ Ä‘Ã£ biáº¿t máº¥t khá»i DAG ká»ƒ tá»« khi báº¯t Ä‘áº§u cháº¡y
    - **Upstream failed**: tÃ¡c vá»¥ ngÆ°á»£c dÃ²ng khÃ´ng thÃ nh cÃ´ng
    - **Queued**: Nhiá»‡m vá»¥ Ä‘Ã£ Ä‘Æ°á»£c giao cho Executor vÃ  Ä‘ang Ä‘á»£i 1 worker cÃ³ sáºµn Ä‘á»ƒ thá»±c thi
    - **Running**: TÃ¡c vá»¥ Ä‘ang Ä‘Æ°á»£c cháº¡y bá»Ÿi má»™t worker
    - **Sucess**: TÃ¡c vá»¥ cháº¡y xong khÃ´ng cÃ³ lá»—i
    - **Failed**: TÃ¡c vá»¥ cÃ³ lá»—i trong khi thá»±c thi vÃ  khÃ´ng cháº¡y Ä‘Æ°á»£c
    - **Up for retry**: TÃ¡c vá»¥ khÃ´ng thÃ nh cÃ´ng nhÆ°ng váº«n cÃ²n cÃ¡c láº§n thá»­ láº¡i vÃ  sáº½ Ä‘Æ°á»£c lÃªn lá»‹ch láº¡i

Váº­y lÃ½ tÆ°á»Ÿng nháº¥t cho má»™t tÃ¡c vá»¥ lÃ  gÃ¬? ÄÃ³ lÃ  Ä‘i tá»«Â **No status --> Scheduled --> Queued --> Running --> Sucess**

#### Khai bÃ¡o DAG

##### Context manager

Vá»›i cÃ¡ch nÃ y báº¡n sáº½ bá»c code cá»§a mÃ¬nh trong DAG báº±ngÂ `with`

```python
from airflow import DAG

with DAG(
    "my_dag_name", start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule_interval="@daily", catchup=False
) as dag:
    op = EmptyOperator(task_id="task")

```

##### Standard constructor

Vá»›i cÃ¡ch nÃ y, báº¡n sáº½ khai bÃ¡o má»™t constructer gá»i tá»›i class DAG

```python
from airflow import DAG

my_dag = DAG("my_dag_name", start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
             schedule_interval="@daily", catchup=False)
op = EmptyOperator(task_id="task", dag=my_dag)
```

##### Decorator

```python
from airflow.decorators import dag

@dag(start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
     schedule_interval="@daily", catchup=False)
def generate_dag():
    op = EmptyOperator(task_id="task")

dag = generate_dag()
```

#### VIáº¿t DAG

**DAG thá»±c hiá»‡n ETL sá»­ dá»¥ng decorator + context manager**
```python
import json
import pendulum
from airflow.decorators import dag, task
from airflow.operators.email_operator import EmailOperator

@dag(
    schedule_interval=None, # lá»‹ch trÃ¬nh cháº¡y @once,Â @hourly, @daily...
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"), # thá»i gian báº¯t Ä‘áº§u
    catchup=False, # náº¿u báº¡n cÃ³Â `start_date`,Â `end_date`Â ( optional ) vÃ Â `schedule_interval`Â thÃ¬ khi báº¡n Ä‘á»ƒ giÃ¡ trá»‹ cá»§a catchup lÃ  True thÃ¬Â `schedule_interval`Â sáº½ khÃ´ng bá»‹ giá»›i háº¡n vÃ  thá»±c thi tasks tá»©c thÃ¬.
    tags=['example'],
)
def tutorial_taskflow_api_etl():
    """
    ### TaskFlow API Tutorial Documentation
    This is a simple ETL data pipeline example which demonstrates the use of
    the TaskFlow API using three simple tasks for Extract, Transform, and Load.
    Documentation that goes along with the Airflow TaskFlow API tutorial is
    located
    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)
    """
    @task()
    def extract():
        """
        #### Extract task
        A simple Extract task to get data ready for the rest of the data
        pipeline. In this case, getting data is simulated by reading from a
        hardcoded JSON string.
        """
        data_string = '{"1001": 301.27, "1002": 433.21, "1003": 502.22}'

        order_data_dict = json.loads(data_string)
        return order_data_dict
    @task(multiple_outputs=True)
    def transform(order_data_dict: dict):
        """
        #### Transform task
        A simple Transform task which takes in the collection of order data and
        computes the total order value.
        """
        total_order_value = 0

        for value in order_data_dict.values():
            total_order_value += value

        return {"total_order_value": total_order_value}
    @task()
    def load(total_order_value: float):
        """
        #### Load task
        A simple Load task which takes in the result of the Transform task and
        instead of saving it to end user review, just prints it out.
        """

        print(f"Total order value is: {total_order_value:.2f}")

	email_notification = EmailOperator( 
		task_id='email_notification', 
		to='noreply@xxx.com', 
		subject='dag completed',
		html_content='the dag has finished')

    order_data = extract()
    order_summary = transform(order_data)
    load(order_summary["total_order_value"]) >> email_notification

tutorial_etl_dag = tutorial_taskflow_api_etl()

```

**Import thÆ° viá»‡n bá»• sung**

Trong trÆ°á»ng há»£p task cá»§a báº¡n cháº¡y cáº§n má»™t thÆ° viá»‡n chá»‰ Ä‘á»‹nh. ChÃºng ta cÃ³ thá»ƒ dÃ¹ng cÃ¡ch viáº¿t DecoratorÂ `@task.virtualenv`Â hoáº·c cÃ¡ch viáº¿t Context ManagerÂ `PythonVirtualenvOperator`. Cá»¥ thá»ƒ nhÆ° sau:

- Decorator
```python
@task.virtualenv(
        task_id="virtualenv_python", requirements=["numpy"], system_site_packages=False
    )
def mul_number(numbs: list):
        import numpy
        return int(numpy.prod(numpy.array(numbs)))
```

- Context Manager
```python
def mul_number(numbs: list):
        import numpy
        return int(numpy.prod(numpy.array(numbs)))

virtualenv_task = PythonVirtualenvOperator(
    task_id="virtualenv_python",
    python_callable=mul_number(numbs),
    requirements=["numpy"],
    system_site_packages=False,
)
```

P/s: Náº¿u báº¡n cÃ³ nhiá»u thÆ° viá»‡n cáº§n cÃ i thÃ¬ cÃ³ thá»ƒ thÃªm Ä‘Æ°á»ng dáº«n tá»›i file requirements.txt:Â `requirements="path\to\requirements.txt"`

#### Optimize DAG Flow

##### Import thÆ° viá»‡n trong hÃ m

Äá»ƒ tá»‘i Æ°u hiá»‡u suáº¥t cá»§a Airflow, **ta tá»‘t nháº¥t lÃ  chá»‰ viáº¿t mÃ£ cáº§n thiáº¿t Ä‘á»ƒ táº¡o cÃ¡c operator**, **Ä‘á»‹nh nghÄ©a cÃ¡c hÃ m callable** vÃ  **Ä‘á»‹nh nghÄ©a quan há»‡ giá»¯a chÃºng**. Äiá»u nÃ y lÃ  do Airflow sáº½ import toÃ n bá»™ file Ä‘á»‹nh nghÄ©a DAG trong lÃºc Ä‘á»c ná»™i dung nháº±m Ä‘á»ƒ cáº­p nháº­t thÃ´ng tin cÃ¡c DAG, tá»©c lÃ  nhá»¯ng Ä‘oáº¡n code **KHÃ”NG THUá»˜C 3 THá»¨ á» TRÃŠN** sáº½ Ä‘Æ°á»£c thá»±c thi ngay láº­p tá»©c vÃ  Ä‘iá»u Ä‘Ã³ quÃ¡ trÃ¬nh xá»­ lÃ½ sáº½ khÃ´ng Ä‘Ãºng kÃ¬ vá»ng vÃ  táº¥t nhiÃªn quÃ¡ trÃ¬nh phÃ¢n tÃ­ch cÃº phÃ¡p sáº½ cháº­m Ä‘i Ä‘Ã¡ng ká»ƒ

Khi sá»­ dá»¥ng numpy, ta nÃªn: 
```python
import pendulum
from airflow import DAG
from airflow.decorators import task

with DAG(
    dag_id="example_python_operator",
    schedule=None,
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    catchup=False,
    tags=["example"],
) as dag:

    @task()
    def print_array():
        """Print Numpy array."""
        
        import numpy as np  # <- THIS IS HOW NUMPY SHOULD BE IMPORTED IN THIS CASE!a = np.arange(15).reshape(3, 5)
        print(a)
        return a

    print_array()

```

##### KhÃ´ng nÃªn Ä‘áº·t **Airflow Variable** bÃªn ngoÃ i *operator* vÃ  *python_callable*

Do cÃ¡c Ä‘oáº¡n mÃ£ nÃ y Ä‘Æ°á»£c thá»±c thi khi láº­p lá»‹ch cÃ¡c task, nÃªn phiÃªn báº£n cá»§a cÃ¡c Variable Ä‘Æ°á»£c sá»­ dá»¥ng sáº½ lÃ  phiÃªn báº£n táº¡i thá»i Ä‘iá»ƒm task Ä‘Æ°á»£c láº­p lá»‹ch chá»© khÃ´ng pháº£i lÃºc task Ä‘Æ°á»£c thá»±c thi. Viá»‡c Ä‘áº·t chÃºng á»Ÿ ngoÃ i nhÆ° váº­y cÅ©ng khiáº¿n cho hiá»‡u nÄƒng giáº£m sÃºt vÃ  quÃ¡ trÃ¬nh parse cÃ¡c file Ä‘á»‹nh nghÄ©a dag cÃ³ thá»ƒ bá»‹ timeout

```python
from airflow.models import Variable

# BAD PRACTICE

foo_var = Variable.get("foo")  # DON'T DO THAT
bash_use_variable_bad_1 = BashOperator(
    task_id="bash_use_variable_bad_1", bash_command="echo variable foo=${foo_env}", env={"foo_env": foo_var}
)

bash_use_variable_bad_2 = BashOperator(
    task_id="bash_use_variable_bad_2",
    bash_command=f"echo variable foo=${Variable.get('foo')}",  # DON'T DO THAT
)

bash_use_variable_bad_3 = BashOperator(
    task_id="bash_use_variable_bad_3",
    bash_command="echo variable foo=${foo_env}",
    env={"foo_env": Variable.get("foo")},  # DON'T DO THAT
)

# GOOD PRACTICE

bash_use_variable_good = BashOperator(
    task_id="bash_use_variable_good",
    bash_command="echo variable foo=$**{foo_env}**",
    env={"foo_env": "{{ var.value.get('foo') }}"},
)

@task
def my_task():
    var = Variable.get("foo")  # this is fine, because func my_task called only run task, not scan DAGs.
    print(var)

```

##### Giáº£m Ä‘á»™ phá»©c táº¡p cá»§a DAG

Náº¿u báº¡n muá»‘n tá»‘i Æ°u hÃ³a DAG cá»§a mÃ¬nh, Ä‘Ã¢y lÃ  má»™t sá»‘ Ä‘iá»u báº¡n cÃ³ thá»ƒ lÃ m:
- **LÃ m cho DAG cá»§a báº¡n táº£i nhanh hÆ¡n**: ThÃ´ng thÆ°á»ng viá»‡c kiá»ƒm tra quÃ¡ trÃ¬nh load má»™t dag cÃ³ thá»ƒ thá»±c hiá»‡n qua cÃ¢u lá»‡nh cháº³ng háº¡n nhÆ° sauÂ `time python airflow/example_dag.py`Â káº¿t quáº£ sáº½ cho thÃ´ng tin vá» viá»‡c Airflow cáº§n bao lÃ¢u Ä‘á»ƒ Ä‘á»c Ä‘Æ°á»£c ná»™i dung DAG Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t. ÄÃ´i khi mÃ£ tá»‘i Æ°u vÃ  khÃ´ng tá»‘i Æ°u chá»‰ chÃªnh nhau vÃ i trÄƒm mili giÃ¢y, tuy nhiÃªn náº¿u sá»‘ lÆ°á»£ng DAG tÄƒng lÃªn theo thá»i gian thÃ¬ Ä‘Ã¢y láº¡i lÃ  váº¥n Ä‘á» lá»›n Ä‘áº¥y.
- **LÃ m cho DAG cá»§a báº¡n táº¡o ra má»™t cáº¥u trÃºc Ä‘Æ¡n giáº£n hÆ¡n**: Ráº¥t rÃµ rÃ ng ta cÃ³ thá»ƒ tháº¥y ráº±ng DAG cÃ³ cáº¥u trÃºc tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£nÂ `A-> B-> C`Â sáº½ máº¥t Ã­t thá»i gian hÆ¡n trong quÃ¡ trÃ¬nh láº­p lá»‹ch cÅ©ng nhÆ° thá»±c thi. Äiá»u Ä‘Ã³ láº¡i hoÃ n toÃ n ngÆ°á»£c láº¡i vá»›i má»™t DAG cÃ³ cáº¥u trÃºc cÃ¢y sÃ¢u vá»›i sá»‘ lÆ°á»£ng cÃ´ng viá»‡c phá»¥ thuá»™c tÄƒng theo cáº¥p sá»‘ nhÃ¢n.
- **CÃ³ sá»‘ lÆ°á»£ng DAG nhá» hÆ¡n trÃªn má»—i file cÃ i Ä‘áº·t**: Trong khi Airflow 2 Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho trÆ°á»ng há»£p cÃ³ nhiá»u DAG trong má»™t tá»‡p, cÃ³ má»™t sá»‘ pháº§n cá»§a há»‡ thá»‘ng lÃ m cho nÃ³ Ã­t hiá»‡u quáº£ hoáº·c gÃ¢y ra nhiá»u sá»± cháº­m trá»… hÆ¡n so vá»›i viá»‡c cÃ³ nhá»¯ng **DAG Ä‘Ã³ chia thÃ nh nhiá»u tá»‡p**. Váº­y nÃªn, náº¿u báº¡n cÃ³ nhiá»u DAG Ä‘Æ°á»£c táº¡o ra tá»« má»™t tá»‡p, hÃ£y xem xÃ©t chia chÃºng náº¿u báº¡n quan sÃ¡t tháº¥y nÃ³ máº¥t nhiá»u thá»i gian Ä‘á»ƒ pháº£n Ã¡nh cÃ¡c thay Ä‘á»•i trong tá»‡p DAG cá»§a báº¡n trong giao diá»‡n ngÆ°á»i dÃ¹ng cá»§a Airflow.
- **Don't repeat yourself**: ThÃ´ng thÆ°á»ng ta sáº½ cÃ³ má»™t hoáº·c má»™t vÃ i pháº§n mÃ£ Ä‘Æ°á»£c sá»­ dá»¥ng chung trong toÃ n bá»™ cÃ¡c DAG. Táº¥t nhiÃªn lÃ  náº¿u sá»‘ láº§n láº·p láº¡i Ã­t thÃ¬ cÅ©ng cháº³ng lÃ m sao cáº£, tuy nhiÃªn tá»‘t hÆ¡n háº¿t ta nÃªn tÃ¡ch chÃºng thÃ nh cÃ¡c Operator riÃªng biá»‡t. Äiá»u nÃ y sáº½ giÃºp ta (1) quáº£n lÃ½ source code dá»… hÆ¡n, (2) kiá»ƒm thá»­ tá»‘t hÆ¡n, (3) trÃ¡nh láº·p code vÃ  háº¡n cháº¿ cÃ¡c váº¥n Ä‘á» phÃ¡t sinh trong quÃ¡ trÃ¬nh phÃ¡t triá»ƒn
#### Action to DAG

##### Mount thÆ° cÃ¡c thÆ° má»¥c cáº§n thiáº¿t vÃ o container

Trong docker-compose.yaml, cÃ¡c cÃ¢u lá»‡nh sau dÃ¹ng Ä‘á»ƒ amount cÃ¡c thÆ° má»¥c trong airflow folder vÃ o container

```yaml

Â  volumes: # mount the whole project directory to the container

Â  Â  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
Â  Â  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
Â  Â  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
Â  Â  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins

```

##### Check DAG Syntax (Before Running)

```sh
docker-compose exec airflow-webserver airflow dags list
```
- If there are any issues, fix them before proceeding.

For detail if there are any error when import DAG to Airflow:
```sh
docker-compose exec airflow-webserver airflow dags list-import-errors
```

##### Test DAG Locally

To verify that your DAG is correct

```sh
docker-compose exec airflow-webserver airflow dags test my_new_dag 2024-02-17
```
- This runs the DAG for a **specific execution date**.
- It does **not** depend on the scheduler.

##### Debug Specific Task Execution
If a specific task is failing, you can debug it by running:
```sh
docker-compose exec airflow-webserver airflow tasks test my_new_dag my_task_id 2024-02-17
```
- This runs only **one task** in the DAG at a specific execution date.

##### Refresh DAG in Airflow UI
If you modify the DAG and need to refresh it:
```sh
# Option 1: Restart Webserver & Scheduler
docker-compose restart airflow-webserver airflow-scheduler

# OR Clear DAG and Reload
docker-compose exec airflow-webserver airflow dags reserialize
```
##### View Logs for Debugging
If a DAG fails or doesnâ€™t appear in the UI, check logs:
```sh
docker-compose logs airflow-webserver
docker-compose logs airflow-scheduler

# To see logs for a specific task execution:
docker-compose exec airflow-webserver airflow tasks logs my_new_dag my_task_id 2024-02-17
```
##### Delete & Restart DAG (If Necessary)

```sh
docker-compose exec airflow-webserver airflow dags delete my_new_dag
```
Or, clear all task instances:
```sh
docker-compose exec airflow-webserver airflow tasks clear my_new_dag --dag-run-state=queued
```

##### Stop & Restart Airflow (If Needed)
If changes are not reflecting:
```sh
docker-compose down 
docker-compose up -d
```

To completely reset Airflow (including database):
```sh
docker-compose down --volumes 
docker-compose up -d
```


**Refresh láº¡i WebUI Ä‘á»ƒ cáº­p nháº­t tráº¡ng thÃ¡i má»›i nháº¥t**.
Náº¿u báº¡n kiá»ƒm tra airflow database sáº½ tháº¥y thÃ´ng tin file DAG Ä‘Ã£ Ä‘Æ°á»£c lÆ°u láº¡i. DÃ¹ng airflow shell script Ä‘á»ƒ kiá»ƒm tra

```python
./airflow.sh dags list
```

![](https://images.viblo.asia/7e6dc372-a0cb-4c89-a583-3bdf351413d3.png)

![](https://images.viblo.asia/25a8309b-1d15-44b9-b368-9b084021ecb9.png)


**Check DAG trÃªn WebUI**

Ban Ä‘áº§u DAGÂ `test_operator`Â trong tráº¡ng thÃ¡i táº¡m dá»«ng, chÃºng ta cáº§n kÃ­ch hoáº¡t nÃ³, sau Ä‘Ã³ trigger DAG náº±m trong má»¥c Actions á»Ÿ áº£nh trÃªn. Má»¥c Runs sáº½ thá»‘ng kÃª tráº¡ng thÃ¡i cá»§a cÃ¡c láº§n cháº¡y DAG, nhÆ° á»Ÿ bÃªn trÃªn thÃ¬ tÃ´i cÃ³ 1 láº§n thÃ nh cÃ´ng vÃ  8 láº§n tháº¥t báº¡i ( do cháº¡y thá»­ Ä‘á»ƒ sá»­a code ). NgoÃ i ra báº¡n cÃ³ thá»ƒ click vÃ o tÃªn file DAG Ä‘á»ƒ xem chi tiáº¿t ( kiáº¿n trÃºc, ngÃ y giá» cháº¡y, thá»i gian cháº¡y cá»§a má»—i task, ... )

**Chá»‰nh sá»­a DAG**: Cá»© thá»±c hiá»‡n chá»‰nh sá»­a trong thÆ° má»¥c Ä‘Ã£ amount (ko cáº§n pháº£i build láº¡i container), dags_script sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c update cho láº§n cháº¡y tiáº¿p theo

**XoÃ¡ DAG**

P/s: Náº¿u cÃ¡c báº¡n muá»‘n xÃ³a file DAG trÃªn web thÃ¬ lÃ m theo cÃ¡c bÆ°á»›c sau:

3. XÃ³a file DAG trong thÆ° má»¥cÂ `dags/`
4. DÃ¹ng cÃ¢u lá»‡nhÂ `./airflow.sh dags delete DAG_ID`Â - DAG_ID á»Ÿ Ä‘Ã¢y lÃ  tÃªn file DAG, vÃ­ dá»¥ "test_operator" Ä‘á»ƒ xÃ³a báº£n ghi trong cÆ¡ sá»Ÿ dá»¯ liá»‡u hoáº·c dÃ¹ng cÃ¢u lá»‡nhÂ `/airflow.sh db reset`
5. TrÃªn Web UI, áº¥n vÃ o icon thÃ¹ng rÃ¡c @@ trong má»¥c Actions

#### Test/Debug DAG

Test kiá»ƒm tra xem DAG cÃ³ load Ä‘Æ°á»£c lÃªn airflow hay khÃ´ng ?
- B1: cháº¡y `python Ä‘Æ°á»ng_dáº«n_tá»›i_tá»‡p_DAG_cá»§a_báº¡n.py`: Ä‘áº£m báº£o ráº±ng DAG cá»§a báº¡n khÃ´ng chá»©a báº¥t ká»³ phá»¥ thuá»™c chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t, lá»—i cÃº phÃ¡p
- B2: cháº¡y `time python airflow/example_dag.py` (Linux) Ä‘á»ƒ tÃ­nh sá»‘ thá»i gian Ä‘á»ƒ load run DAG, Â so sÃ¡nh káº¿t quáº£ trÆ°á»›c vÃ  sau tá»‘i Æ°u hÃ³a
- B3: Viáº¿t pytest:

**Check: táº¥t cáº£ cÃ¡c DAG Ä‘Æ°á»£c cÃ i Ä‘áº·t cÃ³ Ä‘Æ°á»£c load thÃ nh cÃ´ng hay khÃ´ng**
```python
def get_import_errors():
    dag_bag = DagBag(include_examples=False)
    def strip_path_prefix(path):
        return os.path.relpath(path, os.environ.get("AIRFLOW_HOME"))
    return [(None, None)] + [
        (strip_path_prefix(k), v.strip()) for k, v in dag_bag.import_errors.items()
    ]

@pytest.mark.parametrize(
    "rel_path,rv", get_import_errors(), ids=[x[0] for x in get_import_errors()]
)
def test_file_imports(rel_path, rv):
    if rel_path and rv:
        raise Exception(f"{rel_path} failed to import with message \n {rv}")

```

**Check xem DAG cÃ³ Ä‘Æ°á»£c khá»Ÿi táº¡o Ä‘Ãºng cÃ¡ch hay khÃ´ng**
```python
import pytest
from airflow.models import DagBag


@pytest.fixture()
def dagbag():
    return DagBag()


def test_dag_loaded(dagbag):
    dag = dagbag.get_dag(dag_id="hello_world")
    assert dagbag.import_errors == {}
    assert dag is not None 
		assert len(dag.tasks) == 1


```

**Check xem cáº¥u trÃºc DAG cÃ³ giá»‘ng nhÆ° kÃ¬ vá»ng hay khÃ´ng**
```python
def assert_dag_dict_equal(source, dag):
    assert dag.task_dict.keys() == source.keys()
    for task_id, downstream_list in source.items():
        assert dag.has_task(task_id)
        task = dag.get_task(task_id)
        assert task.downstream_task_ids == set(downstream_list)


def test_dag():
    assert_dag_dict_equal(
        {
            "DummyInstruction_0": ["DummyInstruction_1"],
            "DummyInstruction_1": ["DummyInstruction_2"],
            "DummyInstruction_2": ["DummyInstruction_3"],
            "DummyInstruction_3": [],
        },
        dag,
    )
```

**Check xem Custom Operator**
```python
import datetimeimport pendulumimport pytestfrom airflow import DAG
from airflow.utils.state import DagRunState, TaskInstanceState
from airflow.utils.types import DagRunType

DATA_INTERVAL_START = pendulum.datetime(2021, 9, 13, tz="UTC")
DATA_INTERVAL_END = DATA_INTERVAL_START + datetime.timedelta(days=1)

TEST_DAG_ID = "my_custom_operator_dag"
TEST_TASK_ID = "my_custom_operator_task"


@pytest.fixture()
def dag():
    with DAG(
        dag_id=TEST_DAG_ID,
        schedule="@daily",
        start_date=DATA_INTERVAL_START,
    ) as dag:
        MyCustomOperator(
            task_id=TEST_TASK_ID,
            prefix="s3://bucket/some/prefix",
        )
    return dag


def test_my_custom_operator_execute_no_trigger(dag):
    dagrun = dag.create_dagrun(
        state=DagRunState.RUNNING,
        execution_date=DATA_INTERVAL_START,
        data_interval=(DATA_INTERVAL_START, DATA_INTERVAL_END),
        start_date=DATA_INTERVAL_END,
        run_type=DagRunType.MANUAL,
    )
    ti = dagrun.get_task_instance(task_id=TEST_TASK_ID)
    ti.task = dag.get_task(task_id=TEST_TASK_ID)
    ti.run(ignore_ti_state=True)
    assert ti.state == TaskInstanceState.SUCCESS

```

NÃ³i chung lÃ  viá»‡c kiá»ƒm thá»­ vá»›i Airflow khÃ¡ lÃ  máº¥t thá»i gian, nháº¥t lÃ  khi cÃ¡c Operator thÆ°á»ng tÆ°Æ¡ng tÃ¡c vá»›i cÃ¡c dá»‹ch vá»¥ bÃªn ngoÃ i, náº¿u khÃ´ng pháº£i cÃ¡c loáº¡i database, S3 cÃ¡c thá»© thÃ¬ cÅ©ng lÃ  Slack Ä‘á»ƒ gá»­i report cháº³ng háº¡n. Trong nhá»¯ng lÃºc nÃ y cÃ¡c báº¡n sáº½ cáº§n nhiá»u thá»© hÆ¡n vÃ­ dá»¥ nhÆ°Â [mock](https://docs.python.org/dev/library/unittest.mock.html)Â cÃ¡c function Ä‘á»ƒ giáº£ láº­p cÃ¡c chá»©c nÄƒng cháº³ng háº¡n, dá»… dÃ ng test function hÆ¡n khi viáº¿t cÃ¡c hÃ m xá»­ lÃ½ trong cÃ¡c custom operator thay vÃ¬ lÃ  cÃ¡cÂ `python_callable`Â riÃªng láº».
#### Xá»­ lÃ½ Python Packages phá»©c táº¡p, xung Ä‘á»™t

##### Sá»­ dá»¥ngÂ `PythonVirtualenvOperator`

`PythonVirtualenvOperator`Â cho phÃ©p báº¡n táº¡o Ä‘á»™ng má»™tÂ `virtualenv`Â mÃ  chá»©c nÄƒng gá»i Python cá»§a báº¡n sáº½ thá»±c thi. Má»—i task cÃ³ thá»ƒ cÃ³ má»™tÂ `virtualenv`Â Python Ä‘á»™c láº­p (Ä‘Æ°á»£c táº¡o Ä‘á»™ng má»—i láº§n nhiá»‡m vá»¥ Ä‘Æ°á»£c cháº¡y) vÃ  cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh táº­p yÃªu cáº§u chi tiáº¿t cáº§n pháº£i Ä‘Æ°á»£c cÃ i Ä‘áº·t cho nhiá»‡m vá»¥ Ä‘Ã³ Ä‘á»ƒ thá»±c thi.

**Operator sáº½ chá»‹u trÃ¡ch nhiá»‡m:**

- Táº¡o virtualenv dá»±a trÃªn mÃ´i trÆ°á»ng cá»§a báº¡n
- Serializing hÃ m xá»­ lÃ½ vÃ  chuyá»ƒn nÃ³ cho trÃ¬nh thÃ´ng dá»‹ch Python cá»§aÂ `virtualenv`Â Ä‘á»ƒ thá»±c thi
- Thá»±c thi vÃ  láº¥y káº¿t quáº£ cá»§a chá»©c nÄƒng gá»i vÃ  Ä‘áº©y nÃ³ qua Xcom náº¿u Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh

**CÃ¡c lá»£i Ã­ch cá»§a operator nÃ y lÃ :**

- KhÃ´ng cáº§n pháº£i chuáº©n bá»‹ venv trÆ°á»›c. NÃ³ sáº½ Ä‘Æ°á»£c táº¡o Ä‘á»™ng trÆ°á»›c khi nhiá»‡m vá»¥ Ä‘Æ°á»£c cháº¡y vÃ  bá»‹ xÃ³a sau khi hoÃ n táº¥t.
- Báº¡n cÃ³ thá»ƒ cháº¡y nhiá»u task vá»›i cÃ¡c nhiá»u bá»™Â `dependencies`Â khÃ¡c nhau trÃªn cÃ¹ng má»™t worker, do Ä‘Ã³ tÃ i nguyÃªn bá»™ nhá»› Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng.

**Tuy nhiÃªn, cÃ³ nhá»¯ng giá»›i háº¡n vÃ  chi phÃ­ phÃ¡t sinh bá»Ÿi operator nÃ y:**

- Bá»Ÿi vÃ¬ hÃ m xá»­ lÃ½ sáº½ Ä‘Æ°á»£c serialize trÆ°á»›c khi Ä‘Æ°á»£c thá»±c thi nÃªn chÃºng cÃ³ thá»ƒ sáº½ cÃ³ má»™t sá»‘ thay Ä‘á»•i khÃ´ng Ä‘Ã¡ng cÃ³
- Operator thÃªm chi phÃ­ CPU, máº¡ng vÃ  thá»i gian xá»­ lÃ½ Ä‘á»ƒ cháº¡y má»—i task vÃ¬ chÃºng sáº½ luÃ´n táº¡o láº¡i venv cÅ©ng cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n cÃ³ má»—i khi báº¯t Ä‘áº§u vÃ  xÃ³a Ä‘i sau khi hoÃ n táº¥t quÃ¡ trÃ¬nh thá»±c thi
- CÃ¡c worker cáº§n truy cáº­p PyPI hoáº·c cÃ¡c kho lÆ°u trá»¯ riÃªng Ä‘á»ƒ cÃ i Ä‘áº·t cÃ¡cÂ `dependencies`.
- Viá»‡c táº¡o Ä‘á»™ngÂ `virtualenv`Â cÃ³ thá»ƒ dá»… bá»‹ lá»—i táº¡m thá»i.

##### Sá»­ dá»¥ng `DockerOperator` hoáº·c `Kubernetes Pod Operator`

`DockerOperator` vÃ  `KubernetesPodOperator` sáº½ yÃªu cáº§u Airflow cÃ³ quyá»n truy cáº­p vÃ o má»™t Docker Engine hoáº·c cá»¥m Kubernetes cháº¡y cÃ¡c task trong cÃ¡c container tÃ¹y chá»‰nh, vÃ  Ä‘iá»u Ä‘Ã³ sáº½ cÃ³ má»™t sá»‘ Ä‘áº·c Ä‘iá»ƒm nhÆ° sau:

**CÃ¡c lá»£i Ã­ch cá»§a viá»‡c sá»­ dá»¥ng cÃ¡c operator nÃ y lÃ :**

- Báº¡n cÃ³ thá»ƒ cháº¡y cÃ¡c task vá»›i cÃ¡c táº­pÂ `dependencies`Â khÃ¡c nhau cá»§a cáº£ Python vÃ  cáº£ cÃ¡cÂ `dependencies`Â cáº¥p há»‡ thá»‘ng hoáº·c tháº­m chÃ­ lÃ  cÃ¡c task Ä‘Æ°á»£c viáº¿t báº±ng ngÃ´n ngá»¯ hoÃ n toÃ n khÃ¡c nhau hoáº·c kiáº¿n trÃºc bá»™ xá»­ lÃ½ khÃ¡c nhau.
- MÃ´i trÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ cháº¡y cÃ¡c nhiá»‡m vá»¥ Ä‘Æ°á»£c táº­n dá»¥ng cÃ¡c tá»‘i Æ°u hÃ³a vÃ  sáº½ luÃ´n khÃ´ng thay Ä‘á»•i do báº£n cháº¥t cá»§a cÃ¡c container.
- HoÃ n toÃ n Ä‘áº£m báº£o Ä‘Æ°á»£c tÃ­nh cÃ´ láº­p giá»¯a cÃ¡c task cÅ©ng nhÆ° cÃ¡c láº§n cháº¡y khÃ¡c nhau

**Nhá»¯ng Ä‘iá»ƒm háº¡n cháº¿:**

- Báº¡n cáº§n hiá»ƒu rÃµ hÆ¡n vá» cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a Docker Containers hoáº·c Kubernetes.
- KhÃ³ test á»Ÿ dÆ°á»›i local, do báº¡n khÃ³ cÃ³ thá»ƒ dá»±ng má»™t mÃ´i trÆ°á»ng Kubernetes tÆ°Æ¡ng tá»± trÃªn production Ä‘á»ƒ sá»­ dá»¥ng
- Viá»‡c thá»±c thi trong cÃ¡c container thÆ°á»ng cÃ³ má»™t chi phÃ­ phÃ¡t sinh, chá»§ yáº¿u lÃ  liÃªn quan Ä‘áº¿n viá»‡c giao tiáº¿p vá»›i Docker Engine hoáº·c cá»¥m Kubernetes. VÃ  bÃªn cáº¡nh Ä‘Ã³ báº¡n cÅ©ng sáº½ cáº§n chuáº©n bá»‹ trÆ°á»›c má»™t image chá»©a toÃ n bá»™ mÃ´i trÆ°á»ng thá»±c thi cÅ©ng nhÆ° lÆ°u trá»¯ nÃ³ trÃªn má»™t registry Ä‘á»ƒ nÃ³ sáºµn sÃ ng Ä‘Æ°á»£c táº£i vá» khi cáº§n.
### Task

https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html

**Task** lÃ  má»™t Ä‘Æ¡n vá»‹ cÆ¡ báº£n Ä‘á»ƒ thá»±c hiá»‡n má»™t cÃ´ng viá»‡c nhá» (**Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi Developer**) trong quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u. Má»—i Task lÃ  má»™t bÆ°á»›c trong quy trÃ¬nh vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c láº­p lá»‹ch thá»±c hiá»‡n tÃ¹y theo cÃ¡c Ä‘iá»u kiá»‡n cá»¥ thá»ƒ.

Má»™t **Task** trong Airflow cÃ³ cÃ¡c thuá»™c tÃ­nh vÃ  phÆ°Æ¡ng thá»©c sau:
- **`task_id`**: Ä‘á»‹nh danh duy nháº¥t cá»§a task trong DAG.
- **`owner`**: ngÆ°á»i sá»Ÿ há»¯u task.
- **`depends_on_past`**: xÃ¡c Ä‘á»‹nh liá»‡u task hiá»‡n táº¡i cÃ³ phá»¥ thuá»™c vÃ o káº¿t quáº£ cá»§a task trÆ°á»›c Ä‘Ã³ hay khÃ´ng.
- **`retries`**: sá»‘ láº§n thá»­ láº¡i náº¿u task tháº¥t báº¡i.
- **`retry_delay`**: khoáº£ng thá»i gian giá»¯a cÃ¡c láº§n thá»­ láº¡i.
- **`start_date`**: thá»i Ä‘iá»ƒm báº¯t Ä‘áº§u thá»±c hiá»‡n task.
- **`end_date`**: thá»i Ä‘iá»ƒm káº¿t thÃºc thá»±c hiá»‡n task.
- **`execution_timeout`**: thá»i gian tá»‘i Ä‘a cho phÃ©p Ä‘á»ƒ thá»±c hiá»‡n task.
- **`on_failure_callback`**: hÃ m Ä‘Æ°á»£c gá»i khi task tháº¥t báº¡i.
- **`on_success_callback`**: hÃ m Ä‘Æ°á»£c gá»i khi task thÃ nh cÃ´ng.

CÃ³ 3 nhÃ³m cÆ¡ báº£n vá» task:
- *Operator*: predefined task templates that you can string together quickly to build most parts of your DAGs.
- *Sensor*: Â special subclass of Operators which are entirely about waiting for an external event to happen.
- *TaskFlow*: decoratedÂ `@task`, which is a custom Python function packaged up as a Task.

#### XÃ¡c Ä‘á»‹nh thá»© tá»± xá»­ lÃ½ cá»§a cÃ¡c tasks

- CÃ¡ch 1: dÃ¹ng toÃ¡n tá»­Â `<<`Â vÃ Â `>>`

```python
first_task >> [second_task, third_task]
third_task << fourth_task
```

- CÃ¡ch 2: dÃ¹ng hÃ mÂ `set_downstream`Â vÃ Â `set_upstream`

```python
first_task.set_downstream(second_task, third_task)
third_task.set_upstream(fourth_task)
```

TrÆ°á»ng há»£p cÃ¡c tasks giao nhau

```python
from airflow.models.baseoperator import cross_downstream

# Replaces
# [op1, op2] >> op3
# [op1, op2] >> op4
cross_downstream([op1, op2], [op3, op4])
```

TrÆ°á»ng há»£p cÃ¡c tasks táº¡o thÃ nh dÃ¢y chuyá»n

```python
from airflow.models.baseoperator import chain

#1
# op1 >> op2 >> op3 >> op4
chain(op1, op2, op3, op4)

#2
# op1 >> op2 >> op3 >> op4 >> op5 >> op6
chain(*[EmptyOperator(task_id='op' + i) for i in range(1, 6)])

#3
# op1 >> op2 >> op4 >> op6
# op1 >> op3 >> op5 >> op6
chain(op1, [op2, op3], [op4, op5], op6)
```
### Operator

https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html

![](https://images.viblo.asia/520a79e3-35d8-45ee-9e78-e23b380b0a55.png)

Má»—i operator Ä‘áº¡i diá»‡n cho má»™t cÃ´ng viá»‡c cá»¥ thá»ƒ (**Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c bá»Ÿi Airflow**) trong quy trÃ¬nh, vÃ­ dá»¥ nhÆ° Ä‘á»c dá»¯ liá»‡u tá»« má»™t nguá»“n dá»¯ liá»‡u, xá»­ lÃ½ dá»¯ liá»‡u, hoáº·c ghi dá»¯ liá»‡u vÃ o má»™t nguá»“n dá»¯ liá»‡u khÃ¡c.

CÃ¡c operator trong Airflow Ä‘Æ°á»£c phÃ¢n loáº¡i thÃ nh cÃ¡c loáº¡i chÃ­nh sau
- **BashOperator**: Cháº¡y cÃ¡c lá»‡nh Bash hoáº·c script Shell.
- **PythonOperator**: Thá»±c thi cÃ¡c hÃ m Python.
- **EmailOperator**: Gá»­i email thÃ´ng qua SMTP.
- **DummyOperator**: ÄÆ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o cÃ¡c káº¿t ná»‘i giá»¯a cÃ¡c task.
- **PythonVirtualenvOperator**: Thá»±c thi cÃ¡c hÃ m Python trong má»™t mÃ´i trÆ°á»ng áº£o.
- **MySqlOperator**: Thá»±c hiá»‡n cÃ¡c lá»‡nh SQL trÃªn cÆ¡ sá»Ÿ dá»¯ liá»‡u MySQL.
- **PostgresOperator**: Thá»±c hiá»‡n cÃ¡c lá»‡nh SQL trÃªn cÆ¡ sá»Ÿ dá»¯ liá»‡u PostgreSQL.
- **S3FileTransformOperator**: Thá»±c hiá»‡n cÃ¡c chá»©c nÄƒng xá»­ lÃ½ file trÃªn Amazon S3.
- **SparkSqlOperator**: Thá»±c hiá»‡n cÃ¡c truy váº¥n Spark SQL.
- **HdfsSensor**: Kiá»ƒm tra sá»± tá»“n táº¡i cá»§a má»™t tá»‡p trÃªn Hadoop Distributed File System (HDFS).

> VÃ­ dá»¥: Ä‘á»c dá»¯ liá»‡u tá»« má»™t tá»‡p CSV, xá»­ lÃ½ dá»¯ liá»‡u vÃ  lÆ°u káº¿t quáº£ vÃ o má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u PostgreSQL:
- **FileSensor**: Kiá»ƒm tra sá»± tá»“n táº¡i cá»§a tá»‡p CSV trÃªn há»‡ thá»‘ng tá»‡p.
- **BashOperator**: Sá»­ dá»¥ng lá»‡nh Bash Ä‘á»ƒ di chuyá»ƒn tá»‡p CSV Ä‘áº¿n thÆ° má»¥c xá»­ lÃ½.
- **PythonOperator**: Thá»±c hiá»‡n cÃ¡c xá»­ lÃ½ dá»¯ liá»‡u, vÃ­ dá»¥ nhÆ° Ä‘á»c tá»‡p CSV vÃ  chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u thÃ nh Ä‘á»‹nh dáº¡ng phÃ¹ há»£p Ä‘á»ƒ lÆ°u vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u.
- **PostgresOperator**: Thá»±c hiá»‡n cÃ¡c lá»‡nh SQL Ä‘á»ƒ lÆ°u káº¿t quáº£ xá»­ lÃ½ vÃ o PostgreSQL.
- **EmailOperator**: Gá»­i email thÃ´ng bÃ¡o cho ngÆ°á»i dÃ¹ng khi quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u hoÃ n thÃ nh.

### Sensor

Sensor lÃ  má»™t loáº¡i Operator Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒÂ **giÃ¡m sÃ¡t**Â cÃ¡c sá»± kiá»‡n vÃ  Ä‘iá»u kiá»‡n, vÃ  thá»±c hiá»‡n cÃ¡c hÃ nh Ä‘á»™ng tÆ°Æ¡ng á»©ng. Sensor thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒÂ **Ä‘á»£i cho Ä‘áº¿n khi má»™t Ä‘iá»u kiá»‡n nÃ o Ä‘Ã³ xáº£y ra trÆ°á»›c khi tiáº¿p tá»¥c thá»±c hiá»‡n**Â quy trÃ¬nh.

CÃ¡c loáº¡i Sensor trong Airflow bao gá»“m:
- **FileSensor**: Kiá»ƒm tra sá»± tá»“n táº¡i cá»§a má»™t tá»‡p trÃªn há»‡ thá»‘ng tá»‡p.
- **TimeSensor**: Äá»£i cho Ä‘áº¿n khi má»™t khoáº£ng thá»i gian cá»¥ thá»ƒ Ä‘Ã£ trÃ´i qua.
- **HttpSensor**: Kiá»ƒm tra sá»± pháº£n há»“i cá»§a má»™t URL cá»¥ thá»ƒ.
- **HdfsSensor**: Kiá»ƒm tra sá»± tá»“n táº¡i cá»§a má»™t tá»‡p trÃªn Hadoop Distributed File System (HDFS).
- **SqlSensor**: Kiá»ƒm tra sá»± tá»“n táº¡i cá»§a má»™t báº£ng hoáº·c má»™t sá»‘ dÃ²ng dá»¯ liá»‡u trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.
- **S3KeySensor**: Kiá»ƒm tra sá»± tá»“n táº¡i cá»§a má»™t Ä‘á»‘i tÆ°á»£ng trÃªn Amazon S3.
- **ExternalTaskSensor**: Kiá»ƒm tra tráº¡ng thÃ¡i cá»§a má»™t task khÃ¡c trong DAG.

Sensor sáº½ giá»¯ cho task Ä‘ang cháº¡y vÃ  thá»­ láº¡i sau má»™t khoáº£ng thá»i gian cá»¥ thá»ƒ, giÃºp Ä‘áº£m báº£o ráº±ng khÃ´ng cÃ³ dá»¯ liá»‡u bá»‹ máº¥t hoáº·c xá»­ lÃ½ sai.

## Aiflow Workflow

KhÃ´ng giá»‘ng nhÆ° cÃ¡c cÃ´ng cá»¥ Dá»¯ liá»‡u lá»›n nhÆ° Apache Kafka, Apache Storm, Apache Spark,hoáº·c Flink, **Apache Airflow** khÃ´ng pháº£i lÃ  giáº£i phÃ¡p truyá»n dá»¯ liá»‡u. NÃ³ chá»§ yáº¿u lÃ  má»™t trÃ¬nh quáº£n lÃ½ quy trÃ¬nh lÃ m viá»‡c

![image.png](https://images.viblo.asia/e2d80240-05d3-4d64-a82d-3f7a37dd914a.png)

HÃ¬nh váº½ trÃªn tá»•ng quan vá» cÃ¡c thÃ nh pháº§n cÆ¡ báº£n cá»§a Apache Airflow.

- **Scheduler**: giÃ¡m sÃ¡t táº¥t cáº£ cÃ¡c DAG vÃ  cÃ¡c tÃ¡c vá»¥ Ä‘Æ°á»£c liÃªn káº¿t cá»§a chÃºng. Äá»‘i vá»›i 1 task, khi cÃ¡c phá»¥ thuá»™c Ä‘Æ°á»£c Ä‘Ã¡p á»©ng, Scheduler sáº½ khá»Ÿi táº¡o tÃ¡c vá»¥ Ä‘Ã³. NÃ³ kiá»ƒm tra cÃ¡c tÃ¡c vá»¥ Ä‘ang hoáº¡t Ä‘á»™ng Ä‘á»ƒ báº¯t Ä‘áº§u theo Ä‘á»‹nh ká»³: `scheduler`Â láº¥y cÃ¡cÂ `task`Â chÆ°a Ä‘Æ°á»£c thá»±c thi hoáº·c Ä‘Æ°á»£c thá»±c thi láº¡i ra Ä‘á»ƒ xá»­ lÃ½
- **Executor**: xá»­ lÃ½ viá»‡c cháº¡y cÃ¡c task nÃ y báº±ng cÃ¡ch Ä‘Æ°a chÃºng cho worker Ä‘á»ƒ cháº¡y:
	- Äáº¿n lÃºc nÃ y, má»™tÂ `TaskInstance`Â sáº½ Ä‘Æ°á»£c táº¡o ra nháº±m má»¥c Ä‘Ã­ch chá»©a tráº¡ng thÃ¡i cá»§a quÃ¡ trÃ¬nh thá»±c thi task cÅ©ng nhÆ° cÃ¡c thÃ´ng tin khÃ¡c, cháº³ng háº¡n nhÆ° context cá»§a nÃ³.
	- Tiáº¿p Ä‘Ã³ Airflow cho phÃ©p user **render cÃ¡c giÃ¡ trá»‹ táº¡i thá»i Ä‘iá»ƒm task instance Ä‘Æ°á»£c táº¡o thÃ´ng qua Jinja templating**, váº­y nÃªn báº¡n cÃ³ thá»ƒ tham kháº£o cÃ¡ch sá»­ dá»¥ng chÃºng Ä‘á»ƒ code ngáº¯n hÆ¡n táº¡iÂ [Ä‘Ã¢y](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html)
	- QuÃ¡ trÃ¬nh thá»±c thi báº±ng má»™t CeleryExecutor nhÆ° biá»ƒu Ä‘á»“. Ta sáº½ cÃ³ hai tiáº¿n trÃ¬nh Ä‘Æ°á»£c khá»Ÿi táº¡o, vÃ  Ä‘Ã³ lÃ  `RawTaskProcess`(thá»±c thi cÃ¡c mÃ£ nguá»“n Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi ngÆ°á»i dÃ¹ng) vÃ  `LocalTaskJobProcess`(theo dÃµi RawTaskProcess)
	![](https://images.viblo.asia/b38f74de-f259-4a59-931d-acdc2c6cf6d2.png)
	
- **Web server**: giao diá»‡n ngÆ°á»i dÃ¹ng cá»§a Airflow, hiá»‡n thá»‹ tráº¡ng thÃ¡i cá»§a nhiá»‡m vá»¥ vÃ  cho phÃ©p ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tÃ¡c vá»›i cÆ¡ sá»Ÿ dá»¯ liá»‡u cÅ©ng nhÆ° Ä‘á»c tá»‡p nháº­t ká»¹ tá»« kho lÆ°u trá»¯ tá»« xa nhÆ° Google Cloud Storage, S3, ...
- **DAG Directory**: má»™t thÆ° má»¥c chá»©a cÃ¡c file DAG cá»§a cÃ¡c quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u (data pipelines) trong Airflow.
- **Metabase Database**: Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi Scheduler, Executor vÃ  Web Server Ä‘á»ƒ lÆ°u trá»¯ thÃ´ng tin quan trá»ng cá»§a tá»«ng DAG, vÃ­ dá»¥ nhÆ° cÃ¡c phiÃªn báº£n, sá»‘ liá»‡u thá»‘ng kÃª má»—i láº§n cháº¡y, khoáº£ng thá»i gian lÃªn lá»‹ch, ...

### Luá»“ng Ä‘i cá»§a Multi-Nodes Architecture

Airflow sá»­ dá»¥ng **Celery Executor** trong mÃ´ hÃ¬nh **Multi Nodes Architecture**, giÃºp phÃ¢n tÃ¡n vÃ  xá»­ lÃ½ cÃ¡c tÃ¡c vá»¥ (tasks) trÃªn nhiá»u worker nodes. DÆ°á»›i Ä‘Ã¢y lÃ  luá»“ng Ä‘i cá»§a Airflow trong kiáº¿n trÃºc nÃ y:
```mermaid
graph TD;
    A[User] -->|TÆ°Æ¡ng tÃ¡c vá»›i| B[Web Server]
    B -->|Gá»­i thÃ´ng tin DAG Ä‘áº¿n| C[Metastore]
    B -->|Gá»­i DAG Ä‘áº¿n| D[Scheduler]
    D -->|Láº­p lá»‹ch Task| E[Executor]
    E -->|ÄÆ°a Task vÃ o| F[Queue]
    F -->|Gá»­i Task Ä‘áº¿n| G1[Worker Node 1]
    F -->|Gá»­i Task Ä‘áº¿n| G2[Worker Node 2]
    F -->|Gá»­i Task Ä‘áº¿n| G3[Worker Node 3]
    G1 -->|Cáº­p nháº­t tráº¡ng thÃ¡i| C
    G2 -->|Cáº­p nháº­t tráº¡ng thÃ¡i| C
    G3 -->|Cáº­p nháº­t tráº¡ng thÃ¡i| C
    C -->|Cung cáº¥p thÃ´ng tin Task| B
```

**NgÆ°á»i dÃ¹ng tÆ°Æ¡ng tÃ¡c vá»›i Web Server**

- NgÆ°á»i dÃ¹ng truy cáº­p **Web Server** trÃªn **Node 1** Ä‘á»ƒ táº¡o, sá»­a Ä‘á»•i, hoáº·c cháº¡y DAGs (Directed Acyclic Graphs).
- Web Server gá»­i thÃ´ng tin DAG Ä‘áº¿n **Metastore** trÃªn **Node 2**, nÆ¡i lÆ°u trá»¯ thÃ´ng tin vá» DAGs, Task Instances vÃ  Logs.

**Scheduler láº­p lá»‹ch vÃ  gá»­i tÃ¡c vá»¥ Ä‘áº¿n Executor**

- **Scheduler** trÃªn **Node 1** Ä‘á»c thÃ´ng tin tá»« **Metastore** vÃ  quyáº¿t Ä‘á»‹nh tÃ¡c vá»¥ nÃ o cáº§n cháº¡y.
- Scheduler gá»­i tÃ¡c vá»¥ Ä‘áº¿n **Executor**.

**Executor Ä‘Æ°a tÃ¡c vá»¥ vÃ o hÃ ng Ä‘á»£i (Queue)**

- Executor sá»­ dá»¥ng **Queue** (cÃ³ thá»ƒ lÃ  Redis hoáº·c RabbitMQ) Ä‘á»ƒ lÆ°u cÃ¡c tÃ¡c vá»¥ chá» thá»±c thi.
- HÃ ng Ä‘á»£i Ä‘Ã³ng vai trÃ² trung gian, giÃºp phÃ¢n phá»‘i nhiá»‡m vá»¥ Ä‘áº¿n cÃ¡c worker.

**Workers nháº­n tÃ¡c vá»¥ vÃ  thá»±c thi**

- CÃ¡c **Worker Nodes** (Worker Node 1, 2, 3) liÃªn tá»¥c láº¥y cÃ´ng viá»‡c tá»« **Queue** vÃ  thá»±c hiá»‡n chÃºng.
- Má»—i **Worker Node** cháº¡y má»™t hoáº·c nhiá»u **Airflow Workers**, xá»­ lÃ½ cÃ¡c tÃ¡c vá»¥ theo DAG.

**Káº¿t quáº£ Ä‘Æ°á»£c cáº­p nháº­t vá» Metastore**

- Sau khi hoÃ n thÃ nh, worker gá»­i tráº¡ng thÃ¡i cá»§a task (thÃ nh cÃ´ng/tháº¥t báº¡i) trá»Ÿ láº¡i **Metastore** trÃªn **Node 2**.
- NgÆ°á»i dÃ¹ng cÃ³ thá»ƒ kiá»ƒm tra tráº¡ng thÃ¡i trÃªn **Web Server**.

**TÃ³m táº¯t láº¡i quy trÃ¬nh Airflow (Celery Executor)**
2. NgÆ°á»i dÃ¹ng táº¡o DAG trÃªn **Web Server**.
3. Scheduler lÃªn lá»‹ch vÃ  Ä‘áº©y task vÃ o **Queue**.
4. CÃ¡c **Workers** láº¥y task tá»« hÃ ng Ä‘á»£i vÃ  xá»­ lÃ½ chÃºng.
5. Tráº¡ng thÃ¡i task Ä‘Æ°á»£c cáº­p nháº­t trong **Metastore**.
6. NgÆ°á»i dÃ¹ng cÃ³ thá»ƒ xem tráº¡ng thÃ¡i trÃªn **Web Server**.
## Installation & Setup

### Installation

CÃ³ nhiá»u cÃ¡ch setup airflow ([xem thÃªm](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html)):
- [Using released sources](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html#using-released-sources)
- [Using PyPI](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html#using-pypi)
- [Using Production Docker Images](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html#using-production-docker-images)
- [Using Official Airflow Helm Chart](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html#using-official-airflow-helm-chart)
- [Using Managed Airflow Services](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html#using-managed-airflow-services)
- [Using 3rd-party images, charts, deployments](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html#using-3rd-party-images-charts-deployments)
- [Notes about minimum requirements](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html#notes-about-minimum-requirements)

ChÃºng ta sáº½ thá»±c hiá»‡n setup airflow báº±ng docker-compose:
**1. Setup docker-compose**
```bash
sudo curl -L "https://github.com/docker/compose/releases/download/v2.9.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

sudo chmod +x /usr/local/bin/docker-compose

docker-compose --version

```
**2. Setup Airflow docker-compose file**
```
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/lastest/docker-compose.yaml'

# chá»‰nh sá»­a docker-compose file cho phÃ¹ há»£p
```

Trong file yaml nÃ y Ä‘Ã£ cÃ³ chá»©a cÃ¡c services sáºµn:

- **airflow-scheduler**: giÃ¡m sÃ¡t cÃ¡c tasks cÅ©ng nhÆ° cháº¡y tasks khi Ä‘Ã£ cÃ³ Ä‘á»§ dependencies
- **airflow-webserver**: webserver cÃ³ local domainÂ `http://localhost:8080`
- **airflow-worker**: cÃ¡c worker cháº¡y cÃ¡c tasks theo lá»‡nh cá»§a scheduler
- **airflow-init**: dá»‹ch vá»¥ khá»Ÿi táº¡o ban Ä‘áº§u ( táº¡o account, migrate database, ... )
- **postgres**: cÆ¡ sá»Ÿ dá»¯ liá»‡u
- **redis**: cáº§u ná»‘i truyá»n dáº«n cÃ¡c lá»‡nh tá»« scheduler tá»›i worker.

P/S: Náº¿u báº¡n muá»‘n cÃ i thÃªm má»™t sá»‘ thÆ° viá»‡n python hoáº·c nÃ¢ng cáº¥p airflow providers thÃ¬ cÃ³ thá»ƒ Ä‘iá»u chá»‰nh file `docker-compose.yaml` mÃ  báº¡n vá»«a táº£i vá» bÃªn trÃªn.

- Do thÆ° má»¥c DAG sáº½ Ä‘Æ°á»£c mount vÃ o trong cÃ¡c container (chi tiáº¿t vui lÃ²ng xem trong fileÂ [docker-compose.yaml](https://airflow.apache.org/docs/apache-airflow/2.6.0/docker-compose.yaml)Â ) váº­y nÃªn ta cáº§n Ä‘iá»u chá»‰nh má»™t chÃºt thÃ´ng qua viá»‡c Ä‘áº·t biáº¿n mÃ´i trÆ°á»ngÂ `AIRFLOW_UID`Â Ä‘á»ƒ trÃ¡nh cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n quyá»n cÃ³ thá»ƒ phÃ¡t sinh, nháº¥t lÃ  vá»›i cÃ¡c file trong thÆ° má»¥c DAG

**3. Khá»Ÿi táº¡o mÃ´i trÆ°á»ng**
TrÆ°á»›c khi cháº¡y Airflow láº§n Ä‘áº§u, báº¡n cáº§n pháº£i chuáº©n bá»‹ mÃ´i trÆ°á»ng chÃºt: táº¡o files, folders cáº§n thiáº¿t, khá»Ÿi táº¡o cÆ¡ sá»Ÿ dá»¯ liá»‡u
```bash
mkdir -p ./dags ./logs ./plugins
echo -e "AIRFLOW_UID=$(id -u)" > .env
```
- `./dags` : nÆ¡i chá»©a files DAG
- `./logs` : nÆ¡i chá»©a log cá»§a executor vÃ  scheduler
- `./plugins` : nÆ¡i chá»©a cÃ¡c plugin tÃ¹y chá»‰nh cá»§a báº¡n
> á» má»™t sá»‘ há»‡ Ä‘iá»u hÃ nh, náº¿u khÃ´ng set biáº¿nÂ `AIRFLOW_UID`Â sáº½ hiá»‡n lÃªn cáº£nh bÃ¡o, báº¡n cÃ³ thá»ƒ phá»›t lá» chÃºng Ä‘i. Thay vÃ o Ä‘Ã³, báº¡n cÃ³ thá»ƒ táº¡o 1 fileÂ `.env`Â trong cÃ¹ng folder fileÂ `docker-compose.yaml`Â vá»›i ná»™i dung `AIRFLOW_UID=50000`

**4. Khá»Ÿi cháº¡y**

```bash
# Khá»Ÿi táº¡o cÆ¡ sá»Ÿ dá»¯ liá»‡u vÃ  tÃ i khoáº£n trÆ°á»›c
docker-compose up airflow-init

# check instance
docker ps -a

# run airflow
docker-compose up

# báº¡n cÅ©ng cÃ³ thá»ƒ xÃ³a container vÃ  image náº¿u thá»­ nghiá»‡m xong cho trÃ¡nh náº·ng mÃ¡y
docker-compose down --volumes --rmi all

# Hoáº·c chá»‰ xÃ³a má»—i container
docker-compose down --volumes --remove-orphans

```

Truy cáº­p **Web Interface**: `http://localhost:8080` 
- TÃ i khoáº£n / máº­t kháº©u máº·c Ä‘á»‹nh:Â `airflow` / `airflow`
> 	sá»­a tÃ i khoáº£n vÃ  pass trong biáº¿n táº¡i file docker-compose.yaml : `_AIRFLOW_WWW_USER_USERNAME` , `_AIRFLOW_WWW_USER_PASSWORD`
- Náº¿u báº¡n khÃ´ng muá»‘n WebUI chá»©a cÃ¡c file DAG máº«u mÃ  nhÃ  phÃ¡t triá»ƒn cung cáº¥p thÃ¬ cÃ³ thá»ƒ thay Ä‘á»•i envÂ `AIRFLOW__CORE__LOAD_EXAMPLES: 'false'`Â trong file `docker-compose.yaml`

### CÃ i Ä‘áº·t packages
CÃ¡c thÃ nh pháº§n cá»§a airflow sáº½ sá»­ dá»¥ng chung má»™t docker image. Váº­y nÃªn trong trÆ°á»ng há»£p cáº§n tÃ¹y chá»‰nh hoáº·c cÃ i Ä‘áº·t thÃªm cÃ¡c dependencies, cÃ¡ch tá»‘t nháº¥t vá»›i docker-compose Ä‘Æ°á»£c viáº¿t sáºµn kia sáº½ lÃ  build trÆ°á»›c má»™t custom image vÃ  Ä‘á»ƒ tÃªn cá»§a nÃ³ vÃ o fileÂ `.env`. Trong trÆ°á»ng há»£p báº¡n muá»‘n test nhanh má»™t thÆ° viá»‡n nÃ o Ä‘Ã³,Â `PythonVirtualenvOperator`Â sáº½ lÃ  lá»±a chá»n thÃ­ch há»£p

Khi báº¡n sá»­ dá»¥ng **Docker Compose** Ä‘á»ƒ cháº¡y Apache Airflow, táº¥t cáº£ cÃ¡c thÃ nh pháº§n nhÆ° **Scheduler, Webserver, Worker, Triggerer, Flower** Ä‘á»u sá»­ dá»¥ng chung má»™t **Docker image** do Airflow cung cáº¥p (thÆ°á»ng cÃ³ tÃªn `apache/airflow`). Äiá»u nÃ y cÃ³ nghÄ©a lÃ :

- Náº¿u báº¡n muá»‘n cÃ i thÃªm **thÆ° viá»‡n Python hoáº·c package há»‡ thá»‘ng**, báº¡n cáº§n cáº­p nháº­t image nÃ y, thay vÃ¬ cÃ i Ä‘áº·t riÃªng láº» trÃªn tá»«ng thÃ nh pháº§n.
- Náº¿u chá»‰ sá»­a má»™t thÃ nh pháº§n, nhá»¯ng thÃ nh pháº§n khÃ¡c váº«n bá»‹ áº£nh hÆ°á»Ÿng vÃ¬ táº¥t cáº£ Ä‘á»u dÃ¹ng chung image.

#### ğŸŸ¢ CÃ¡ch tá»‘t nháº¥t: Build má»™t **custom image**

Vá»›i Docker Compose, cÃ¡ch tá»‘i Æ°u nháº¥t Ä‘á»ƒ cÃ i Ä‘áº·t thÃªm thÆ° viá»‡n hoáº·c thay Ä‘á»•i cÃ i Ä‘áº·t lÃ :

7. **Táº¡o má»™t Dockerfile** Ä‘á»ƒ má»Ÿ rá»™ng image gá»‘c cá»§a Airflow.
8. **CÃ i Ä‘áº·t thÃªm thÆ° viá»‡n, dependencies vÃ o image má»›i**.
9. **Cáº­p nháº­t file `.env` Ä‘á»ƒ sá»­ dá»¥ng image nÃ y** thay vÃ¬ image gá»‘c.

ğŸ”¹ **VÃ­ dá»¥** vá» `Dockerfile` Ä‘á»ƒ thÃªm thÆ° viá»‡n `pandas` vÃ  `requests`:

Dockerfile tuá»³ chá»‰nh
```dockerfile
# Use the official Apache Airflow image as the base
FROM apache/airflow:2.10.5

# Switch to the root user to install OS-level packages
USER root

# Install OS-level packages
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
       vim \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Switch back to the airflow user
USER airflow

# Install Python packages
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
```

**Build and Tag the Docker Image:** Navigate to the directory containing your `Dockerfile` and execute the build command:
```bash
docker build . --pull --tag my-airflow-image:latest
```
--> builds the image and tags it as `my-airflow-image:latest`

**Update Your Docker Compose Configuration:** If you're using Docker Compose, modify your `docker-compose.yaml` to use your custom image:

```yaml
version: '3'
services:
  airflow:
    image: my-airflow-image:latest
    # ... other configurations ...

```

#### ğŸŸ¢ Thá»­ nghiá»‡m nhanh vá»›i thÆ° viá»‡n má»›i:

Náº¿u báº¡n chá»‰ muá»‘n thá»­ nghiá»‡m nhanh má»™t thÆ° viá»‡n trong má»™t tÃ¡c vá»¥ cá»¥ thá»ƒ mÃ  khÃ´ng muá»‘n táº¡o láº¡i Docker image, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng `PythonVirtualenvOperator`. Operator nÃ y cho phÃ©p táº¡o má»™t mÃ´i trÆ°á»ng áº£o Python riÃªng biá»‡t trong quÃ¡ trÃ¬nh cháº¡y tÃ¡c vá»¥, nÆ¡i báº¡n cÃ³ thá»ƒ cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n mÃ´i trÆ°á»ng chung cá»§a Airflow.

```python
from airflow import DAG
from airflow.operators.python import PythonVirtualenvOperator
from datetime import datetime

def test_pandas():
    import pandas as pd
    df = pd.DataFrame({"A": [1, 2, 3]})
    print(df)

with DAG("test_virtualenv", start_date=datetime(2025, 1, 1), schedule_interval=None) as dag:
    run_task = PythonVirtualenvOperator(
        task_id="install_and_run_pandas",
        python_callable=test_pandas,
        requirements=["pandas"],  # ThÆ° viá»‡n cáº§n cÃ i Ä‘áº·t trong virtualenv
        system_site_packages=False
    )

    run_task

```

- **KhÃ´ng cáº§n build láº¡i Docker image:** Tiáº¿t kiá»‡m thá»i gian vÃ  cÃ´ng sá»©c khi chá»‰ cáº§n thá»­ nghiá»‡m nhanh.
- **KhÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡c thÃ nh pháº§n khÃ¡c:** MÃ´i trÆ°á»ng áº£o Ä‘Æ°á»£c táº¡o riÃªng cho tá»«ng tÃ¡c vá»¥, Ä‘áº£m báº£o tÃ­nh Ä‘á»™c láº­p.
- **Dá»… dÃ ng thá»­ nghiá»‡m cÃ¡c thÆ° viá»‡n má»›i:** Nhanh chÃ³ng kiá»ƒm tra tÃ­nh tÆ°Æ¡ng thÃ­ch vÃ  hiá»‡u quáº£ cá»§a thÆ° viá»‡n trÆ°á»›c khi tÃ­ch há»£p chÃ­nh thá»©c.

### Deployment

- For cloud/airflow service: Â [Amazon Managed Workflows for Apache Airflow (MWAA)](https://aws.amazon.com/managed-workflows-for-apache-airflow/)Â hayÂ [Google Cloud Composer](https://cloud.google.com/composer)Â vÃ Â [Azure Data Factory Managed Airflow](https://learn.microsoft.com/azure/data-factory/concept-managed-airflow)
- Hoáº·c sá»­ dá»¥ng **k8s**

Sá»­ dá»¥ng Kubernetes cho Airflow:
- Sá»­ dá»¥ng má»™t sá»‘ chart Ä‘Æ°á»£c viáº¿t sáºµn vÃ  chá»‰nh sá»­a thÃªm, check chart: https://github.com/airflow-helm/charts
- Äá»“ng bá»™ phiÃªn báº£n DAG nhÆ° tháº¿ nÃ o ? Â ThÃ´ng thÆ°á»ng thÃ¬ khÃ´ng pháº£i DAG cá»© code phÃ¡t lÃ  xong luÃ´n mÃ  ta sáº½ cáº§n chá»‰nh sá»­a chÃºng qua cÃ¡c phiÃªn báº£n khÃ¡c nhau. Váº­y lÃ m tháº¿ nÃ o Ä‘á»ƒ ta cÃ³ thá»ƒ tá»± Ä‘á»™ng cáº­p nháº­t phiÃªn báº£n DAG má»—i khi chÃºng Ä‘Æ°á»£c thay Ä‘á»•i? Giáº£i phÃ¡p mÃ¬nh (vÃ  Ä‘a sá»‘) chá»n lÃ  sá»­ dá»¥ng git-sync sidecar. Viá»‡c sá»­ dá»¥ng nÃ³ Ä‘Æ¡n giáº£n sáº½ lÃ  ta táº¡o thÃªm má»™t container cháº¡y song song vá»›i container chÃ­nh, cÃ³ nhiá»‡m vá»¥ theo dÃµi má»™t repo Git vÃ  cáº­p nháº­t má»i thay Ä‘á»•i tá»« Ä‘Ã³. Chi tiáº¿t thÃ¬ má»i ngÆ°á»i cÃ³ thá»ƒ Ä‘á»c thÃªm táº¡iÂ [https://github.com/kubernetes/git-sync](https://github.com/kubernetes/git-sync)
- **PodDisruptionBudget**: VÃ¬ Ä‘ang triá»ƒn khai trÃªn K8s nÃªn táº¥t nhiÃªn ta sáº½ cáº§n quan tÃ¢m Ä‘áº¿n má»™t sá»‘ khÃ¡i niá»‡m cá»§a nÃ³ ná»¯a. PodDisruptionBudget thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ quáº£n lÃ½ sá»‘ giÃ¡n Ä‘oáº¡n cÃ³ thá»ƒ cÃ³, khÃ¡ lÃ  thÃ­ch há»£p khi cÃ¡c báº¡n cÃ³ má»™t nhÃ³m cÃ¡c Worker vÃ  ta cáº§n Ä‘áº£m báº£o sá»‘ lÆ°á»£ng tá»‘i thiá»ƒu worker Ä‘ang hoáº¡t Ä‘á»™ng. Vá» chi tiáº¿t cÃ¡ch sá»­ dá»¥ng thÃ¬ báº¡n cÃ³ thá»ƒ xem thÃªm táº¡iÂ [https://kubernetes.io/docs/tasks/run-application/configure-pdb/](https://kubernetes.io/docs/tasks/run-application/configure-pdb/)

#### Triá»ƒn khai Airflow trÃªn k8s

#### Triá»ƒn khai Airflow trÃªn Google Composer

### Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng

Cáº¥u hÃ¬nh thÃ´ng qua cÃ¡c biáº¿n mÃ´i trÆ°á»ng lÃ  cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº£m báº£o quÃ¡ trÃ¬nh cÃ i Ä‘áº·t vÃ  triá»ƒn khai háº§u háº¿t cÃ¡c há»‡ thá»‘ng (https://12factor.net/vi/config)

Vá»›i Airflow, ta sáº½ cÃ³ hai khÃ¡i niá»‡m Ä‘á»ƒ sá»­ dá»¥ng, bao gá»“m Variable vÃ  Connection.

#### Variable

Äáº§u tiÃªn lÃ  **Variable**, Ä‘Ã¢y lÃ  khÃ¡i niá»‡m runtime configuration cá»§a Airflow, Ä‘Æ°á»£c lÆ°u trá»¯ theo tá»«ng cáº·p key/value vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng toÃ n cá»¥c táº¡i táº¥t cáº£ cÃ¡c vá»‹ trÃ­. Viá»‡c sá»­ dá»¥ng cÃ¡c Airflow Variable cÃ³ thá»ƒ thá»±c hiÃªn theo hai cÃ¡ch nhÆ° sau:

- Láº¥y ra dá»¯ liá»‡u tá»« phÆ°Æ¡ng thá»©c `get`

```python
from airflow.modelsimport Variable

# Normal call style
foo = Variable.get("foo")

# Auto-deserializes a JSON value
bar = Variable.get("bar", deserialize_json=True)

# Returns the value of default_var (None) if the variable is not set
baz = Variable.get("baz", default_var=None)
```

- Hoáº·c render ra thÃ´ng qua cÃ¡c marco náº¿u nhÆ° báº¡n Ä‘ang khÃ´ng sá»­ dá»¥ng cÃ¡cÂ `PythonOperator`

```bash
# Raw value
echo {{ var.value.<variable_name> }}

# Auto-deserialize JSON value
echo {{ var.json.<variable_name> }}
```

Váº­y ta cÃ³ thá»ƒ Ä‘áº·t giÃ¡ trá»‹ cÅ©ng quáº£n lÃ½ chÃºng nhÆ° tháº¿ nÃ o? CÃ¡ch Ä‘Æ¡n giáº£n nháº¥t sáº½ lÃ  thÃ´ng qua WebUI vÃ  Airflow cÃ³ sáºµn má»™t tab trÃªn trang webserver Ä‘á»ƒ ta quáº£n lÃ½ cÃ¡c Variable. Trang nÃ y cÃ³ giao diá»‡n nhÆ° sau:

![Untitled 4.png](https://images.viblo.asia/5a884978-f5bc-4986-804c-689fe3a526e1.png)

ÄÃ´i khi ta sáº½ muá»‘n Ä‘áº·t sáºµn cÃ¡c Variable thÃ´ng qua cÃ¡c biáº¿n mÃ´i trÆ°á»ng, cháº³ng háº¡n nhÆ° lÃºc deploy. Khi Ä‘Ã³, Airflow sáº½ Ä‘á»c cÃ¡c Variable tá»« cÃ¡c biáº¿n mÃ´i trÆ°á»ng Ä‘Æ°á»£c báº¯t Ä‘áº§u báº±ng tiá»n tá»‘Â `AIRFLOW_VAR_`Â cháº³ng háº¡n nhÆ° sau:

```bash
export AIRFLOW_VAR_FOO=BAR

# To use JSON, store them as JSON strings
export AIRFLOW_VAR_FOO_BAZ='{"hello":"world"}'
```

Tiáº¿p Ä‘Ã³, thÃ´ng thÆ°á»ng cÃ¡c thÃ´ng tin Ä‘á»ƒ truy cáº­p Ä‘áº¿n cÃ¡c dá»‹ch vá»¥ khÃ¡c sáº½ gá»“m tá»« ba Ä‘áº¿n 5 trÆ°á»ng cháº³ng háº¡n nhÆ°Â `AWS S3`Â sáº½ yÃªu cáº§uÂ `AWS Access Key ID**,`**Â `AWS Secret Access Key`**,**Â `Region Name`, â€¦ vÃ  cÃ³ thá»ƒ cÃ²n nhiá»u hÆ¡n ná»¯a. Váº­y nÃªn thay vÃ¬ Ä‘áº·t chÃºng á»Ÿ cÃ¡c Variable riÃªng biá»‡t, Airflow cho phÃ©p chÃºng ta nhÃ³m chÃºng thÃ nh má»™t Connecton vÃ  chÃºng ta cÃ³ thá»ƒ truyá»n id cá»§a chÃºng vÃ o cÃ¡c Operator tÆ°Æ¡ng á»©ng Ä‘á»ƒ sá»­ dá»¥ng, cháº³ng háº¡n nhÆ° vá»›iÂ `SlackConnection`Â `slack_conn`Â vafÂ `SnowFlake Connection snowflake_conn`Â Ä‘Æ°á»£c sá»­ dá»¥ng trongÂ `SnowflakeToSlackOperator`nhÆ° sau:

```python
from airflow.decorators import dag
from pendulum import datetime
from airflow.providers.snowflake.transfers.snowflake_to_slack import (
    SnowflakeToSlackOperator,
)

@dag(start_date=datetime(2022, 7, 1), schedule=None, catchup=False)
def snowflake_to_slack_dag():
    transfer_task = SnowflakeToSlackOperator(
        task_id="transfer_task",
        # the two connections are passed to the operator here:
        snowflake_conn_id="snowflake_conn",
        slack_conn_id="slack_conn",
        params={"table_name": "ORDERS", "col_to_sum": "O_TOTALPRICE"},
        sql="""
            SELECT
              COUNT(*) AS row_count,
              SUM({{ params.col_to_sum }}) AS sum_price
            FROM {{ params.table_name }}
        """,
        slack_message="""The table {{ params.table_name }} has
            => {{ results_df.ROW_COUNT[0] }} entries
            => with a total price of {{results_df.SUM_PRICE[0]}}""",
    )

    transfer_task

snowflake_to_slack_dag()
```

#### Connection

Viá»‡c quáº£n lÃ½ cÃ¡c **Connection** cÅ©ng khÃ¡ giá»‘ng vá»›i Variable khi ta cÃ³ thá»ƒ Ä‘á»ƒ Airflow Ä‘á»c chÃºng thÃ´ng qua cÃ¡c biáº¿n mÃ´i trÆ°á»ng cÃ³ tiá»n tá»‘ lÃ Â `AIRFLOW_CONN_`Â hoáº·c quáº£n lÃ½ thÃ´ng qua WebUI nhÆ° dÆ°á»›i Ä‘Ã¢y

![Untitled 5.png](https://images.viblo.asia/87cdf9e6-d073-44bf-8756-54ce343196fc.png)

## Airflow Command

### Quáº£n lÃ½ Docker

- **Hiá»ƒn thá»‹ cÃ¡c container Docker Ä‘ang cháº¡y:**  `docker ps`

- **Thá»±c thi lá»‡nh `/bin/bash` trong container cÃ³ `container_id` Ä‘á»ƒ má»Ÿ má»™t phiÃªn shell:** `docker exec -it container_id /bin/bash`
  
### Quáº£n lÃ½ thÆ° má»¥c vÃ  Ä‘Æ°á»ng dáº«n

- **Hiá»ƒn thá»‹ Ä‘Æ°á»ng dáº«n hiá»‡n táº¡i báº¡n Ä‘ang Ä‘á»©ng:** `pwd`

- **Hiá»ƒn thá»‹ danh sÃ¡ch cÃ¡c file vÃ  thÆ° má»¥c trong thÆ° má»¥c hiá»‡n táº¡i:** `ls`
### Quáº£n lÃ½ cÆ¡ sá»Ÿ dá»¯ liá»‡u cá»§a Airflow

- **Khá»Ÿi táº¡o metadatabase:** `airflow db init`

- **Khá»Ÿi táº¡o láº¡i metadatabase (xÃ³a táº¥t cáº£ dá»¯ liá»‡u):** `airflow db reset`

- **NÃ¢ng cáº¥p metadatabase (Ã¡p dá»¥ng schema vÃ  giÃ¡ trá»‹ má»›i nháº¥t):** `airflow db upgrade`

### Khá»Ÿi Ä‘á»™ng cÃ¡c thÃ nh pháº§n cá»§a Airflow

- **Khá»Ÿi Ä‘á»™ng webserver cá»§a Airflow:** `airflow webserver`

- **Khá»Ÿi Ä‘á»™ng scheduler cá»§a Airflow:** `airflow scheduler`

- **Khá»Ÿi Ä‘á»™ng má»™t Celery worker (há»¯u Ã­ch trong cháº¿ Ä‘á»™ phÃ¢n tÃ¡n Ä‘á»ƒ phÃ¢n chia nhiá»‡m vá»¥ giá»¯a cÃ¡c mÃ¡y/nÃºt):** `airflow celery worker`

### Quáº£n lÃ½ DAGs

- **Liá»‡t kÃª danh sÃ¡ch cÃ¡c DAG Ä‘Ã£ biáº¿t (bao gá»“m cÃ¡c DAG trong thÆ° má»¥c `examples` hoáº·c `dags`):** `airflow dags list`

- **KÃ­ch hoáº¡t DAG `example_python_operator` vá»›i ngÃ y hiá»‡n táº¡i lÃ m execution date:** `airflow dags trigger example_python_operator`

- **KÃ­ch hoáº¡t DAG `example_python_operator` vá»›i má»™t ngÃ y trong quÃ¡ khá»© lÃ m execution date (LÆ°u Ã½: Lá»‡nh nÃ y sáº½ khÃ´ng kÃ­ch hoáº¡t cÃ¡c task cá»§a DAG náº¿u khÃ´ng Ä‘áº·t tÃ¹y chá»n `catchup=True` trong Ä‘á»‹nh nghÄ©a DAG):**  
  `airflow dags trigger example_python_operator -e 2021-01-01`

- **KÃ­ch hoáº¡t DAG `example_python_operator` vá»›i má»™t ngÃ y trong tÆ°Æ¡ng lai (Ä‘iá»u chá»‰nh sao cho cÃ³ thÃªm khoáº£ng 2 phÃºt so vá»›i ngÃ y hiá»‡n táº¡i hiá»ƒn thá»‹ trong giao diá»‡n Airflow UI). DAG sáº½ Ä‘Æ°á»£c lÃªn lá»‹ch cháº¡y vÃ o ngÃ y Ä‘Ã³:**  
  `airflow dags trigger example_python_operator -e '2021-01-01 19:04:00+00:00'`

- **Hiá»ƒn thá»‹ lá»‹ch sá»­ cÃ¡c láº§n cháº¡y cá»§a DAG `example_python_operator`:** `airflow dags list-runs -d example_python_operator`

### Quáº£n lÃ½ Tasks

- **Liá»‡t kÃª cÃ¡c task cÃ³ trong DAG `example_python_operator`:** `airflow tasks list example_python_operator`

- **Kiá»ƒm thá»­ task `print_the_context` cá»§a DAG `example_python_operator` cho ngÃ y `2021-01-01` mÃ  khÃ´ng cáº§n quan tÃ¢m Ä‘áº¿n dependencies hoáº·c cÃ¡c láº§n cháº¡y trÆ°á»›c Ä‘Ã³ (Há»¯u Ã­ch cho viá»‡c debug):**  
  `airflow tasks test example_python_operator print_the_context 2021-01-01`
